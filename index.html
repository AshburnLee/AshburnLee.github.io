<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Junhui&#39;s Journal">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junhui">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-cpp-void型指针" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/" class="article-date">
  <time datetime="2020-06-08T12:19:39.000Z" itemprop="datePublished">2020-06-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/">cpp-void型指针</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面这个函数什么意思：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">foo</span><span class="params">(<span class="keyword">void</span>* a)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>他表示foo接受任何类型的指针，并输出任何类型的指针。使用方式总结如下：</p>
<h2 id="1-void指针可以指向任何类型指针-但反过来就不对了"><a href="#1-void指针可以指向任何类型指针-但反过来就不对了" class="headerlink" title="1. void指针可以指向任何类型指针,但反过来就不对了"></a>1. void指针可以指向任何类型指针,但反过来就不对了</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> f = <span class="number">5.5</span>;</span><br><span class="line"><span class="keyword">float</span>* pf = &amp;f;</span><br><span class="line"><span class="keyword">void</span>* pv = pf;  <span class="comment">// void指针可以指向float型指针</span></span><br><span class="line"><span class="keyword">float</span>* pf2 = pv; <span class="comment">// 错，float指针不能指向void指针</span></span><br></pre></td></tr></table></figure>

<h2 id="2-void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型"><a href="#2-void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型" class="headerlink" title="2.void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型"></a>2.void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> x = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">void</span>* yv = foo(&amp;x);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;yv&lt;&lt;<span class="string">" "</span>&lt;&lt;*(<span class="keyword">float</span>*)yv&lt;&lt;<span class="built_in">endl</span>; <span class="comment">// 返回0x7ffc78a54e1c 4</span></span><br><span class="line"><span class="comment">//cout&lt;&lt;*yv&lt;&lt;endl;  // 编译错误</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*(<span class="keyword">double</span>*)yv&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 返回1.44068e+273。返回值错误</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;(<span class="keyword">double</span>)(*(<span class="keyword">float</span>*)yv)&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 返回4。正确</span></span><br></pre></td></tr></table></figure>

<h2 id="3-void指针可以使用nullptr初始化"><a href="#3-void指针可以使用nullptr初始化" class="headerlink" title="3. void指针可以使用nullptr初始化"></a>3. void指针可以使用nullptr初始化</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span>* vPtr = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*vPtr&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 编译错误，vPtr不指向任何对象，所以取不到任何内容</span></span><br></pre></td></tr></table></figure>
<h2 id="4-接受任何类型的指针，并输出任何类型的指针"><a href="#4-接受任何类型的指针，并输出任何类型的指针" class="headerlink" title="4. 接受任何类型的指针，并输出任何类型的指针"></a>4. 接受任何类型的指针，并输出任何类型的指针</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span>* vPtr = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">void</span>* vFunPtr = foo(vPtr);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;vFunPtr&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 空指针，所以返回0</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/" data-id="ckb6h8i2f00006sfze2n59ljw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-Layer中有什么" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/" class="article-date">
  <time datetime="2020-06-07T12:27:32.000Z" itemprop="datePublished">2020-06-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/">caffe-Layer中有什么</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一个Layer对象以一个Blob为输入（bottom），另一个Blob为输出（top）。主要机选包括前向计算和后向计算：前向计算对输入blob进行处理，得到输出blob。后向计算对输出blob的diff部分做处理得到输入blob的diff。</p>
<p>注意了，caffe中的<code>top</code> 和<code>bottom</code>都是<code>vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt;&gt;&gt;</code>，<font color="red">其元素为多个指向blob的指针。而并非值一个blob对象！</font></p>
<p>既然<code>blobs_</code>是训练参数，那么向该层输入的数据在哪？？？</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span> &#123;</span></span><br><span class="line"><span class="comment">// 先看类成员属性，以下划线结尾的变量，对类内可见，对该类之外不可见。</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="comment">// 保存该层参数的 protobuf</span></span><br><span class="line">    <span class="comment">// LayerParameter类声明在这里：</span></span><br><span class="line">    <span class="comment">// .build_release/src/caffe/proto/caffe.pb.h 这是编译后自动生成的文件</span></span><br><span class="line">    LayerParameter layer_param_;</span><br><span class="line">    <span class="comment">// 这层所处是哪个阶段，train OR test</span></span><br><span class="line">    Phase phase_;</span><br><span class="line">    <span class="comment">// 多个Blob指针，指向这层内部的学习参数，w，b</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;&gt; blobs_;</span><br><span class="line">    <span class="comment">// 是否计算对应参数的误差梯度</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; param_propagate_down_;</span><br><span class="line">    <span class="comment">// 目标函数中是否每个Top blob都有非零权值</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;Dtype&gt; loss_;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 上述三个vector的长度一样！</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造，从LayerParameter对象中加载参数</span></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Layer</span><span class="params">(<span class="keyword">const</span> LayerParameter&amp; param)</span></span></span><br><span class="line">    : layer_param_(param) &#123;</span><br><span class="line">        <span class="comment">// 设置阶段</span></span><br><span class="line">        phase_ = param.phase();</span><br><span class="line">        <span class="comment">// 如果有数据，则设置blob，具体是从磁盘读取到这个Layer的blob</span></span><br><span class="line">        <span class="keyword">if</span> (layer_param_.blobs_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// WHY blob的个数resize到blob的大小 ？？？</span></span><br><span class="line">            blobs_.resize(layer_param_.blobs_size()); </span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; layer_param_.blobs_size(); ++i) &#123;</span><br><span class="line">                <span class="comment">// 这个blob[i]指针接管一个新的blob指针</span></span><br><span class="line">                blobs_[i].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;());</span><br><span class="line">                <span class="comment">// 从磁盘读取数据到当前blob[i]</span></span><br><span class="line">                blobs_[i]-&gt;FromProto(layer_param_.blobs(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">virtual</span> ~Layer() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 不能覆盖这个方法，提供4个功能</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 检查bottom 和topblob是否满足这层的要求</span></span><br><span class="line">        CheckBlobCounts(bottom, top);</span><br><span class="line">        <span class="comment">//2. 调用自己实现的层配置函数</span></span><br><span class="line">        LayerSetUp(bottom, top);</span><br><span class="line">        <span class="comment">//3. 对输出blob 变形</span></span><br><span class="line">        Reshape(bottom, top);</span><br><span class="line">        <span class="comment">//4. </span></span><br><span class="line">        SetLossWeights(top);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 层的相关配置，由自己实现（子类实现）</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">LayerSetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自己实现（子类实现），调整top blob和中间buffer的形状</span></span><br><span class="line">    <span class="comment">// 以适应bottom blob的形状。纯虚函数</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 根据bottom blob 计算top blob和loss。返回这一层的总loss。</span></span><br><span class="line">    <span class="comment">// 该函数调用Forward_cpu()和Forward_gpu()执行真正的计算；</span></span><br><span class="line">    <span class="comment">// 如果该层有非零权值，则计算并返回loss。</span></span><br><span class="line">    <span class="comment">// 在子类实现Forward_cpu()和Forward_gpu()。毕竟不同层的计算方式不同。</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">Forward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反向传播计算，给定top 的梯度，计算bottom的梯度。</span></span><br><span class="line">    <span class="comment">// 参数中top 中含有上层来的梯度误差diff</span></span><br><span class="line">    <span class="comment">// propagate_down 其长度与bottom长度相同，</span></span><br><span class="line">    <span class="comment">// 其中每一个值表示是否将对应的误差传到对应的bottom。</span></span><br><span class="line">    <span class="comment">// bottom 输入blobs，经过Backward()计算后， 其diff 保存误差梯度，</span></span><br><span class="line">    <span class="comment">// 实际上的后向传播的执行由Backward_cpu() 和 Backward_gpu()实现。</span></span><br><span class="line">    <span class="comment">// 子类需要实现Backward_cpu() 和 Backward_gpu()</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">Backward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回这层中可训练参数 blob向量</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blobs() &#123;</span><br><span class="line">        <span class="keyword">return</span> blobs_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该层的 曾参数（由protobuff提供）</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> LayerParameter&amp; <span class="title">layer_param</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; </span><br><span class="line">        <span class="keyword">return</span> layer_param_; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将该层层参数写入 protobuff</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">ToProto</span><span class="params">(LayerParameter* param, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回top指定index的blob的loss值</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">loss</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> top_index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (loss_.size() &gt; top_index) ? loss_[top_index] : Dtype(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为top指定index的blob的loss 设值</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">set_loss</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> top_index, <span class="keyword">const</span> Dtype value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (loss_.size() &lt;= top_index) &#123;</span><br><span class="line">            loss_.resize(top_index + <span class="number">1</span>, Dtype(<span class="number">0</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        loss_[top_index] = value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该层的类型</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">""</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该层需要输入或输出的blobs数，由子类实现。</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">ExactNumBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MaxBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">ExactNumTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MaxTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 该层的top blobs个数和bottom blobs个数是否相同。子类实现</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">EqualNumBottomTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 是否需要自动创造匿名top blobs，</span></span><br><span class="line">    <span class="comment">// 如果返回true，Net::Init()会创建足够多的匿名top blobs来满足</span></span><br><span class="line">    <span class="comment">// ExactNumTopBlobs() 或MinTopBlobs().</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">AutoTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">AllowForceBackward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> bottom_index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">param_propagate_down</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (param_propagate_down_.size() &gt; param_id) ?</span><br><span class="line">        param_propagate_down_[param_id] : <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">set_param_propagate_down</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id, <span class="keyword">const</span> <span class="keyword">bool</span> value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (param_propagate_down_.size() &lt;= param_id) &#123;</span><br><span class="line">            param_propagate_down_.resize(param_id + <span class="number">1</span>, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        param_propagate_down_[param_id] = value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cpu和gpu 前行计算，其具体实现在具体的层中，将一直看到</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// LOG(WARNING) &lt;&lt; "Using CPU code as backup.";</span></span><br><span class="line">        <span class="keyword">return</span> Forward_cpu(bottom, top);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// LOG(WARNING) &lt;&lt; "Using CPU code as backup.";</span></span><br><span class="line">        Backward_cpu(top, propagate_down, bottom);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called by the parent Layer's SetUp to check that the number of bottom</span></span><br><span class="line"><span class="comment">     * and top Blobs provided as input match the expected numbers specified by</span></span><br><span class="line"><span class="comment">     * the &#123;ExactNum,Min,Max&#125;&#123;Bottom,Top&#125;Blobs() functions.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 最后两个函数由父类Layer 的SetUp()函数调用</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">CheckBlobCounts</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">                                <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (ExactNumBottomBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_EQ(ExactNumBottomBlobs(), bottom.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer takes "</span> &lt;&lt; ExactNumBottomBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" bottom blob(s) as input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MinBottomBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_LE(MinBottomBlobs(), bottom.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer takes at least "</span> &lt;&lt; MinBottomBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" bottom blob(s) as input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MaxBottomBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_GE(MaxBottomBlobs(), bottom.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer takes at most "</span> &lt;&lt; MaxBottomBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" bottom blob(s) as input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ExactNumTopBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_EQ(ExactNumTopBlobs(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces "</span> &lt;&lt; ExactNumTopBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" top blob(s) as output."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MinTopBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_LE(MinTopBlobs(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces at least "</span> &lt;&lt; MinTopBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" top blob(s) as output."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MaxTopBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_GE(MaxTopBlobs(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces at most "</span> &lt;&lt; MaxTopBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" top blob(s) as output."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (EqualNumBottomTopBlobs()) &#123;</span><br><span class="line">            CHECK_EQ(bottom.size(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces one top blob as output for each "</span></span><br><span class="line">                &lt;&lt; <span class="string">"bottom blob input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called by SetUp to initialize the weights associated with any top blobs in</span></span><br><span class="line"><span class="comment">     * the loss function. Store non-zero loss weights in the diff blob.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">SetLossWeights</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_loss_weights = layer_param_.loss_weight_size();</span><br><span class="line">        <span class="keyword">if</span> (num_loss_weights) &#123;</span><br><span class="line">            CHECK_EQ(top.size(), num_loss_weights) &lt;&lt; <span class="string">"loss_weight must be "</span></span><br><span class="line">                <span class="string">"unspecified or specified once per top blob."</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</span><br><span class="line">                <span class="keyword">const</span> Dtype loss_weight = layer_param_.loss_weight(top_id);</span><br><span class="line">                <span class="keyword">if</span> (loss_weight == Dtype(<span class="number">0</span>)) &#123; <span class="keyword">continue</span>; &#125;</span><br><span class="line">                <span class="keyword">this</span>-&gt;set_loss(top_id, loss_weight);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> count = top[top_id]-&gt;count();</span><br><span class="line">                Dtype* loss_multiplier = top[top_id]-&gt;mutable_cpu_diff();</span><br><span class="line">                caffe_set(count, loss_weight, loss_multiplier);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    DISABLE_COPY_AND_ASSIGN(Layer);</span><br><span class="line">&#125;;  <span class="comment">// class Layer</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/" data-id="ckb581vn200007ofzbv2f7tac" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-blob-cpp文件" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/" class="article-date">
  <time datetime="2020-06-06T15:51:19.000Z" itemprop="datePublished">2020-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/">caffe-blob.cpp文件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>定义Blob类中每个成员函数，<code>void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp;)</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape) &#123;</span><br><span class="line">    <span class="comment">// 确保Shape中元素少于Blob允许的最大维度数</span></span><br><span class="line">    CHECK_LE(shape.size(), kMaxBlobAxes);</span><br><span class="line">    count_ = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 调用vector.resize(), </span></span><br><span class="line">    shape_.resize(shape.size());</span><br><span class="line">    <span class="comment">// shape_data_为空指针，或这个指针多指向的内存小于变形后的大小，则：</span></span><br><span class="line">    <span class="keyword">if</span> (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)) &#123;</span><br><span class="line">        <span class="comment">// 让shape_data_接管一个新的指针，它指向一块新的内存</span></span><br><span class="line">        shape_data_.reset(<span class="keyword">new</span> SyncedMemory(shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开辟临时空间向其传入当前CPU数据，返回一个指针shape_data</span></span><br><span class="line">    <span class="keyword">int</span>* shape_data = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>*&gt;(shape_data_-&gt;mutable_cpu_data());</span><br><span class="line">    <span class="comment">// 遍历Shape中每一元素（每一维度）</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.size(); ++i) &#123;</span><br><span class="line">        CHECK_GE(shape[i], <span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 只要这个Blob中的count_(元素个数)不是0，即这个Blob存在元素，则？？？</span></span><br><span class="line">        <span class="comment">// 若Blob中不存在元素，则？？？。</span></span><br><span class="line">        <span class="keyword">if</span> (count_ != <span class="number">0</span>) &#123; </span><br><span class="line">            CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; <span class="string">"blob size exceeds INT_MAX"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 更新count_, 记录变形后的这个Blob元素个数</span></span><br><span class="line">        count_ *= shape[i];</span><br><span class="line">        <span class="comment">// 更新shape_，用新的这个维度值替换旧的</span></span><br><span class="line">        shape_[i] = shape[i];</span><br><span class="line">        <span class="comment">// ????????</span></span><br><span class="line">        shape_data[i] = shape[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当数据个数超过Blob容量，怎更新容量大小</span></span><br><span class="line">    <span class="keyword">if</span> (count_ &gt; capacity_) &#123;</span><br><span class="line">        capacity_ = count_;</span><br><span class="line">        <span class="comment">// 让data_和diff_分别接管一块新的内存</span></span><br><span class="line">        data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));</span><br><span class="line">        diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个函数的功能，当新的reshape后所有维度元素个数N之和小于原来元素个数M，只取M的前N个元素；<br>当等于时，元素不变。当大于时，所有元素变为零。不过正常的使用是元素个数相同间的reshape。</p>
<p>有了上述的方法，下面的方法调用上面的方法：<br><code>void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp;)</code><br><code>void Blob&lt;Dtype&gt;::Reshape(int,int,int,int)</code>：</p>
<p>读取Blob中已有的cpu_data</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data() <span class="keyword">const</span> &#123;</span><br><span class="line">  CHECK(data_);</span><br><span class="line">  <span class="comment">// 成员data_为共享指针，其指向的存储空间含有cpu_data</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为Blob设置cpu_data</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) &#123;</span><br><span class="line">    CHECK(data);</span><br><span class="line">    <span class="comment">// Make sure CPU and GPU sizes remain equal</span></span><br><span class="line">    <span class="keyword">size_t</span> size = count_ * <span class="keyword">sizeof</span>(Dtype);</span><br><span class="line">    <span class="keyword">if</span> (data_-&gt;size() != size) &#123;</span><br><span class="line">        data_.reset(<span class="keyword">new</span> SyncedMemory(size));</span><br><span class="line">        diff_.reset(<span class="keyword">new</span> SyncedMemory(size));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 用传入参数'data'设置data_成员变量</span></span><br><span class="line">    data_-&gt;set_cpu_data(data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可写访问CPU_data</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() &#123;</span><br><span class="line">    CHECK(data_);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其他关于访问，设置cpu，gpu的 data和diff都类似，略：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// const只读访问</span></span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// 只读访问GPU数据形状</span></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="title">gpu_shape</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">gpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">gpu_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// mutable读写访问</span></span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_diff</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_gpu_diff</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 设置cpu和gpu数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(Dtype* data)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(Dtype* data)</span></span>;</span><br></pre></td></tr></table></figure>

<p>共享BLob数据data：共享diff与下面代码一样。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareData(<span class="keyword">const</span> Blob&amp; other) &#123;</span><br><span class="line">  CHECK_EQ(count_, other.count());</span><br><span class="line">  <span class="comment">// 将这个BLob的data_设为与other一样的值，共享</span></span><br><span class="line">  data_ = other.data();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行Updata():<br>其中：<code>caffe_axpy</code>在<code>src/caffe/util/math_functions.cpp</code>中，<br><code>caffe_gpu_axpy</code>在<code>src/caffe/util/math_functions.cu</code>中。<br>这两个操作实际是：<code>data_[i] = data_[i] - diff_[i], 其中i=0,1,2,3,4...</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Update() &#123;</span><br><span class="line">    <span class="comment">// 执行计算取决于数据在哪</span></span><br><span class="line">    <span class="keyword">switch</span> (data_-&gt;head()) &#123;    <span class="comment">// 获得当前SyncedMemory对象状态</span></span><br><span class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:  <span class="comment">// 如果在cpu则</span></span><br><span class="line">        <span class="comment">// 执行在CPU上的计算</span></span><br><span class="line">        caffe_axpy&lt;Dtype&gt;(count_, </span><br><span class="line">            Dtype(<span class="number">-1</span>),</span><br><span class="line">            <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;cpu_data()),</span><br><span class="line">            <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</span><br><span class="line">    <span class="keyword">case</span> SyncedMemory::SYNCED:</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">    <span class="comment">// 若使用GPU，则执行在GPU上update()</span></span><br><span class="line">    caffe_gpu_axpy&lt;Dtype&gt;(count_, </span><br><span class="line">        Dtype(<span class="number">-1</span>),</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;gpu_data()),</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    NO_GPU;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        LOG(FATAL) &lt;&lt; <span class="string">"Syncedmem not initialized."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其他类似的函数结构相同，只是核心操作不同，略</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算l1范数 元素和</span></span><br><span class="line"><span class="function">Dtype <span class="title">asum_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype <span class="title">asum_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// 计算l2范数 元素平方和</span></span><br><span class="line"><span class="function">Dtype <span class="title">sumsq_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype <span class="title">sumsq_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// 元素可以一个常数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_data</span><span class="params">(Dtype scale_factor)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_diff</span><span class="params">(Dtype scale_factor)</span></span>;</span><br></pre></td></tr></table></figure>

<p>共享数据很直接，略</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 共享other这个Blob的data_和diff_</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShareData</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShareDiff</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br></pre></td></tr></table></figure>

<p>从其他blob拷贝数据到当前Blob：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, </span><br><span class="line">    <span class="keyword">bool</span> reshape) &#123;</span><br><span class="line">    <span class="comment">// 必要时reshape</span></span><br><span class="line">    <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</span><br><span class="line">        <span class="keyword">if</span> (reshape) &#123;</span><br><span class="line">            ReshapeLike(source);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG(FATAL) &lt;&lt; <span class="string">"Trying to copy blobs of different sizes."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">switch</span> (Caffe::mode()) &#123;</span><br><span class="line">        <span class="comment">// 如果是GPU模式就拷贝GPU数据</span></span><br><span class="line">        <span class="keyword">case</span> Caffe::GPU:</span><br><span class="line">            <span class="keyword">if</span> (copy_diff) &#123;</span><br><span class="line">                caffe_copy(count_, source.gpu_diff(),</span><br><span class="line">                    <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                caffe_copy(count_, source.gpu_data(),</span><br><span class="line">                    <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        <span class="comment">// 如果是CPU模式就拷贝CPU数据</span></span><br><span class="line">        <span class="keyword">case</span> Caffe::CPU:</span><br><span class="line">            <span class="keyword">if</span> (copy_diff) &#123;</span><br><span class="line">            caffe_copy(count_, source.cpu_diff(),</span><br><span class="line">                <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            caffe_copy(count_, source.cpu_data(),</span><br><span class="line">                <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">        LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<p>反序列化数据，将磁盘数据读入protobuff：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::FromProto(<span class="keyword">const</span> BlobProto&amp; proto, <span class="keyword">bool</span> reshape) &#123;</span><br><span class="line">    <span class="comment">// 如果需要reshape，先reshape</span></span><br><span class="line">    <span class="keyword">if</span> (reshape) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape;</span><br><span class="line">        <span class="keyword">if</span> (proto.has_num() || proto.has_channels() ||</span><br><span class="line">            proto.has_height() || proto.has_width()) &#123;</span><br><span class="line">            <span class="comment">// Using deprecated 4D Blob dimensions --</span></span><br><span class="line">            <span class="comment">// shape is (num, channels, height, width).</span></span><br><span class="line">            shape.resize(<span class="number">4</span>);</span><br><span class="line">            shape[<span class="number">0</span>] = proto.num();</span><br><span class="line">            shape[<span class="number">1</span>] = proto.channels();</span><br><span class="line">            shape[<span class="number">2</span>] = proto.height();</span><br><span class="line">            shape[<span class="number">3</span>] = proto.width();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            shape.resize(proto.shape().dim_size());</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; proto.shape().dim_size(); ++i) &#123;</span><br><span class="line">                shape[i] = proto.shape().dim(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        Reshape(shape); <span class="comment">// 按照维度信息变换</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        CHECK(ShapeEquals(proto)) &lt;&lt; <span class="string">"shape mismatch (reshape not set)"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 从protobuff拷贝数据到当前Blob：</span></span><br><span class="line">    <span class="comment">// 获取当前Blob的mutable_cpu_data的地址data_vec,</span></span><br><span class="line">    <span class="comment">// 将protobuff中double或float数据 data写入到地址data_vec  </span></span><br><span class="line">    <span class="comment">// diff 与data一样：</span></span><br><span class="line">    Dtype* data_vec = mutable_cpu_data();</span><br><span class="line">    <span class="keyword">if</span> (proto.double_data_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        CHECK_EQ(count_, proto.double_data_size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            data_vec[i] = proto.double_data(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    CHECK_EQ(count_, proto.data_size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            data_vec[i] = proto.data(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (proto.double_diff_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        CHECK_EQ(count_, proto.double_diff_size());</span><br><span class="line">        Dtype* diff_vec = mutable_cpu_diff();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            diff_vec[i] = proto.double_diff(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (proto.diff_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        CHECK_EQ(count_, proto.diff_size());</span><br><span class="line">        Dtype* diff_vec = mutable_cpu_diff();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            diff_vec[i] = proto.diff(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将数据序列化（写入磁盘）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;<span class="keyword">double</span>&gt;::ToProto(BlobProto* proto, <span class="keyword">bool</span> write_diff) <span class="keyword">const</span> &#123;</span><br><span class="line">    <span class="comment">// 重置protobuff维度，清理原有的数据 并写入新的cpu_data数据</span></span><br><span class="line">    proto-&gt;clear_shape();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape_.size(); ++i) &#123;</span><br><span class="line">        proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    proto-&gt;clear_double_data();</span><br><span class="line">    proto-&gt;clear_double_diff();</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span>* data_vec = cpu_data();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">        proto-&gt;add_double_data(data_vec[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果需要写入diff，也要写入cpu_diff数据</span></span><br><span class="line">    <span class="keyword">if</span> (write_diff) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span>* diff_vec = cpu_diff();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            proto-&gt;add_double_diff(diff_vec[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/" data-id="ckb3x32no0000bcfze29lg327" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-SyncedMemory" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/06/caffe-SyncedMemory/" class="article-date">
  <time datetime="2020-06-06T15:46:51.000Z" itemprop="datePublished">2020-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/06/caffe-SyncedMemory/">caffe-SyncedMemory</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>结构：caffe命名空间中包含两个inline函数和类SyncedMemory的声明。 </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CAFFE_SYNCEDMEM_HPP_</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CAFFE_SYNCEDMEM_HPP_</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MKL</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"mkl.h"</span>  <span class="comment">// 使用intel数学运算库MKL</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/common.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> caffe &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用cudaMallocHost()方法从Host内存开辟空间。从这里开辟空间对于单个GPU性能提升不明显，但是对于大的模型在多GPU上的训练，有较大的性能提升。</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeMallocHost</span><span class="params">(<span class="keyword">void</span>** ptr, <span class="keyword">size_t</span> size, <span class="keyword">bool</span>* use_cuda)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY <span class="comment">// 如果没有指明CPU_ONLY，则：</span></span></span><br><span class="line">    <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123;</span><br><span class="line">        CUDA_CHECK(cudaMallocHost(ptr, size));</span><br><span class="line">        *use_cuda = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MKL <span class="comment">// 如果使用MKL，则：</span></span></span><br><span class="line">    *ptr = mkl_malloc(size ? size:<span class="number">1</span>, <span class="number">64</span>);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span>  <span class="comment">// 否则：</span></span></span><br><span class="line">    *ptr = <span class="built_in">malloc</span>(size);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    *use_cuda = <span class="literal">false</span>;</span><br><span class="line">    CHECK(*ptr) &lt;&lt; <span class="string">"host allocation of size "</span> &lt;&lt; size &lt;&lt; <span class="string">" failed"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与开辟空间对应，用于释放内存。</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeFreeHost</span><span class="params">(<span class="keyword">void</span>* ptr, <span class="keyword">bool</span> use_cuda)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">    <span class="keyword">if</span> (use_cuda) &#123;</span><br><span class="line">        CUDA_CHECK(cudaFreeHost(ptr));</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MKL</span></span><br><span class="line">    mkl_free(ptr);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">free</span>(ptr);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 负责Host和Device内存的分配，和两者间内存同步</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SyncedMemory</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造和析构函数</span></span><br><span class="line">    SyncedMemory();</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">SyncedMemory</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line">    ~SyncedMemory();</span><br><span class="line">    <span class="comment">// 只读方式只读数据</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 设置数据</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</span><br><span class="line">    <span class="comment">// 读写方式读取数据</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 状态变量：未初始化，CPU数据有效，GPU数据有效，已同步</span></span><br><span class="line">    <span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, </span><br><span class="line">                        HEAD_AT_CPU, </span><br><span class="line">                        HEAD_AT_GPU, </span><br><span class="line">                        SYNCED &#125;;</span><br><span class="line">    <span class="comment">// 获取当前状态变量值</span></span><br><span class="line">    <span class="function">SyncedHead <span class="title">head</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> head_; &#125;</span><br><span class="line">    <span class="comment">// 返回当前存储空间的大小</span></span><br><span class="line">    <span class="keyword">size_t</span> size() <span class="keyword">const</span> &#123; <span class="keyword">return</span> size_; &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">async_gpu_push</span><span class="params">(<span class="keyword">const</span> cudaStream_t&amp; stream)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">check_device</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">to_cpu</span><span class="params">()</span></span>;  <span class="comment">// 同步数据到host</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">to_gpu</span><span class="params">()</span></span>;  <span class="comment">// 同步数据到Device</span></span><br><span class="line">    <span class="keyword">void</span>* cpu_ptr_;  <span class="comment">// 位于Host的数据指针</span></span><br><span class="line">    <span class="keyword">void</span>* gpu_ptr_;  <span class="comment">// 位于Device的数据指针</span></span><br><span class="line">    <span class="keyword">size_t</span> size_;  <span class="comment">// 存储空间的大小</span></span><br><span class="line">    SyncedHead head_;  <span class="comment">// 当前状态变量</span></span><br><span class="line">    <span class="keyword">bool</span> own_cpu_data_;  <span class="comment">// 是否有cpu数据的所有权</span></span><br><span class="line">    <span class="keyword">bool</span> cpu_malloc_use_cuda_;  <span class="comment">// 是否</span></span><br><span class="line">    <span class="keyword">bool</span> own_gpu_data_;  <span class="comment">// 是否有gpu数据的所有权</span></span><br><span class="line">    <span class="keyword">int</span> device_;  <span class="comment">// 设备号</span></span><br><span class="line"></span><br><span class="line">    DISABLE_COPY_AND_ASSIGN(SyncedMemory);</span><br><span class="line">&#125;;  <span class="comment">// class SyncedMemory</span></span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace caffe</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// CAFFE_SYNCEDMEM_HPP_</span></span></span><br></pre></td></tr></table></figure>

<h1 id="补充，条件编译预编译指令"><a href="#补充，条件编译预编译指令" class="headerlink" title="补充，条件编译预编译指令"></a>补充，条件编译预编译指令</h1><p><code>#define</code><br><code>#undef</code><br><code>#if</code><br><code>#ifdef</code><br><code>#ifndef</code><br><code>#elif</code><br><code>#else</code><br><code>#endif</code><br><code>defined</code>：与#if, #elif配合使用，判断某个宏是否被定义</p>
<p>宏<code>USE_MKL</code>的定义在文件<code>include/caffe/util/mkl_alternate.hpp</code>中。<br>宏<code>CPU_ONLY</code>的定义在文件``？？？？？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/06/caffe-SyncedMemory/" data-id="ckb3tbsv90002v6fzfnpmc64r" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-Blob-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/06/caffe-Blob-2/" class="article-date">
  <time datetime="2020-06-06T10:01:02.000Z" itemprop="datePublished">2020-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/06/caffe-Blob-2/">caffe-Blob-(2)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>读Blob的头文件，注释每个方法的作用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CAFFE_BLOB_HPP_</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CAFFE_BLOB_HPP_</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/common.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/proto/caffe.pb.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/syncedmem.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Blob的最大维数 32</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> kMaxBlobAxes = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> caffe &#123;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blob</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>: </span><br><span class="line">    <span class="comment">// 构造函数</span></span><br><span class="line">    Blob(): data_(), diff_(), count_(<span class="number">0</span>), capacity_(<span class="number">0</span>) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 改变blob形状</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="keyword">int</span> height,<span class="keyword">const</span> <span class="keyword">int</span> width)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> BlobShape&amp; shape)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ReshapeLike</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印形状信息</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="built_in">string</span> <span class="title">shape_string</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="built_in">ostringstream</span> stream;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape_.size(); ++i) &#123;</span><br><span class="line">            stream &lt;&lt; shape_[i] &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        stream &lt;&lt; <span class="string">"("</span> &lt;&lt; count_ &lt;&lt; <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">return</span> stream.str();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回形状，维度相关信息</span></span><br><span class="line">    <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape() <span class="keyword">const</span> &#123; <span class="keyword">return</span> shape_; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">shape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> shape_[CanonicalAxisIndex(index)];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num_axes</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> shape_.size(); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> count_; &#125;</span><br><span class="line">    <span class="comment">// 返回制定维度间的元素个数：</span></span><br><span class="line">    <span class="comment">// 返回Blob中从start_axis到end_axis间制定维度的元素个数</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis, <span class="keyword">int</span> end_axis)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        CHECK_LE(start_axis, end_axis); <span class="comment">// 确保start_axis &lt;= end_axis</span></span><br><span class="line">        CHECK_GE(start_axis, <span class="number">0</span>);    <span class="comment">// &gt;=</span></span><br><span class="line">        CHECK_GE(end_axis, <span class="number">0</span>); </span><br><span class="line">        CHECK_LE(start_axis, num_axes());</span><br><span class="line">        CHECK_LE(end_axis, num_axes());</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = start_axis; i &lt; end_axis; ++i) &#123;</span><br><span class="line">            count *= shape(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 返回从start_axis到最后维度的元素个数</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count(start_axis, num_axes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入负索引，从后向前的访问。</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">CanonicalAxisIndex</span><span class="params">(<span class="keyword">int</span> axis_index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        CHECK_GE(axis_index, -num_axes())</span><br><span class="line">            &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</span><br><span class="line">            &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</span><br><span class="line">        CHECK_LT(axis_index, num_axes())</span><br><span class="line">            &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</span><br><span class="line">            &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</span><br><span class="line">        <span class="keyword">if</span> (axis_index &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> axis_index + num_axes();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> axis_index;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回某个维度大小</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">0</span>); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">channels</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">1</span>); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">height</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">2</span>); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">width</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">3</span>); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">LegacyShape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        CHECK_LE(num_axes(), <span class="number">4</span>)</span><br><span class="line">            &lt;&lt; <span class="string">"Cannot use l4egacy accessors on Blobs with &gt; 4 axes."</span>;</span><br><span class="line">        CHECK_LT(index, <span class="number">4</span>);</span><br><span class="line">        CHECK_GE(index, <span class="number">-4</span>);</span><br><span class="line">        <span class="keyword">if</span> (index &gt;= num_axes() || index &lt; -num_axes()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> shape(index);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据n,c,h,w计算全局Index</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c = <span class="number">0</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="keyword">int</span> h = <span class="number">0</span>,<span class="keyword">const</span> <span class="keyword">int</span> w = <span class="number">0</span>)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        CHECK_GE(n, <span class="number">0</span>);</span><br><span class="line">        CHECK_LE(n, num());</span><br><span class="line">        CHECK_GE(channels(), <span class="number">0</span>);</span><br><span class="line">        CHECK_LE(c, channels());</span><br><span class="line">        CHECK_GE(height(), <span class="number">0</span>);</span><br><span class="line">        CHECK_LE(h, height());</span><br><span class="line">        CHECK_GE(width(), <span class="number">0</span>);</span><br><span class="line">        CHECK_LE(w, width());</span><br><span class="line">        <span class="comment">// 看到了，最右边变化是最快的</span></span><br><span class="line">        <span class="keyword">return</span> ((n * channels() + c) * height() + h) * width() + w;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 也是计算全局index，只是传入的是vector</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; indices)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        CHECK_LE(indices.size(), num_axes());</span><br><span class="line">        <span class="keyword">int</span> offset = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_axes(); ++i) &#123;</span><br><span class="line">            offset *= shape(i);</span><br><span class="line">            <span class="keyword">if</span> (indices.size() &gt; i) &#123;</span><br><span class="line">                CHECK_GE(indices[i], <span class="number">0</span>);</span><br><span class="line">                CHECK_LT(indices[i], shape(i));</span><br><span class="line">                offset += indices[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拷贝一个Blob到当前Blob</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">CopyFrom</span><span class="params">(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; source, <span class="keyword">bool</span> copy_diff = <span class="literal">false</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">bool</span> reshape = <span class="literal">false</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一下4函数均是访问制定位置的数据。提供局部Index，调用offset计算全局index，后可访问</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="keyword">int</span> h,<span class="keyword">const</span> <span class="keyword">int</span> w)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cpu_data()[offset(n, c, h, w)];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> h, <span class="keyword">const</span> <span class="keyword">int</span> w)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cpu_diff()[offset(n, c, h, w)];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cpu_data()[offset(index)];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cpu_diff()[offset(index)];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取data_</span></span><br><span class="line">    <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span> &#123;</span><br><span class="line">        CHECK(data_);</span><br><span class="line">        <span class="keyword">return</span> data_;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 取diff_</span></span><br><span class="line">    <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; diff() <span class="keyword">const</span> &#123;</span><br><span class="line">        CHECK(diff_);</span><br><span class="line">        <span class="keyword">return</span> diff_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// const只读访问</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="comment">// 只读访问GPU数据形状</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="title">gpu_shape</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> Dtype* <span class="title">gpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> Dtype* <span class="title">gpu_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="comment">// mutable读写访问</span></span><br><span class="line">    <span class="function">Dtype* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">Dtype* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">Dtype* <span class="title">mutable_cpu_diff</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">Dtype* <span class="title">mutable_gpu_diff</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 设置cpu和gpu数据</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(Dtype* data)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(Dtype* data)</span></span>;</span><br><span class="line">    <span class="comment">// 执行data = data-diff操作，即学习操作</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Update</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 从磁盘读取数据到blob，反序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">FromProto</span><span class="params">(<span class="keyword">const</span> BlobProto&amp; proto, <span class="keyword">bool</span> reshape = <span class="literal">true</span>)</span></span>;</span><br><span class="line">    <span class="comment">// 保存Blob数据到磁盘，序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ToProto</span><span class="params">(BlobProto* proto, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算l1范数 元素和</span></span><br><span class="line">    <span class="function">Dtype <span class="title">asum_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function">Dtype <span class="title">asum_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="comment">// 计算l2范数 元素平方和</span></span><br><span class="line">    <span class="function">Dtype <span class="title">sumsq_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function">Dtype <span class="title">sumsq_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="comment">// 元素可以一个常数</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">scale_data</span><span class="params">(Dtype scale_factor)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">scale_diff</span><span class="params">(Dtype scale_factor)</span></span>;</span><br><span class="line">    <span class="comment">// 共享other这个Blob的data_和diff_</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ShareData</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ShareDiff</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br><span class="line">    <span class="comment">// 当前blob是否与other有相同的内容</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">ShapeEquals</span><span class="params">(<span class="keyword">const</span> BlobProto&amp; other)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="comment">// 成员属性：</span></span><br><span class="line">    <span class="comment">// 均指针，指向data，diff和shape_data的内存位置</span></span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; shape_data_;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_; </span><br><span class="line">    <span class="keyword">int</span> count_;      <span class="comment">// 当前blob的元素个数</span></span><br><span class="line">    <span class="keyword">int</span> capacity_;   <span class="comment">// blob容量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 就Blob类，禁用其copy构造函数和赋值运算符。</span></span><br><span class="line">    DISABLE_COPY_AND_ASSIGN(Blob);</span><br><span class="line">&#125;;  <span class="comment">// Blob 类结束</span></span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace caffe 结束</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// CAFFE_BLOB_HPP_</span></span></span><br></pre></td></tr></table></figure>

<p>Blob类使用到<code>syncedmem.hpp</code>，其与Host和Device内存即内存同步相关操作。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/06/caffe-Blob-2/" data-id="ckb3tbsuy0000v6fz6tf258zm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-Blob-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/04/caffe-Blob-1/" class="article-date">
  <time datetime="2020-06-04T15:10:24.000Z" itemprop="datePublished">2020-06-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/04/caffe-Blob-1/">caffe-Blob-(1)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>先看读lob.hpp, 再读Blob.cpp。</p>
<p>若要调用caffe文件，可以使用g++命令，也可以使用cmake建立一个项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">└── test_blob</span><br><span class="line">    ├── build</span><br><span class="line">    ├── CMakeLists.txt</span><br><span class="line">    └── main.cpp</span><br></pre></td></tr></table></figure>

<p>这个项目可以位于任何位置。接着编辑CMakeLists.tx，如下:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.5</span>)</span><br><span class="line"><span class="keyword">project</span>(test_blob)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">set</span>(Caffe_INCLUDE_DIRS ~/caffe-master/<span class="keyword">include</span> \</span><br><span class="line">/usr/local/cuda/<span class="keyword">include</span> ~/caffe-master/build/src)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(Caffe_LIBRARIES caffe boost_system glog)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;Caffe_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(~/caffe-master/build/lib) </span><br><span class="line"> </span><br><span class="line"><span class="keyword">add_executable</span>(test_blob main.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(test_blob <span class="variable">$&#123;Caffe_LIBRARIES&#125;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">install</span>(TARGETS test_blob RUNTIME DESTINATION bin)</span><br></pre></td></tr></table></figure>

<p>使用下面命令编译执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ rm -rf build/*</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ ./test_blob</span><br></pre></td></tr></table></figure>

<p>在代码文件头部添加基本内容：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;caffe/blob.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;  <span class="comment">// 代码中依旧加上这个namespace</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br></pre></td></tr></table></figure>

<p>之后就可以在main函数中，进行探索测试了。</p>
<p>基本地，Blob在内存中是4为数组，维度从高到低为（num，channels，height，width）。并包含data（待学习参数）和diff（增量）。包含一些基本操作。</p>
<h1 id="帮助函数"><a href="#帮助函数" class="headerlink" title="帮助函数"></a>帮助函数</h1><p>先在main.cpp外自定义了两个函数：向Blob中写入数据，从Blob读取数据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 向Blob中 写入数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">writeInto</span><span class="params">(caffe::Blob&lt;<span class="keyword">float</span>&gt;&amp; a)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 获取cpu数据data部分</span></span><br><span class="line">    <span class="keyword">float</span>* ptr = a.mutable_cpu_data(); </span><br><span class="line">    <span class="comment">// 给每个位置赋值。count()表示元素个数</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;a.count();i++)&#123;</span><br><span class="line">        ptr[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 打印blob信息 读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printBlob</span><span class="params">(<span class="keyword">const</span> caffe::Blob&lt;<span class="keyword">float</span>&gt;&amp; blob)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// .shape_string() 返回一个string，包含shape和元素个数</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Size: "</span>&lt;&lt;blob.shape_string()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (blob.shape_string() == <span class="string">"(0)"</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"No data initialized"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 从内向外打印数据</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> u=<span class="number">0</span>; u&lt;blob.num(); u++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;blob.channels(); v++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> w=<span class="number">0</span>; w&lt;blob.height(); w++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> x=<span class="number">0</span>; x&lt;blob.width(); x++)&#123;</span><br><span class="line">                    <span class="comment">// .data_at(,,,): 访问某个位置数据</span></span><br><span class="line">                    <span class="built_in">cout</span>&lt;&lt;<span class="string">"blob: "</span>&lt;&lt;u&lt;&lt;v&lt;&lt;w&lt;&lt;x&lt;&lt;<span class="string">"-&gt;"</span>&lt;&lt;blob.data_at(u,v,w,x)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 打印l1范数（绝对值之和） l2范数（平方和）</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"ASUM = "</span>&lt;&lt;blob.asum_data()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SUMQ = "</span>&lt;&lt;blob.sumsq_data()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="定义Blob"><a href="#定义Blob" class="headerlink" title="定义Blob"></a>定义Blob</h1><p>Blob.hpp中声明了构造函数，其用法见下例：</p>
<p>如果没有特殊指明，以下代码均在main.cpp中。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认构造函数</span></span><br><span class="line">caffe::Blob&lt;<span class="keyword">float</span>&gt; a;</span><br><span class="line"></span><br><span class="line"><span class="comment">//指明shape，1*2*3*4=24 共有24个数值，初始值为0</span></span><br><span class="line">caffe::Blob&lt;<span class="keyword">float</span>&gt; b(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);   </span><br><span class="line"></span><br><span class="line"><span class="comment">// 传入Shape每个维度为元素的vector</span></span><br><span class="line"><span class="keyword">int</span> shape_c[] = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>&#125;; </span><br><span class="line">caffe::Blob&lt;<span class="keyword">float</span>&gt; c(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(shape_c, shape_c+<span class="number">4</span>));</span><br><span class="line"></span><br><span class="line">writeInto(c);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将c大小从（1,3,2,4）变为（1,2,3,4）</span></span><br><span class="line">c.Reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">printBlob(c);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将c变为与相同大小</span></span><br><span class="line">c.ReshapeLike(b);</span><br><span class="line">printBlob(c);</span><br></pre></td></tr></table></figure>

<p>如果只打印最后一次数的c：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Size: 1 2 3 4 (24)</span><br><span class="line">Position: 0000-&gt; value: 0</span><br><span class="line">Position: 0001-&gt; value: 1</span><br><span class="line">Position: 0002-&gt; value: 2</span><br><span class="line">Position: 0003-&gt; value: 3</span><br><span class="line">Position: 0010-&gt; value: 4</span><br><span class="line">Position: 0011-&gt; value: 5</span><br><span class="line">Position: 0012-&gt; value: 6</span><br><span class="line">Position: 0013-&gt; value: 7</span><br><span class="line">Position: 0020-&gt; value: 8</span><br><span class="line">Position: 0021-&gt; value: 9</span><br><span class="line">Position: 0022-&gt; value: 10</span><br><span class="line">Position: 0023-&gt; value: 11</span><br><span class="line">Position: 0100-&gt; value: 12</span><br><span class="line">Position: 0101-&gt; value: 13</span><br><span class="line">Position: 0102-&gt; value: 14</span><br><span class="line">Position: 0103-&gt; value: 15</span><br><span class="line">Position: 0110-&gt; value: 16</span><br><span class="line">Position: 0111-&gt; value: 17</span><br><span class="line">Position: 0112-&gt; value: 18</span><br><span class="line">Position: 0113-&gt; value: 19</span><br><span class="line">Position: 0120-&gt; value: 20</span><br><span class="line">Position: 0121-&gt; value: 21</span><br><span class="line">Position: 0122-&gt; value: 22</span><br><span class="line">Position: 0123-&gt; value: 23</span><br><span class="line">ASUM = 276</span><br><span class="line">SUMQ = 4324</span><br></pre></td></tr></table></figure>

<p>从上可以明显看出，（num，channels，height，width）最右端变化最快，逐渐向左。</p>
<h1 id="更新参数data"><a href="#更新参数data" class="headerlink" title="更新参数data"></a>更新参数data</h1><p>更新参数由Blob的<code>Update()</code>成员函数实现。具体是<code>data=data-diff</code>。所以要测试<code>Updata()</code>，首先就需要向data和diff分别读入数据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义blob</span></span><br><span class="line">caffe::Blob&lt;<span class="keyword">float</span>&gt; a;</span><br><span class="line">a.Reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取cpu数据data部分 和diff部分</span></span><br><span class="line"><span class="keyword">float</span>* dataPtr = a.mutable_cpu_data();</span><br><span class="line"><span class="keyword">float</span>* diffPtr = a.mutable_cpu_diff();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别为data和diff写入值</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;a.count();i++)&#123;</span><br><span class="line">    dataPtr[i] = i;</span><br><span class="line">    diffPtr[i] = a.count()<span class="number">-1</span>-i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行更新操作</span></span><br><span class="line">a.Update();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印更新后的结果</span></span><br><span class="line">printBlob(a);</span><br></pre></td></tr></table></figure>

<p>更新后的data（待学习参数）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Size: 1 2 3 4 (24)</span><br><span class="line">Position: 0000-&gt; value: -23</span><br><span class="line">Position: 0001-&gt; value: -21</span><br><span class="line">Position: 0002-&gt; value: -19</span><br><span class="line">Position: 0003-&gt; value: -17</span><br><span class="line">Position: 0010-&gt; value: -15</span><br><span class="line">Position: 0011-&gt; value: -13</span><br><span class="line">Position: 0012-&gt; value: -11</span><br><span class="line">Position: 0013-&gt; value: -9</span><br><span class="line">Position: 0020-&gt; value: -7</span><br><span class="line">Position: 0021-&gt; value: -5</span><br><span class="line">Position: 0022-&gt; value: -3</span><br><span class="line">Position: 0023-&gt; value: -1</span><br><span class="line">Position: 0100-&gt; value: 1</span><br><span class="line">Position: 0101-&gt; value: 3</span><br><span class="line">Position: 0102-&gt; value: 5</span><br><span class="line">Position: 0103-&gt; value: 7</span><br><span class="line">Position: 0110-&gt; value: 9</span><br><span class="line">Position: 0111-&gt; value: 11</span><br><span class="line">Position: 0112-&gt; value: 13</span><br><span class="line">Position: 0113-&gt; value: 15</span><br><span class="line">Position: 0120-&gt; value: 17</span><br><span class="line">Position: 0121-&gt; value: 19</span><br><span class="line">Position: 0122-&gt; value: 21</span><br><span class="line">Position: 0123-&gt; value: 23</span><br><span class="line">ASUM = 288</span><br><span class="line">SUMQ = 4600</span><br></pre></td></tr></table></figure>

<p>由结果可看出，其实计算的是<code>data=data-diff</code>，<font color="red" size="4">模型就是正向传播求diff，反向传播更新data</font>。</p>
<h1 id="保存Blob数据到磁盘，或从磁盘再如数据到Blob"><a href="#保存Blob数据到磁盘，或从磁盘再如数据到Blob" class="headerlink" title="保存Blob数据到磁盘，或从磁盘再如数据到Blob"></a>保存Blob数据到磁盘，或从磁盘再如数据到Blob</h1><p>从Blob写入protobuff：<code>.ToProto()</code>；从protobuff写入Blob：<code>.FromProto</code>。</p>
<p>需要添加头文件<code>#include &lt;caffe/util/io.hpp&gt;</code>从而调用函数<code>WriteProtoToBinaryFile()</code>和<code>ReadProtoFromBinaryFileOrDie()</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//g++编译时加上 -lglog -lboost_system</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实例化一个BlobProtot对象</span></span><br><span class="line">caffe::BlobProto bp;  </span><br><span class="line"><span class="comment">// 将a序列化，包括diff，写入protobuff</span></span><br><span class="line">a.ToProto(&amp;bp, <span class="literal">true</span>); </span><br><span class="line"><span class="comment">// 将bp对象写入磁盘文件“a.blob”</span></span><br><span class="line">WriteProtoToBinaryFile(bp, <span class="string">"a.blob"</span>); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">caffe::BlobProto bp2;</span><br><span class="line"><span class="comment">// 将磁盘文件“a.blob”内容读入protobuff对象bp2</span></span><br><span class="line">ReadProtoFromBinaryFileOrDie(<span class="string">"a.blob"</span>, &amp;bp2);</span><br><span class="line">caffe::Blob&lt;<span class="keyword">float</span>&gt; b;</span><br><span class="line"><span class="comment">// 将protobuff中内容 写入b</span></span><br><span class="line">b.FromProto(bp2, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">printBlob(b);</span><br></pre></td></tr></table></figure>
<p>上述过程从读取Blob，写入磁盘，后从磁盘读取文件，写入里一个Blob。这对于加载，保存模型参数（权值）很实用。</p>
<hr>
<p>接下来从caffe.proto文件，读关于Blob的描述。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/04/caffe-Blob-1/" data-id="ckb0x3j440000tvfz2wl24zk6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-所使用的数据格式" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/04/caffe-%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/" class="article-date">
  <time datetime="2020-06-04T03:12:30.000Z" itemprop="datePublished">2020-06-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/04/caffe-%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/">caffe-所使用的数据格式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="caffe为什么使用LMDB和levelDB，而不是直接使用原始数据"><a href="#caffe为什么使用LMDB和levelDB，而不是直接使用原始数据" class="headerlink" title="caffe为什么使用LMDB和levelDB，而不是直接使用原始数据"></a>caffe为什么使用LMDB和levelDB，而不是直接使用原始数据</h1><p>第一，原始数据的类型，种类很多，如二进制文件，.npy文件，多种图像可是文件等等，不可能使用统一的代码来读取，所以转换成统一的格式可以简化数据读取层的实现。具体说是将不同类型的数据存储为<code>key-value</code>的对应关系。便于caffe <code>DataLayer</code>获取这些数据。</p>
<p>第二，使用LMDB和LEVELDB可以提高磁盘的IO利用率。</p>
<p>较新的caffe，都使用<code>LMDB</code>，<code>LEVELDB</code>是caffe早起使用的。</p>
<h1 id="什么是protobuff"><a href="#什么是protobuff" class="headerlink" title="什么是protobuff"></a>什么是protobuff</h1><p>ProtoBuff是一种实现内存和其他存储介质数据交换的协议。在caffe中protobuff用于解析<code>.prototxt</code>文件，包括超参数设置，和模型结构的定义。protobuff将<code>.prototxt</code>文件中的配置参数按照<code>caffe.proto</code>的协议解析并加载到内存变量<code>Caffe::SolverParameter</code>对象中。自己可以尝试对一个<code>solver.prototxt</code>文件按照<code>caffe.proto</code>解析出参数.</p>
<h1 id="什么是HDF5"><a href="#什么是HDF5" class="headerlink" title="什么是HDF5"></a>什么是HDF5</h1><p>HDF5是一种数据格式，同时还有处理这种数据格式的同意函数库。</p>
<h1 id="HDF5-LMDB-ProtoBuff分别在什么时候使用"><a href="#HDF5-LMDB-ProtoBuff分别在什么时候使用" class="headerlink" title="HDF5, LMDB, ProtoBuff分别在什么时候使用"></a>HDF5, LMDB, ProtoBuff分别在什么时候使用</h1><p>caffe 模型的定义和超参数的定义，使用ProtoBuff（默认），可以看到相应的文件都是以<code>.prototxt</code>为后缀，也可以使用HDF5。而caffe所处理的数据，要转换成LMDB。</p>
<p>在caffe官方文档中，由关于data layer的描述：数据可以是来自高效的LMDB，levelDB数据库，可以使来自内存，或者来自HDF4或通用的图像格式的数据（当效率不是很重要时）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/04/caffe-%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/" data-id="ckb07i8ne0000lofz6d850jmp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-命令行与python接口" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/03/caffe-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8Epython%E6%8E%A5%E5%8F%A3/" class="article-date">
  <time datetime="2020-06-03T08:07:58.000Z" itemprop="datePublished">2020-06-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/03/caffe-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8Epython%E6%8E%A5%E5%8F%A3/">caffe 命令行与python接口</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="命令行接口-cmdcaffe"><a href="#命令行接口-cmdcaffe" class="headerlink" title="命令行接口 cmdcaffe"></a>命令行接口 cmdcaffe</h1><p>caffe经过编译后才会生成对应的工具，这个工具在目录<code>caffe-ROOT/build/tools</code>中，在此路目录中可用的命令有：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./caffe train           <span class="comment">#train or finetune a model</span></span><br><span class="line">./caffe <span class="built_in">test</span>            <span class="comment">#score a model</span></span><br><span class="line">./caffe device_query    <span class="comment">#show GPU diagnostic information</span></span><br><span class="line">./caffe time            <span class="comment">#benchmark model execution time</span></span><br></pre></td></tr></table></figure>

<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>caffe提供三种训练方式。</p>
<ol>
<li><p>从头开始训练模型。需要提供<code>.prototxt</code>配置文件的路径，如：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练，默认使用CPU</span></span><br><span class="line">./build/tools/caffe train -solver examples/mnist/lenet_solver.prototxt</span><br><span class="line"><span class="comment"># 使用编号为2 的GPU训练</span></span><br><span class="line">./build/tools/caffe train -solver examples/mnist/lenet_solver.prototxt -gpu 2</span><br></pre></td></tr></table></figure>
</li>
<li><p>从snapshot中恢复训练。需要提供<code>.solverstate</code>文件路径</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提供 -snapshot继续训练</span></span><br><span class="line">./build/tools/caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate</span><br></pre></td></tr></table></figure>

<p> <font color="orange" size="4">如果最初设定的最大训练次数不够的话</font>，可以在配置文件<code>lenet_prototxt.solver</code>中修改<code>max_iter: 10000</code>，比如增加此时为20000.</p>
</li>
<li><p>使用预训练模型微调(迁移学习)。需要提供<code>.caffemodel</code>文件路径</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指明 -weights 关键字，提供预训练模型</span></span><br><span class="line">./build/tools/caffe train -solver examples/finetuning_on_flickr_style/solver.prototxt -weights models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel</span><br></pre></td></tr></table></figure>

<p> 这里由完整的微调例子<code>examples/finetuning_on_flickr_style</code></p>
</li>
</ol>
<h3 id="多GPU并行"><a href="#多GPU并行" class="headerlink" title="多GPU并行"></a>多GPU并行</h3><p>在<code>-gpu</code>后指定要使用的GPU编号，如<code>-gpu 0,1,2,3</code>，表示使用4个GPU并行计算。使用多GPU时，相同的网络配置会被分配到每一个gpu，每一个GPU所处理数据的batch_size相同，所以，整体并行处理的数据量是<code>batch_size*4</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用可用的所有GPU设备</span></span><br><span class="line">caffe train -solver examples/mnist/lenet_solver.prototxt -gpu all</span><br></pre></td></tr></table></figure>

<h3 id="检查GPU"><a href="#检查GPU" class="headerlink" title="检查GPU"></a>检查GPU</h3><p>使用如下命令检查指定GPU是否正常工作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe device_query -gpu 0</span><br></pre></td></tr></table></figure>
<p>返回0号GPU的硬件信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">I0603 16:40:28.905443 13455 caffe.cpp:138] Querying GPUs 0</span><br><span class="line">I0603 16:40:28.927069 13455 common.cpp:178] Device id:                     0</span><br><span class="line">I0603 16:40:28.927090 13455 common.cpp:179] Major revision number:         6</span><br><span class="line">I0603 16:40:28.927093 13455 common.cpp:180] Minor revision number:         1</span><br><span class="line">I0603 16:40:28.927096 13455 common.cpp:181] Name:                          GeForce GTX 1050</span><br><span class="line">I0603 16:40:28.927099 13455 common.cpp:182] Total global memory:           2099904512</span><br><span class="line">I0603 16:40:28.927103 13455 common.cpp:183] Total shared memory per block: 49152</span><br><span class="line">I0603 16:40:28.927106 13455 common.cpp:184] Total registers per block:     65536</span><br><span class="line">I0603 16:40:28.927109 13455 common.cpp:185] Warp size:                     32</span><br><span class="line">I0603 16:40:28.927112 13455 common.cpp:186] Maximum memory pitch:          2147483647</span><br><span class="line">I0603 16:40:28.927115 13455 common.cpp:187] Maximum threads per block:     1024</span><br><span class="line">I0603 16:40:28.927119 13455 common.cpp:188] Maximum dimension of block:    1024, 1024, 64</span><br><span class="line">I0603 16:40:28.927122 13455 common.cpp:191] Maximum dimension of grid:     2147483647, 65535, 65535</span><br><span class="line">I0603 16:40:28.927125 13455 common.cpp:194] Clock rate:                    1493000</span><br><span class="line">I0603 16:40:28.927129 13455 common.cpp:195] Total constant memory:         65536</span><br><span class="line">I0603 16:40:28.927151 13455 common.cpp:196] Texture alignment:             512</span><br><span class="line">I0603 16:40:28.927155 13455 common.cpp:197] Concurrent copy and execution: Yes</span><br><span class="line">I0603 16:40:28.927160 13455 common.cpp:199] Number of multiprocessors:     5</span><br><span class="line">I0603 16:40:28.927183 13455 common.cpp:200] Kernel execution timeout:      Yes</span><br></pre></td></tr></table></figure>

<h3 id="准确度测试"><a href="#准确度测试" class="headerlink" title="准确度测试"></a>准确度测试</h3><p>测试会给出模型的每一batch的loss和accuracy以及整体平均的loss和accuracy。<font color="red"><code>test</code>表示只进行forward计算，没有backward。即推理，而非训练</font>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe <span class="built_in">test</span> -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 100</span><br></pre></td></tr></table></figure>

<p>在<code>lenet_train_test.prototxt</code>所定义的模型结构上，使用模型<code>lenet_iter_10000.caffemodel</code>，对测试样本执行100次iteration。batch_size为100，所以iteration×batch_size=10000，覆盖了所有的测试样本<font color="orange">这个测试数据在哪??</font></p>
<h3 id="时间测试"><a href="#时间测试" class="headerlink" title="时间测试"></a>时间测试</h3><p>指明<code>./build/tools/caffe time</code>测试模型，输出每一层的前先计算后向计算的时间。</p>
<ol>
<li><p>下面为lenet计时cpu计算10次迭代。（默认测试50次迭代）</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10</span><br></pre></td></tr></table></figure>
<p> 结果：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">I0603 17:30:35.768501 15346 caffe.cpp:365] *** Benchmark begins ***</span><br><span class="line">I0603 17:30:35.768518 15346 caffe.cpp:366] Testing <span class="keyword">for</span> 10 iterations.</span><br><span class="line">I0603 17:30:35.835475 15346 caffe.cpp:394] Iteration: 1 forward-backward time: 66 ms.</span><br><span class="line">I0603 17:30:35.902711 15346 caffe.cpp:394] Iteration: 2 forward-backward time: 67 ms.</span><br><span class="line">I0603 17:30:35.969769 15346 caffe.cpp:394] Iteration: 3 forward-backward time: 67 ms.</span><br><span class="line">I0603 17:30:36.036651 15346 caffe.cpp:394] Iteration: 4 forward-backward time: 66 ms.</span><br><span class="line">I0603 17:30:36.105055 15346 caffe.cpp:394] Iteration: 5 forward-backward time: 68 ms.</span><br><span class="line">I0603 17:30:36.174151 15346 caffe.cpp:394] Iteration: 6 forward-backward time: 69 ms.</span><br><span class="line">I0603 17:30:36.241129 15346 caffe.cpp:394] Iteration: 7 forward-backward time: 66 ms.</span><br><span class="line">I0603 17:30:36.308782 15346 caffe.cpp:394] Iteration: 8 forward-backward time: 67 ms.</span><br><span class="line">I0603 17:30:36.376447 15346 caffe.cpp:394] Iteration: 9 forward-backward time: 67 ms.</span><br><span class="line">I0603 17:30:36.443658 15346 caffe.cpp:394] Iteration: 10 forward-backward time: 67 ms.</span><br><span class="line">I0603 17:30:36.443676 15346 caffe.cpp:397] Average time per layer: </span><br><span class="line">I0603 17:30:36.443698 15346 caffe.cpp:400]      mnist	forward: 0.015 ms.</span><br><span class="line">I0603 17:30:36.443706 15346 caffe.cpp:403]      mnist	backward: 0.0009 ms.</span><br><span class="line">I0603 17:30:36.443711 15346 caffe.cpp:400]      conv1	forward: 7.4511 ms.</span><br><span class="line">I0603 17:30:36.443714 15346 caffe.cpp:403]      conv1	backward: 7.8538 ms.</span><br><span class="line">I0603 17:30:36.443718 15346 caffe.cpp:400]      pool1	forward: 3.3165 ms.</span><br><span class="line">I0603 17:30:36.443740 15346 caffe.cpp:403]      pool1	backward: 0.5728 ms.</span><br><span class="line">I0603 17:30:36.443745 15346 caffe.cpp:400]      conv2	forward: 12.81 ms.</span><br><span class="line">I0603 17:30:36.443769 15346 caffe.cpp:403]      conv2	backward: 25.1095 ms.</span><br><span class="line">I0603 17:30:36.443774 15346 caffe.cpp:400]      pool2	forward: 1.5992 ms.</span><br><span class="line">I0603 17:30:36.443778 15346 caffe.cpp:403]      pool2	backward: 0.5698 ms.</span><br><span class="line">I0603 17:30:36.443783 15346 caffe.cpp:400]        ip1	forward: 2.6873 ms.</span><br><span class="line">I0603 17:30:36.443787 15346 caffe.cpp:403]        ip1	backward: 4.9053 ms.</span><br><span class="line">I0603 17:30:36.443791 15346 caffe.cpp:400]      relu1	forward: 0.0563 ms.</span><br><span class="line">I0603 17:30:36.443809 15346 caffe.cpp:403]      relu1	backward: 0.0507 ms.</span><br><span class="line">I0603 17:30:36.443814 15346 caffe.cpp:400]        ip2	forward: 0.1712 ms.</span><br><span class="line">I0603 17:30:36.443819 15346 caffe.cpp:403]        ip2	backward: 0.2362 ms.</span><br><span class="line">I0603 17:30:36.443845 15346 caffe.cpp:400]       loss	forward: 0.0529 ms.</span><br><span class="line">I0603 17:30:36.443848 15346 caffe.cpp:403]       loss	backward: 0.0013 ms.</span><br><span class="line">I0603 17:30:36.443868 15346 caffe.cpp:408] Average Forward pass: 28.1725 ms.</span><br><span class="line">I0603 17:30:36.443892 15346 caffe.cpp:410] Average Backward pass: 39.3101 ms.</span><br><span class="line">I0603 17:30:36.443895 15346 caffe.cpp:412] Average Forward-Backward: 67.5 ms.</span><br><span class="line">I0603 17:30:36.443900 15346 caffe.cpp:414] Total Time: 675 ms.</span><br><span class="line">I0603 17:30:36.443918 15346 caffe.cpp:415] *** Benchmark ends ***</span><br></pre></td></tr></table></figure></li>
<li><p>使用GPU测试10 侧迭代：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe time -model examples/mnist/lenet_train_test.prototxt -gpu 0 -iterations 10</span><br></pre></td></tr></table></figure>

<p> 结果：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">I0603 17:32:11.830056 15434 caffe.cpp:365] *** Benchmark begins ***</span><br><span class="line">... <span class="comment"># 省略</span></span><br><span class="line">I0603 17:32:11.876488 15434 caffe.cpp:414] Total Time: 43.9143 ms.</span><br><span class="line">I0603 17:32:11.876494 15434 caffe.cpp:415] *** Benchmark ends ***</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试某个训练好的模型各层执行时间。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe time -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 10</span><br></pre></td></tr></table></figure>
<h1 id="python接口-pycaffe"><a href="#python接口-pycaffe" class="headerlink" title="python接口 pycaffe"></a>python接口 pycaffe</h1></li>
</ol>
<p>pycaffe接口需要先编译，看<a href="https://ashburnlee.github.io/2020/03/06/caffe-%E5%AE%89%E8%A3%85%E5%8F%8Atrouble-shooting/" target="_blank" rel="noopener">这里</a></p>
<p>在<code>caffe/examples</code>中的ipython notebook中是使用pycaffe的实例。 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/03/caffe-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8Epython%E6%8E%A5%E5%8F%A3/" data-id="ckaz6nepw0000edfz4ynb076l" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-cpp-lambda-function" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/03/cpp-lambda-function/" class="article-date">
  <time datetime="2020-06-03T04:54:16.000Z" itemprop="datePublished">2020-06-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/03/cpp-lambda-function/">cpp-lambda function</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在STL中的许多函数都需要提供一个<code>binary comp function</code>，指明是从大到小还是从小到大，比如<code>sort()</code>函数，最大最小堆等。</p>
<p>这个 <code>comp</code>函数 可以是函数指针或函数对象。也可以是个<code>lambda</code>函数：</p>
<h2 id="lambda函数"><a href="#lambda函数" class="headerlink" title="lambda函数"></a>lambda函数</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>&lt;&lt;[](<span class="keyword">float</span> f)-&gt;<span class="keyword">int</span> &#123;<span class="keyword">return</span> <span class="built_in">abs</span>(f)&#125;;(<span class="number">-3.5</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p>返回3.</p>
<p>其中<code>[]</code>中是<code>lambda indicators</code>。用法如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[ ]：<span class="comment">//不捕获任何外部变量</span></span><br><span class="line">[=]：<span class="comment">//以值的形式捕获所有外部变量</span></span><br><span class="line">[&amp;]：<span class="comment">//以引用的形式捕获所有外部变量</span></span><br><span class="line">[x, &amp;y]：<span class="comment">//x以值捕获，y以引用捕获</span></span><br><span class="line">[=, &amp;z]：<span class="comment">//z以引用捕获，其他以值形式捕获</span></span><br><span class="line">[&amp;, x]：<span class="comment">//x以值行形式捕获，其他以引用形式捕获</span></span><br></pre></td></tr></table></figure>

<p>例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> comp = [](<span class="keyword">const</span> <span class="keyword">auto</span>&amp; x, <span class="keyword">const</span> <span class="keyword">auto</span>&amp; y) &#123; <span class="keyword">return</span> x.second &lt; y.second; &#125;;</span><br><span class="line"></span><br><span class="line">sort(vec.begin(), vec.end(), comp)</span><br><span class="line">sort(vec.begin(), vec.end(), [](<span class="keyword">auto</span> x, <span class="keyword">auto</span> y)&#123;<span class="keyword">return</span> x&gt;y&#125;);</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/03/cpp-lambda-function/" data-id="ckayvo1uj0000sffz0sm18y4i" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-pip下载加速" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/03/pip%E4%B8%8B%E8%BD%BD%E5%8A%A0%E9%80%9F/" class="article-date">
  <time datetime="2020-06-03T04:45:06.000Z" itemprop="datePublished">2020-06-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Utility/">Utility</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/03/pip%E4%B8%8B%E8%BD%BD%E5%8A%A0%E9%80%9F/">pip下载加速</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>pip下载时添加国内源：</p>
<p>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> \</p>
<h2 id="临时使用："><a href="#临时使用：" class="headerlink" title="临时使用："></a>临时使用：</h2><p>可以在使用pip的时候加参数<code>-i https://pypi.tuna.tsinghua.edu.cn/simple</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple protobuf</span><br></pre></td></tr></table></figure>

<h2 id="永久修改："><a href="#永久修改：" class="headerlink" title="永久修改："></a>永久修改：</h2><p>Linux下，在文件<code>~/.pip/pip.conf</code> (没有就创建一个文件夹及文件)添加内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host&#x3D;mirrors.aliyun.com</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/03/pip%E4%B8%8B%E8%BD%BD%E5%8A%A0%E9%80%9F/" data-id="ckayvgf000009lufzeopu6kdu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">43</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 15px;">CUDA</a> <a href="/tags/Test-Analysis/" style="font-size: 10px;">Test Analysis</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/">cpp-void型指针</a>
          </li>
        
          <li>
            <a href="/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/">caffe-Layer中有什么</a>
          </li>
        
          <li>
            <a href="/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/">caffe-blob.cpp文件</a>
          </li>
        
          <li>
            <a href="/2020/06/06/caffe-SyncedMemory/">caffe-SyncedMemory</a>
          </li>
        
          <li>
            <a href="/2020/06/06/caffe-Blob-2/">caffe-Blob-(2)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>