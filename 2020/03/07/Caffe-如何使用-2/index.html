<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Caffe-Blob Layer Net | Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Layer和Netcaffe中的每个层对象，都是一个模型的基本计算单元。一个层对象包括 “…filters, pool, take inner products, apply nonlinearities like rectified-linear and sigmoid and other elementwise transformations, normalize, load data, an">
<meta property="og:type" content="article">
<meta property="og:title" content="Caffe-Blob Layer Net">
<meta property="og:url" content="http://yoursite.com/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:description" content="Layer和Netcaffe中的每个层对象，都是一个模型的基本计算单元。一个层对象包括 “…filters, pool, take inner products, apply nonlinearities like rectified-linear and sigmoid and other elementwise transformations, normalize, load data, an">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/logreg.jpg">
<meta property="article:published_time" content="2020-03-07T14:30:38.000Z">
<meta property="article:modified_time" content="2020-06-16T13:18:45.780Z">
<meta property="article:author" content="Junhui">
<meta property="article:tag" content="Caffe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/logreg.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Caffe-如何使用-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/" class="article-date">
  <time datetime="2020-03-07T14:30:38.000Z" itemprop="datePublished">2020-03-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Caffe-Blob Layer Net
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Layer和Net"><a href="#Layer和Net" class="headerlink" title="Layer和Net"></a>Layer和Net</h1><p>caffe中的每个层对象，都是一个模型的基本计算单元。一个层对象包括 “…filters, pool, take inner products, apply nonlinearities like rectified-linear and sigmoid and other elementwise transformations, normalize, load data, and compute losses like softmax and hinge”。见<a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html" target="_blank" rel="noopener">官方文档</a></p>
<p>几乎所有的层类都会继承自<code>Layer</code>类：<code>class caffe::Layer&lt;Dtype&gt;</code> 这些层类必须实现一个 <code>Forward()</code>, 它接受一个<code>input Blob（bottom）</code>计算输出的<code>Blob（top）</code>。还要实现一个 <code>Backward()</code>，它使用给定其输出<code>Blob</code>的误差梯度，计算相对于其输入<code>Blob</code>的误差梯度。（实现反向传播）</p>
<p>每个层对象一定包含三个基本方法：setup()，forward()，backward()。</p>
<ol>
<li>setup()：一些计算前的操作，用来初始化层。</li>
<li>forward()：前向计算。多个Layer构成一个Net，一个Net中连续的前向传播由<code>Net::Forward()</code>实现。</li>
<li>backward()：官方文档解释的很好：“given the gradient w.r.t. the top output, compute the gradient w.r.t. to the input and send to the bottom. A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.”。其中<code>w.r.t.</code>表示“with respect to”，中文表示“对…（求梯度等操作）”。这就是反向对参数求梯度的过程。当多个层链接到一起，就会形成反向传播的链条（对应了链式法则）。这个链式法则在<code>Net::Backward()</code>中实现</li>
</ol>
<p>前向计算和后向计算都有CPU和GPU版本的实现。实现自己的层也是不难的，只需要定义好上述三个关键方法。</p>
<p><a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="noopener">这里</a>是管方文档中所有的层。<br><br><a href="http://caffe.berkeleyvision.org/tutorial/forward_backward.html" target="_blank" rel="noopener">这里</a>是caffe中的前向传播和后向传播的官方描述。</p>
<p>一个Net由多个Layer构成，如下面一个计算图（有向无环图）：</p>
<div align="center"><img src="/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/logreg.jpg" width="350"></div>

<p>其中蓝色矩形表示layer，里边的小写表示层的name，大写表示层的type；黄色多边形表示在图中游走的数据<code>Blob</code>，数据移动方向从下向上，下为<code>bottom</code>，上为<code>top</code>。这个图对应的<code>.prototxt</code>如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"LogReg"</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span>   <span class="comment">#输出</span></span><br><span class="line">  top: <span class="string">"label"</span>  <span class="comment">#输出</span></span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="built_in">source</span>: <span class="string">"input_leveldb"</span></span><br><span class="line">    batch_size: 64</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"data"</span>  <span class="comment">#输入</span></span><br><span class="line">  top: <span class="string">"ip"</span>       <span class="comment">#输出</span></span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip"</span>     <span class="comment">#输入</span></span><br><span class="line">  bottom: <span class="string">"label"</span>   <span class="comment">#输入</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个Net的初始化通过执行<code>Net::Init()</code>，其作用是生成Blob和Layers，并调用Layer的setup函数。终端的输出信息表示caffe还有记录的工作被执行。<br><br>用<code>Caffe::mode()</code>和<code>Caffe::set_mode()</code>指定使用CPU或GPU执行.CPU和GPU的切换是无缝的，与模型的定义是无关的。<br><br>模型的定义在机器中是以<code>protocol buffer schema (prototxt)</code>形式存在的。对应训练好的模型是以<code>binary protocol buffer (binaryproto)</code>存在与磁盘的，就是相应路径中的<code>.caffemodel</code>文件。<br><br>模型在caffe中的格式<code>。prototxt</code>由<code>caffe.proto</code>解析。细节见<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto" target="_blank" rel="noopener">这里</a>本地路径：<code>caffe-ROOT/src/caffe/proto/caffe.proto</code>。所使用的技术是<a href="https://code.google.com/p/protobuf/" target="_blank" rel="noopener">Google Protocol Buffer</a>通过科学上网查看。</p>
<p>为什么要使用<code>protocal buffer</code>来格式化模型？<br>因为，你哪里见过一边砌墙一边烧砖的。使用 protobuff 将整个流程分开，实现每一个组件，之后在将所需的组件拼接起来，最后训练。</p>
<h1 id="Blob"><a href="#Blob" class="headerlink" title="Blob"></a>Blob</h1><p>Blob为模型提供数据和数据载体，在模型的正向反向传播中移动。它还提供了CPU和GPU间的同步机制。Blob中的数据可以是一批图像，模型参数，中间计算结果。在Blob中不同类型的数据，其大小是不同的。</p>
<ol>
<li><p>图像数据Blob，其中的数据是4维的：<code>（N, K, H, W）</code>分别表示batch size，channel数量，长和宽。当一个Blob对象中的数据变化时，变化优先级从右向左。所以索引为<code>（n, k, h, w）</code>的数值在物理存储中的索引是<code>((n×K + k)×H + h)×W + w</code>。</p>
</li>
<li><p>对于非图像的数据，使用2D Blob，（N, D），此时通常与<code>InnerProductLayer</code>一同使用。</p>
</li>
<li><p>对于参数Blob，其维度要具体问题具体分析了。比如对于conv层，由64个conv kernel，一个kernel的大小是11×11，输入通道数为3，那么此情况的参数Blob大小为（96,3,11,11）。而对于全连接层或上述的<code>InnerProductLayer</code>，如果输出1000维，输入1024维，那么此情况的参数Blob大小为（1000,1024）。</p>
</li>
</ol>
<p>Blob数据一旦定以好后，caffe的模块化就可以做剩下的工作了。</p>
<h1 id="Blob细节"><a href="#Blob细节" class="headerlink" title="Blob细节"></a>Blob细节</h1><p>通常我们关心Blob中的数值（一般的数据，比如一批输入图像）和梯度值。所以一个Blob由两块儿存储：<code>data</code>和<code>diff</code>。</p>
<p>更进一步，Blob中的数值有两种访问的方式，cont和mutable，前者数值不变，后者数值可改变。比如</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<p>同样的，对于<code>cpu_diff</code>和<code>gpu_data</code>, <code>gpu_diff</code> 也有两种访问方式。</p>
<h1 id="Blob为什么要如此设计？"><a href="#Blob为什么要如此设计？" class="headerlink" title="Blob为什么要如此设计？"></a>Blob为什么要如此设计？</h1><p>官方文档给出如下解释：</p>
<ol>
<li><p>Blob使用<code>SyncedMem</code>类实现CPU与GPU间的数据拷贝，目的是隐藏延时。</p>
</li>
<li><p>如果不想更改数值，那么始终使用const方式调用，而且永远不要将指针存储在自己的对象中。每次处理Blob时，都要调用函数来获取指针，因为<code>SyncedMem</code>类需要这个函数来确定何时复制数据。</p>
</li>
<li><p>在实际中，调用设备内核来执行GPU计算的同时，CPU将数据从磁盘加载到Blob，并将Blob转移到下一层，这是GPU和CPU延时隐藏的基本技巧。因为大部分层都有GPU实现，所以所有的中间数据和梯度都将保留在GPU的内存中。</p>
</li>
</ol>
<p>其他原因如，Blob与其他深度学习框架的数据结构相对应，tensorflow中的tensor，cuda-convnet中的NVMatrix，等，这使得框架之间的转换更容易。</p>
<h2 id="Blob中数据在CPU和GPU之间拷贝行为"><a href="#Blob中数据在CPU和GPU之间拷贝行为" class="headerlink" title="Blob中数据在CPU和GPU之间拷贝行为"></a>Blob中数据在CPU和GPU之间拷贝行为</h2><p>假设数据在CPU（Host）上被初始化，存在于一个Blob中。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个const访问指针，一个mutable访问指针</span></span><br><span class="line"><span class="keyword">const</span> Dtype* foo;   </span><br><span class="line">Dtype* bar;</span><br><span class="line"><span class="comment">// GPU上没有数据数据</span></span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line"><span class="comment">// 上下两句间没有任何操作，所以下一句并不会发生数据拷贝</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// no data copied since both have up-to-date contents.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// 一样，没有数据拷贝.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 对数据进行操作...</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// 当前位于GPU上，没有数据拷贝到gpu.   ？？？？？？</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// 因为数据被操作，所以数据拷贝从GPU到CPU发生。</span></span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// 因为GPU和CPU都是最新数据，所以没有数据拷贝发生。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ？？？？？？</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// still no data copied.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// data copied gpu-&gt;cpu.</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/" data-id="ckatxvx570000lzfzc08zem2b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/03/07/caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          caffe-使用已有的lenet模型
        
      </div>
    </a>
  
  
    <a href="/2020/03/06/caffe-%E5%AE%89%E8%A3%85%E5%8F%8Atrouble-shooting/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">caffe 安装及trouble shooting</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">23</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">50</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BST/" rel="tag">BST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">33</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/BST/" style="font-size: 10px;">BST</a> <a href="/tags/CUDA/" style="font-size: 18px;">CUDA</a> <a href="/tags/Caffe/" style="font-size: 16px;">Caffe</a> <a href="/tags/Test-Analysis/" style="font-size: 14px;">Test Analysis</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 12px;">二分查找</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 10px;">递归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">26</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/17/caffe-%E5%9C%A8%E6%96%B0%E6%A0%B7%E6%9C%AC%E4%B8%8A%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B/">caffe-在新样本上使用训练好的模型</a>
          </li>
        
          <li>
            <a href="/2020/06/16/LeetCode-BST%E7%9B%B8%E5%85%B3/">LeetCode-BST相关</a>
          </li>
        
          <li>
            <a href="/2020/06/15/LeetCode-top-k/">LeetCode-top-k</a>
          </li>
        
          <li>
            <a href="/2020/06/13/LeetCode-153-%E6%89%BE%E5%88%B0%E6%9C%80%E5%B0%8F%E5%80%BC/">LeetCode-153-找到最小值</a>
          </li>
        
          <li>
            <a href="/2020/06/12/LeetCode-34-find-positions-of-elements/">LeetCode-34-find-positions-of-elements</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>