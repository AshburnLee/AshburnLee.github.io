<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>文本分类(三)-构建模型I-built-in-LSTM | Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这篇笔记记录使用tensorflow的built-in LSTM创建一个文本分类模型，数据来自文本预处理(二)词编码。 超参数首先定义模型使用的超参数，使用tf.contrib.training.HParams()来管理,如下： 1234567891011def huper_param():    return tf.contrib.training.HParams(            num">
<meta property="og:type" content="article">
<meta property="og:title" content="文本分类(三)-构建模型I-built-in-LSTM">
<meta property="og:url" content="http://yoursite.com/2019/08/04/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%B8%89-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8BI-built-in-LSTM/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:description" content="这篇笔记记录使用tensorflow的built-in LSTM创建一个文本分类模型，数据来自文本预处理(二)词编码。 超参数首先定义模型使用的超参数，使用tf.contrib.training.HParams()来管理,如下： 1234567891011def huper_param():    return tf.contrib.training.HParams(            num">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-08-04T07:58:14.000Z">
<meta property="article:modified_time" content="2020-03-02T06:12:56.688Z">
<meta property="article:author" content="Junhui">
<meta property="article:tag" content="Algorithms">
<meta property="article:tag" content="Test Analysis">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-文本分类-三-构建模型I-built-in-LSTM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/04/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%B8%89-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8BI-built-in-LSTM/" class="article-date">
  <time datetime="2019-08-04T07:58:14.000Z" itemprop="datePublished">2019-08-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      文本分类(三)-构建模型I-built-in-LSTM
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这篇笔记记录使用tensorflow的built-in LSTM创建一个文本分类模型，数据来自<a href="https://ashburnlee.github.io/2019/08/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%BA%8C-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86II-%E8%AF%8D%E7%BC%96%E7%A0%81/" target="_blank" rel="noopener">文本预处理(二)词编码</a>。</p>
<h1 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h1><p>首先定义模型使用的超参数，使用<code>tf.contrib.training.HParams()</code>来管理,如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huper_param</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.contrib.training.HParams(</span><br><span class="line">            num_embedding_size=<span class="number">16</span>,  <span class="comment">#  每一个词所作embedding的向量长度</span></span><br><span class="line">            encoded_length=<span class="number">50</span>,      <span class="comment">#  每一条编码后的样本切取或补充后的长度，</span></span><br><span class="line">            num_word_threshold=<span class="number">20</span>,  <span class="comment">#  词频数&lt;=该值，不考虑</span></span><br><span class="line">            num_lstm_nodes=[<span class="number">32</span>, <span class="number">32</span>],  <span class="comment"># 每一层LSTM的节点个数 </span></span><br><span class="line">            num_lstm_layers=<span class="number">2</span>,       <span class="comment">#  LSTM层数</span></span><br><span class="line">            num_fc_nodes=<span class="number">32</span>,         <span class="comment">#  全连接层节点数</span></span><br><span class="line">            batch_size=<span class="number">100</span>,          <span class="comment">#  每一次输入样本书</span></span><br><span class="line">            learning_rate=<span class="number">0.001</span>,    <span class="comment"># 学习率</span></span><br><span class="line">            clip_lstm_grads=<span class="number">1.0</span>, )   <span class="comment">#  设置LSTM梯度大小，防止梯度爆炸</span></span><br></pre></td></tr></table></figure>

<p>默认每一个参数含初始值，各个参数的含义见注释。其中具体解释两个：</p>
<ul>
<li><code>num_embedding_size</code>： 每一个词会用一个向量来表示，该值指明这个向量的大小。而且这个向量是被学习的。</li>
<li><code>clip_lstm_grads</code>： 这是LSTM<strong>梯度值的上限</strong>，当一个梯度值大于这个上限时，把这个值设置为上限值，来防止<strong>梯度爆炸</strong>。</li>
</ul>
<p>调用该函数生成一个对象，可以用<code>对象名.参数名</code>来使用相应的参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hp = huper_param()</span><br><span class="line">encoded_length = hp.encoded_length</span><br></pre></td></tr></table></figure>

<h1 id="定义计算图"><a href="#定义计算图" class="headerlink" title="定义计算图"></a>定义计算图</h1><p>先定义输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = tf.placeholder(tf.int32, (batch_size, encoded_length))</span><br><span class="line">outputs = tf.placeholder(tf.int32, (batch_size, ))</span><br></pre></td></tr></table></figure>

<p>定义DropOut比率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder(tf.float32, name=<span class="string">'keep_prob'</span>)</span><br></pre></td></tr></table></figure>

<p>保存当前训练到了那一步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(tf.zeros([], tf.int64), name=<span class="string">'global_step'</span>, trainable=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h2 id="1-Embedding层"><a href="#1-Embedding层" class="headerlink" title="1. Embedding层"></a>1. Embedding层</h2><p>使用均匀分布来初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embedding_init = tf.random_uniform_initializer(<span class="number">-1.0</span>, <span class="number">1.</span>)</span><br></pre></td></tr></table></figure>

<p>定义embedding：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">embedding = tf.get_variable(</span><br><span class="line">            <span class="string">'embedding'</span>,</span><br><span class="line">            [vocab_size, hps.embedding_size],   <span class="comment"># size of embedding matrix</span></span><br><span class="line">            tf.float32</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ul>
<li>使用<code>get_varable()</code>，当这个变量存在，就重用它，不存在，则创建它。</li>
<li><font color="red">embedding矩阵</font>：<code>[vocab_size, hps.embedding_size]</code>：一共有多少个词，每个词用多大的向量表示。</li>
</ul>
<p>下一步，将每一条输入中每一个词对应的向量在<code>embedding matrix</code>中<font color="red">查找</font>，比如，当前词<code>id</code>为12，就从<code>embedding matrix</code>中把第12行的向量取出来，对一条记录中每个词作此操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[2, 34, 5, 67]-&gt;[[234,565,1,45,57,73],</span><br><span class="line">                 [12,76,23,54,123,48],</span><br><span class="line">                 [43,87,239,57,13,14],</span><br><span class="line">                 [98,64,421,13,63,36]]</span><br></pre></td></tr></table></figure>

<p>用长度为6的向量表示一个词的id。可以看作是对每一条记录的<font color="red" size="4">进一步编码</font>。而且这个编码是要<font color="red" size="4">被学习</font>的。</p>
<p>给出完整embedding层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">embedding_init = tf.random_uniform_initializer(<span class="number">-1.0</span>, <span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'embedding'</span>, initializer=embedding_init):</span><br><span class="line">        embedding = tf.get_variable(</span><br><span class="line">            <span class="string">'embedding'</span>,</span><br><span class="line">            [vocab_size, hps.embedding_size],   <span class="comment"># size of embedding matrix</span></span><br><span class="line">            tf.float32</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        embedded_inputs = tf.nn.embedding_lookup(embedding, inputs)</span><br></pre></td></tr></table></figure>

<h2 id="2-LSTM-层"><a href="#2-LSTM-层" class="headerlink" title="2. LSTM 层"></a>2. LSTM 层</h2><p>定义initializer：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scale = <span class="number">1.0</span>/math.sqrt(hps.num_embedding_size + hps.nums_lstm_nodes[<span class="number">-1</span>])/<span class="number">3.0</span></span><br><span class="line">lstm_init = tf.random_uniform_initializer(-scale, scale)</span><br></pre></td></tr></table></figure>

<p>即使用<a href>xavior</a>初始化。</p>
<p>定义两层LSTM：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cells = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    cell = tf.contrib.rnn.BasicLSTMCell(</span><br><span class="line">        hps.nums_lstm_nodes[i],</span><br><span class="line">        state_is_tuple=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    cell = tf.contrib.rnn.DropoutWrapper(</span><br><span class="line">        cell,</span><br><span class="line">        output_keep_prob=keep_prob</span><br><span class="line">    )</span><br><span class="line">    cells.append(cell)</span><br></pre></td></tr></table></figure>

<p><code>cells</code>接收每一层，使用<code>BasicLSTMCell</code>创建一LSTM层。紧接着使用<code>DropoutWrapper</code>执行DropOut操作。此时<code>cells</code>中含有两层LSTM。</p>
<p>然后使用<code>MultiRNNCell</code>合并两LSTM层，第一个<code>cell</code>的输出为第二个<code>cell</code>的输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cell = tf.contrib.rnn.MultiRNNCell(cells)</span><br></pre></td></tr></table></figure>

<p>此时就可以把两层的LSTM当作模型中的一层来操作。</p>
<p>紧接着初始化LSTM单元中的<code>state</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initialize_state = cell.zero_state(batch_size, tf.float32)</span><br></pre></td></tr></table></figure>

<p>此时便可以使用<code>dynamic_rnn</code>把<font color="red" size="4">序列式的输入</font>传入LSTM层，后得到一系列中间状态和输出值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rnn_outputs, _ = tf.nn.dynamic_rnn(cell, embedded_inputs, initial_state=initialize_state)</span><br></pre></td></tr></table></figure>

<p>其中<code>_</code>表示中间隐含状态，不需要。<code>rnn_outputs</code>中包含了所有中间输出。对于<strong>多对一</strong>的问题，我们只需要最后一个值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">last = rnn_outputs[:, <span class="number">-1</span>, :]</span><br></pre></td></tr></table></figure>

<p>给出完整的LSTM层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">scale = <span class="number">1.0</span>/math.sqrt(hps.num_embedding_size + hps.nums_lstm_nodes[<span class="number">-1</span>])/<span class="number">3.0</span></span><br><span class="line">lstm_init = tf.random_uniform_initializer(-scale, scale)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'lstm'</span>, initializer=lstm_init):</span><br><span class="line">    <span class="comment"># store two LSTM layers</span></span><br><span class="line">    cells = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(hps.num_lstm_layer):</span><br><span class="line">        cell = tf.contrib.rnn.BasicLSTMCell(</span><br><span class="line">            hps.nums_lstm_nodes[i],</span><br><span class="line">            state_is_tuple=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        cell = tf.contrib.rnn.DropoutWrapper(</span><br><span class="line">            cell,</span><br><span class="line">            output_keep_prob=keep_prob</span><br><span class="line">        )</span><br><span class="line">        cells.append(cell)</span><br><span class="line">    <span class="comment"># combine two LSTM layers: 第一个cell的输出为第二个cell的输入</span></span><br><span class="line">    cell = tf.contrib.rnn.MultiRNNCell(cells)</span><br><span class="line">    <span class="comment"># init state</span></span><br><span class="line">    initialize_state = cell.zero_state(batch_size, tf.float32)</span><br><span class="line">    <span class="comment"># input a sentence to cell, 此时就可以把句子输入到cell中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># rnn_outputs: [batch_size, encoded_length, lstm_output[-1]]</span></span><br><span class="line">    rnn_outputs, _ = tf.nn.dynamic_rnn(</span><br><span class="line">        cell, embedded_inputs, initial_state=initialize_state)</span><br><span class="line">    last = rnn_outputs[:, <span class="number">-1</span>, :]</span><br></pre></td></tr></table></figure>

<h2 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3. 全连接层"></a>3. 全连接层</h2><p>使用<code>dence()</code>构建全连接层，指定<code>ReLU</code>为激活函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fc1 = tf.layers.dense(last, hps.num_fc_nodes, activation=tf.nn.relu, name=<span class="string">'fc1'</span>)</span><br></pre></td></tr></table></figure>

<p>紧接着进行<code>dropOut</code>操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)</span><br></pre></td></tr></table></figure>

<p>最后再接一个全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits = tf.layers.dense(fc1_dropout, classes_size, name=<span class="string">'fc2'</span>)</span><br></pre></td></tr></table></figure>

<p>给出完整的全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fc_init = tf.uniform_unit_scaling_initializer(factor=<span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'fc'</span>, initializer=fc_init):</span><br><span class="line">    fc1 = tf.layers.dense(last, hps.num_fc_nodes, activation=tf.nn.relu, name=<span class="string">'fc1'</span>)</span><br><span class="line">    fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)</span><br><span class="line">    logits = tf.layers.dense(fc1_dropout, classes_size, name=<span class="string">'fc2'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4-模型输出"><a href="#4-模型输出" class="headerlink" title="4. 模型输出"></a>4. 模型输出</h2><p>首先：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=outputs)</span><br></pre></td></tr></table></figure>

<p><code>tf.nn.sparse_softmax_cross_entropy_with_logits()</code>做了三件事：</p>
<ul>
<li>填坑</li>
<li>填坑</li>
<li>填坑</li>
</ul>
<p>其次，传入代价函数，并且算出模型输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(softmax_loss)</span><br><span class="line">y_pred = tf.argmax(tf.nn.softmax(logits), <span class="number">1</span>, output_type=tf.int32)</span><br></pre></td></tr></table></figure>

<p>最后，用最简单的正确率衡量模型性能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">correct_pred = tf.equal(outputs, y_pred)</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br></pre></td></tr></table></figure>

<p>说明下面二者的不同：</p>
<ul>
<li><code>tf.variable_scope</code>：需要初始化</li>
<li><code>tf.name_scope</code>：无需初始化</li>
</ul>
<p>此部分完整实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'metrics'</span>):</span><br><span class="line">    softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits, labels=outputs</span><br><span class="line">    )</span><br><span class="line">    loss = tf.reduce_mean(softmax_loss)</span><br><span class="line">    y_pred = tf.argmax(tf.nn.softmax(logits), <span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">    correct_pred = tf.equal(outputs, y_pred)</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br></pre></td></tr></table></figure>

<h2 id="5-得到train-op"><a href="#5-得到train-op" class="headerlink" title="5. 得到train_op"></a>5. 得到train_op</h2><p>因为之前对梯度值设定了一个上界，所以要把截断后的梯度值得到，作用于所有可训练变量。所以第一步得到所有可训练变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainable_vars = tf.trainable_variables()</span><br></pre></td></tr></table></figure>

<p>可以查看所有的可训练变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> trainable_vars:</span><br><span class="line">    print(<span class="string">'variable name: %s'</span> % (var.name))</span><br></pre></td></tr></table></figure>

<p>对所有可训练变量求导数，得到实际梯度后对其<font color="red" size="4">执行剪切操作</font>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), hps.clip_lstm_grads)</span><br></pre></td></tr></table></figure>

<p>指定优化算法，应用剪切后的梯度于所有可训练变量。最后训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(hps.learning_rate)</span><br><span class="line">train_op = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)</span><br></pre></td></tr></table></figure>

<p>完整实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train_op'</span>):</span><br><span class="line">    trainable_vars = tf.trainable_variables()</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> trainable_vars:</span><br><span class="line">        print(<span class="string">'variable name: %s'</span> % (var.name))</span><br><span class="line">    grads, _ = tf.clip_by_global_norm(</span><br><span class="line">        tf.gradients(loss, trainable_vars), hps.clip_lstm_grads</span><br><span class="line">    )</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(hps.learning_rate)</span><br><span class="line">    train_op = optimizer.apply_gradients(</span><br><span class="line">        zip(grads, trainable_vars), global_step=global_step</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h2 id="6-返回值"><a href="#6-返回值" class="headerlink" title="6. 返回值"></a>6. 返回值</h2><p>最后指定函数返回值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> ((inputs, outputs, keep_prob),  <span class="comment">#  all placeholders</span></span><br><span class="line">        (loss, accuracy),              <span class="comment"># loss &amp; accuracy</span></span><br><span class="line">        (train_op, global_step))      <span class="comment"># tain_op</span></span><br></pre></td></tr></table></figure>

<p>到此位置计算图设计完成。</p>
<p>假设上述定义计算图可以封装到函数：<code>create_model()</code>。测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataPreProcess <span class="keyword">import</span> encodeWords</span><br><span class="line">vocab_instance = encodeWords.VocabDict(vocab_file, hps.num_word_threshold)</span><br><span class="line">catego_instance = encodeWords.CategoryDict(category_file)</span><br><span class="line">placeholders, metrics, others = create_model(hps,</span><br><span class="line">                                             vocab_instance.size(),</span><br><span class="line">                                             catego_instance.size())</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/AshburnLee/text-classification-revise/blob/master/dataPreProcess/encodeWords.py" target="_blank" rel="noopener">encodedWords</a>中是在<em>文本预处理(二)词编码篇</em>实现的两个类<code>VocabDict</code>和<code>CategoryDict</code>。分别调用其<code>.size()</code>方法，可返回词数量和类别数量。</p>
<p>打印所有可训练变量，控制台结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">variable name: &lt;tf.Variable <span class="string">'embedding/embedding:0'</span> shape=(50513, 16) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'lstm/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'</span> shape=(48, 128) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'lstm/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'</span> shape=(128,) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'lstm/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'</span> shape=(64, 128) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'lstm/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'</span> shape=(128,) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'fc/fc1/kernel:0'</span> shape=(32, 32) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'fc/fc1/bias:0'</span> shape=(32,) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'fc/fc2/kernel:0'</span> shape=(32, 10) dtype=float32_ref&gt;</span><br><span class="line">variable name: &lt;tf.Variable <span class="string">'fc/fc2/bias:0'</span> shape=(10,) dtype=float32_ref&gt;</span><br></pre></td></tr></table></figure>

<p>注意，训练并没有执行计算，只是打印了计算图中的可训练变量。结果显示有三部分：</p>
<ul>
<li>embedding层</li>
<li>两层LSTM的权值阈值</li>
<li>两层全连接层的权值和阈值</li>
</ul>
<p>并且每部分参数的形状也可知。</p>
<h1 id="执行计算流程"><a href="#执行计算流程" class="headerlink" title="执行计算流程"></a>执行计算流程</h1><p>先执行<code>create_model()</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">placeholders, metrics, others = create_model(hps,</span><br><span class="line">                                             vocab_instance.size(),</span><br><span class="line">                                             catego_instance.size())</span><br><span class="line">inputs, outputs, keep_prob = placeholders</span><br><span class="line">loss, accuracy = metrics</span><br><span class="line">train_op, global_step = others</span><br></pre></td></tr></table></figure>
<p>然后初始化整个网络，给训练过程的<code>keep_prob</code>赋值，并指明训练步数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">train_keep_prob = <span class="number">0.8</span></span><br><span class="line">num_train_steps = <span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p>最后创建执行图的<code>tf.session</code>，并执行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># init whole network</span></span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train_steps):</span><br><span class="line">        batch_inputs, batch_label = train_dataset.next_batch(hps.batch_size)</span><br><span class="line">        <span class="comment"># training: global_step+1 when sess.run() is called</span></span><br><span class="line">        outputs_val = sess.run([loss, accuracy, train_op, global_step],</span><br><span class="line">                               feed_dict=&#123;</span><br><span class="line">                                   inputs: batch_inputs,</span><br><span class="line">                                   outputs: batch_label,</span><br><span class="line">                                   keep_prob: train_keep_prob</span><br><span class="line">                               &#125;)</span><br><span class="line">        <span class="comment"># get three values from output_val</span></span><br><span class="line">        loss_val, accuracy_val, _, global_step_val = outputs_val</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print for every 100 times</span></span><br><span class="line">        <span class="keyword">if</span> global_step_val % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"step: %5d, loss: %3.3f, accuracy: %3.5f"</span> %</span><br><span class="line">                  (global_step_val, loss_val, accuracy_val)</span><br><span class="line">                  )</span><br></pre></td></tr></table></figure>

<p>其中用到了一个重要方法：给placeholders赋值。所以在运行之前使用训练集创建<code>EncodedDataset</code>对象，就可以调用<code>train_dataset.next_batch(hps.batch_size)</code>了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset &#x3D; createEncodedDataset.EncodedDataset(</span><br><span class="line">        seg_train_file, vocab_instance, catego_instance, hps.encoded_length)</span><br></pre></td></tr></table></figure>

<p>如下时执行1000次的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">step:   200, loss: 1.732, accuracy: 0.25000</span><br><span class="line">step:   400, loss: 1.720, accuracy: 0.32000</span><br><span class="line">step:   600, loss: 1.537, accuracy: 0.39000</span><br><span class="line">step:   800, loss: 1.279, accuracy: 0.53000</span><br><span class="line">step:  1000, loss: 0.994, accuracy: 0.67000</span><br></pre></td></tr></table></figure>

<p>至少证明模型是正确的。完整实现<a href="https://github.com/AshburnLee/text-classification-revise/blob/master/models/LSTM_built_in.py" target="_blank" rel="noopener">看这里</a>。<br>这是第一步，之后便可以进一步优化。本笔记只记录使用<code>tf</code>内置LSTM模块构建基本LSTM文本分类模型，对于优化，调参以后讨论。</p>
<h1 id="最后一点"><a href="#最后一点" class="headerlink" title="最后一点*"></a>最后一点*</h1><p>在参数列表中有一项<code>num_lstm_nodes</code>，什么意思？！在构建LSTM时的核心函数是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cell &#x3D; tf.contrib.rnn.BasicLSTMCell( hps.nums_lstm_nodes[i], state_is_tuple&#x3D;True)</span><br></pre></td></tr></table></figure>

<p><font color="green" size="5">敲黑板</font></p>
<p>查看官方文档：首个参数<code>num_units</code>：它表示<font color="red">LSTM单元内部的神经元数量</font>，即<strong>输出神经元数</strong>。<a href="https://ashburnlee.github.io/2019/08/05/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%B8%89-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8BII-LSTM%E5%B1%82%E7%BB%93%E7%82%B9%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/" target="_blank" rel="noopener">LSTM结点结构图</a>中有5个主要非线性变换，他们中的每一个都相当于普通神经网络的的<font color="red">一个<strong>神经原</strong></font>，相对于解决异或问题只需要3个神经元(逻辑门)，解决复杂问题的网络神经元数量都<font color="red">远远不止一个</font>。<br><br><font color="red">相同道理</font>，包含5个非线性变换的一个LSTM结点在解决复杂问题时一定也远不只需要一个。图中只是示意图，表示一个结点，实际上会有很多。从LSTM层使用xavior初始化的角度看，<code>sqrt(hps.num_embedding_size + hps.nums_lstm_nodes[-1])</code>定义代表<code>sqrt(输入大小 + 输出大小)</code>，<code>hps.nums_lstm_nodes[-1]</code>正对应这一层的输出大小。<br><br>诶，在图中也有多个LSTM节点呀！？，这些结点是逻辑上按时间序列展开的节点，空间上只有一个。这里讨论的是另一个维度的LSTM结点。并不矛盾。可以从<a href="https://ashburnlee.github.io/2019/08/05/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%B8%89-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8BII-LSTM%E5%B1%82%E7%BB%93%E7%82%B9%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/" target="_blank" rel="noopener">下一节笔记</a>的代码实现中体会。</p>
<p>理解这个很重要！</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/04/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%B8%89-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8BI-built-in-LSTM/" data-id="ck74urap0000f4nfzfcw0bn8k" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/05/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%B8%89-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8BII-LSTM%E5%B1%82%E7%BB%93%E7%82%B9%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          文本分类(三)-构建模型II-LSTM层结点实现细节
        
      </div>
    </a>
  
  
    <a href="/2019/08/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E4%BA%8C-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86II-%E8%AF%8D%E7%BC%96%E7%A0%81/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">文本分类(二)-文本预处理II-词编码</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">38</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hardware/" rel="tag">hardware</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 16.67px;">CUDA</a> <a href="/tags/Test-Analysis/" style="font-size: 13.33px;">Test Analysis</a> <a href="/tags/hardware/" style="font-size: 10px;">hardware</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/28/CUDA-%E5%B9%B6%E8%A1%8C%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF/">CUDA-并行一维卷积</a>
          </li>
        
          <li>
            <a href="/2020/02/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-stack/">LeetCode-方法论-stack</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%9D%82%E8%AE%B0%E5%BE%85%E5%BD%92%E7%B1%BB/">CUDA-杂记待归类</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95/">CUDA-扫描算法</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E5%86%8D%E7%9C%8B%E8%A7%84%E7%BA%A6-%E4%B8%80%E6%AE%B5%E8%A7%84%E7%BA%A6/">CUDA-再看规约-一段规约</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>