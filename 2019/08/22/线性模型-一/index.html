<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>线性模型-(一)-最小二乘 | Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="线性模型形式简单，具有很好的可解释性。虽然简单却蕴藏着重要的机器学习的基本思想。一般实践中，都会先使用线性回归方法尝试。 把分类问题的样本点放在在坐标轴上，坐标轴每一个轴代表一个特征。而回归问题的样本点放到坐标轴上，x轴是连续的，可以是其中一个特征的连续值，其他轴代表不同的特征。 线性回归的目标是度量出模型没有拟合住样本点的部分，此时的目标函数称为 loss fucntion。 而有的算法中度量">
<meta property="og:type" content="article">
<meta property="og:title" content="线性模型-(一)-最小二乘">
<meta property="og:url" content="http://yoursite.com/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:description" content="线性模型形式简单，具有很好的可解释性。虽然简单却蕴藏着重要的机器学习的基本思想。一般实践中，都会先使用线性回归方法尝试。 把分类问题的样本点放在在坐标轴上，坐标轴每一个轴代表一个特征。而回归问题的样本点放到坐标轴上，x轴是连续的，可以是其中一个特征的连续值，其他轴代表不同的特征。 线性回归的目标是度量出模型没有拟合住样本点的部分，此时的目标函数称为 loss fucntion。 而有的算法中度量">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/linear.png">
<meta property="article:published_time" content="2019-08-22T07:02:28.000Z">
<meta property="article:modified_time" content="2020-01-11T10:29:15.861Z">
<meta property="article:author" content="Junhui">
<meta property="article:tag" content="Algorithms">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/linear.png">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-线性模型-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-22T07:02:28.000Z" itemprop="datePublished">2019-08-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      线性模型-(一)-最小二乘
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>线性模型形式简单，具有很好的可解释性。虽然简单却蕴藏着重要的机器学习的基本思想。一般实践中，都会先使用线性回归方法尝试。</li>
<li>把分类问题的样本点放在在坐标轴上，坐标轴每一个轴代表一个特征。而回归问题的样本点放到坐标轴上，<strong>x轴是连续的，可以是其中一个特征的连续值</strong>，其他轴代表不同的特征。</li>
<li>线性回归的目标是度量出模型没有拟合住样本点的部分，此时的目标函数称为 loss fucntion。</li>
<li>而有的算法中度量的是拟合的程度，此时成目标函数为效用函数 utility function 。</li>
<li>通过分析问题，确定问题的目标函数，通过最优化目标(最小化loss 或最大化utility)，获得机器学习模型。</li>
<li>为参数学习，找到一组参数，这组参数可以最小化loss 或最大化utility。涉及到<strong>最优化原理</strong>或<strong>凸优化理论</strong>，如常见的牛顿法，梯度下降，模拟退火等算法。许多计算机问题如最短路径，背包问题都属于最优化问题。</li>
<li>对于(一元或多元)线性回归，直接用最小二乘法求解。</li>
<li>使用线性模型的前提，是假设数据与目标间由线性关系。</li>
</ul>
<h1 id="最小二乘法解一元线性回归"><a href="#最小二乘法解一元线性回归" class="headerlink" title="最小二乘法解一元线性回归"></a>最小二乘法解一元线性回归</h1><p>基于<strong>最小化均方误差</strong>来进行模型求解的方法为<strong>最小二乘法</strong>，即试图找一条直线，使得所有样本点到该直线的欧氏距离之和最小。具体的结果是经过数学推导，而非迭代参数学习。</p>
<p>数据为一维向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 小写X表示这是一个向量</span></span><br><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,])</span><br><span class="line">y = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">9</span>,<span class="number">11</span>,])</span><br></pre></td></tr></table></figure>

<p>根据最小二乘法结果公式的得：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最小二乘法</span></span><br><span class="line">x_bar = np.mean(x)</span><br><span class="line">y_bar = np.mean(y)</span><br><span class="line"></span><br><span class="line">fenzi = <span class="number">0.0</span></span><br><span class="line">fenmu = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> zip(x, y):</span><br><span class="line">    fenzi += (i - x_bar) * (j - y_bar)</span><br><span class="line">    fenmu += (i - x_bar)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">a = fenzi / fenmu</span><br><span class="line">b = y_bar - a * x_bar</span><br></pre></td></tr></table></figure>

<p>绘制拟合直线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制直线</span></span><br><span class="line">y_hat = a*x + b</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, y_hat, color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">12</span>, <span class="number">0</span>, <span class="number">12</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<div align="center"><img src="/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/linear.png" width="500"></div>

<p>使用模型，分别预测单个值，预测一组值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测单个值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测多个值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_X</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.asarray([a * x + b <span class="keyword">for</span> x <span class="keyword">in</span> X])</span><br></pre></td></tr></table></figure>

<p>上述计算过程是遍历每一个x和y，部分相乘后相加，这样计算的效率并不高。另一种方式是使用矩阵的乘法，即<strong>内积(Dot Product 点乘)</strong></p>
<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>使用矩阵的内积，便可以用python中的向量相乘法则快速计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_mean = np.mean(x_train)</span><br><span class="line">y_mean = np.mean(y_train)</span><br><span class="line"></span><br><span class="line">a = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean)</span><br><span class="line">b = y_mean - a * x_mean</span><br></pre></td></tr></table></figure>

<p>用内积代替了循环遍历，计算性能上会有很大提升。对于1亿的数据量，两者使用时间：</p>
<pre><code>循环遍历： 55.904422
内积：    2.4315210000000036</code></pre><p>很多时候，<font color="red" size="4">算法原理的数学推导，最终要能变化成向量内积的形式，很重要</font>。</p>
<h1 id="评价现行模型的性能"><a href="#评价现行模型的性能" class="headerlink" title="评价现行模型的性能"></a>评价现行模型的性能</h1><p>评价线性模型主要用MSE, RMSE, MAE:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公式：</span></span><br><span class="line">mse = np.sum((y_predict - y_test)**<span class="number">2</span> / len(y_test))</span><br><span class="line">rmse = math.sqrt(mse)</span><br><span class="line">mae = np.sum(np.absolute(y_predict - y_test)) / len(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br></pre></td></tr></table></figure>

<p>相对来说，RMSE比MAE好，最小化RMSE，它表示错误样本中最大的错误值相应的比较小。而在线性回归中最好的指标要数R Squared。</p>
<h1 id="R-Squared"><a href="#R-Squared" class="headerlink" title="R Squared"></a>R Squared</h1><p>这个是线性回归最常用的性能指标。其定义为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r_squared = <span class="number">1</span> - (y_predict - y_test).dot(y_predict - y_test)</span><br><span class="line">               /</span><br><span class="line">               (y_means - y_test).dot(y_means - y_test)</span><br></pre></td></tr></table></figure>

<p>其中分子可以表示，使用我们的模型预测产生的错误。<br>而分母表示使用baseline 模型y=y_means 预测产生的错误。</p>
<p>分母的预测是不论x是多少，我都把他预测成y_means，错误率自然多。而分子是我们训练模型的实际预测，即我的模型实际拟合住样本的地方。</p>
<p>所以</p>
<ul>
<li>r_squared 是我的模型与baseline的比较。</li>
<li>两者相除小于等于1，</li>
<li>r_squared 越大，表示我们的模型越好</li>
<li>当r_squared = 1，表示，我的模型没有犯任何错。</li>
<li>当r_squared = 0，表示，我的模型性能等同于baseline。</li>
<li>当r_squared &lt; 0，表示，我的模型不如baseline，很可能，原数据不存在现<strong>线性关系</strong>。</li>
</ul>
<p>当r_squared 定义中分子分母同时除以测试样本个数，r_squared还可以写成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r_squared = <span class="number">1</span> - mse(y_predict, y_test)/var(y_test)</span><br></pre></td></tr></table></figure>
<p>其中var(y_test)，表示y_test 数据方差。所以r_squared 是由统计意义的。</p>
<p>在sklearn中使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"></span><br><span class="line">r2_score(y_predict, y_test)</span><br></pre></td></tr></table></figure>
<p>在sklearn中的LinearRegression模型里的 score成员方法返回的就是r_squared：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MODEL 表示任何模型的实例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(x_test, y_test)</span>:</span></span><br><span class="line">    y_predict = MODEL.predict(x_test)</span><br><span class="line">    <span class="keyword">return</span> r2_score(y_test, y_predict)</span><br></pre></td></tr></table></figure>

<h1 id="多元线性回归与Normal-Equation"><a href="#多元线性回归与Normal-Equation" class="headerlink" title="多元线性回归与Normal Equation"></a>多元线性回归与Normal Equation</h1><p>依旧可以使用最小二乘法得到最终解，即normal equation。但是计算机求解normal equation 的时间复杂度为O(n^3)。优点是根据数学解方程得到，不需要归一化处理。</p>
<p>sklearn中多元线性回归套路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line"><span class="comment"># print(boston.DESCR)  # (503, 13)</span></span><br><span class="line"><span class="comment"># print(boston.feature_names)</span></span><br><span class="line"></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除去异常点</span></span><br><span class="line">X = X[y &lt; <span class="number">50</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(lin_reg.coef_)      <span class="comment">#  得到最终模型所有参数</span></span><br><span class="line">print(lin_reg.intercept_)  <span class="comment"># 包括截距</span></span><br><span class="line">print(lin_reg.score(X_test, y_test))   <span class="comment"># 应用在测试数据集上的得分</span></span><br></pre></td></tr></table></figure>

<p>实现normal equation：</p>
<p>首先在原本X_train中加入一列全为1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br></pre></td></tr></table></figure>

<p>根据normal equation 的公式，其中intercept_表示直线上的截距，截距与样本特征无关，或者说，截距对应的特征是常数1。coef_表示coefficient，每个特征的系数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)  <span class="comment"># 完全根据公式</span></span><br><span class="line"></span><br><span class="line">intercept_ = _theta[<span class="number">0</span>]  <span class="comment"># 参数的第一列为截距</span></span><br><span class="line">coef_ = _theta[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>

<p>有了参数，就可以用于测试样本。写成函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X_sample)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> intercept_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> coef_ <span class="keyword">in</span> <span class="keyword">not</span> <span class="literal">None</span>, \</span><br><span class="line">        <span class="string">"Must fit before predict"</span></span><br><span class="line">    <span class="keyword">assert</span> X_sample.shape[<span class="number">1</span>] == len(coef_), \</span><br><span class="line">        <span class="string">"the feature number of X_sample must be equal to the lenght of coef_"</span></span><br><span class="line">    </span><br><span class="line">    X_b = np.hstack([np.ones((len(X_sample), <span class="number">1</span>)), X_sample])</span><br><span class="line">    <span class="keyword">return</span> X_b.dot(_theta)</span><br></pre></td></tr></table></figure>
<p>创建样本，使用模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_sample = np.array([[<span class="number">0.5</span>, <span class="number">20.</span>, <span class="number">4.</span>, <span class="number">0.</span>, <span class="number">0.57</span>, <span class="number">7.</span>, <span class="number">52.</span>, <span class="number">2.8</span>, <span class="number">5.</span>, <span class="number">264.</span>, <span class="number">13.</span>, <span class="number">390.</span>, <span class="number">3.16</span>]])</span><br><span class="line"></span><br><span class="line">lr.predict(X_sample)  <span class="comment"># 返回模型认为的预测值</span></span><br></pre></td></tr></table></figure>
<h2 id="线性回归的可解释性"><a href="#线性回归的可解释性" class="headerlink" title="线性回归的可解释性"></a>线性回归的可解释性</h2><p>对所有数据解normal equation 方程得所有系数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ -1.05574295e-01   3.52748549e-02  -4.35179251e-02   4.55405227e-01</span><br><span class="line">  -1.24268073e+01   3.75411229e+00  -2.36116881e-02  -1.21088069e+00</span><br><span class="line">   2.50740082e-01  -1.37702943e-02  -8.38888137e-01   7.93577159e-03</span><br><span class="line">  -3.50952134e-01]</span><br></pre></td></tr></table></figure>
<p>系数的正负表示，这个特征与预测指标(如房价)是正相关还是负相关。<br>正值表示正相关，越大表示这个特征越大房价越高。<br>负值表示负相关，越大表示这个特征越大，房价越低。 <br>系数的绝对值的大小表示了该特征对房价的影响程度。可以把特征的值从小到大排列，分析什么特征的影响大，什么特和那个的影响小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按系数排序，返回排序后的索引</span></span><br><span class="line">sort_index = np.argsort(lin_reg2.coef_)</span><br><span class="line"><span class="comment"># 使用排序后的索引，给特征名称排序 </span></span><br><span class="line">boston.feature_names[sort_index]</span><br></pre></td></tr></table></figure>
<p>比如，最大值系数表示房间数目。房间数目与房价正相关，且越大房价越高。合理。<br>最小系数对应一氧化氮浓度。浓度与房价成负相关，且值越大，房价越低。合理。</p>
<p>这就是“现行模型的可解释性”，有针对的采集更多数据，帮助决策。 </p>
<p><strong>注意</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])  </span><br><span class="line">np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
<p>前者是向量，大小为： (8,)<br><br>后者是矩阵，大小为： (2, 8)</p>
<p><font color="gree" size="5">敲黑板</font>机器学习算法实现不是目的，一个算法的优劣是将它放在特定任务中，与其他算法比较得出的。也就是说，单单训练好一个模型，还没结束，而是与其他训练好的模型一起评价，比如使用由confusion matrix 得到的指标：查准率，查全率，F-alpha，P-R曲线， ROC曲线等。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/" data-id="ckatsrgvd008exqfz5a1h4bst" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          LeetCode-方法论-回溯法
        
      </div>
    </a>
  
  
    <a href="/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Boosting-%E5%9B%9B/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">集成学习-Boosting-(四)</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">43</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 15px;">CUDA</a> <a href="/tags/Test-Analysis/" style="font-size: 10px;">Test Analysis</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/04/caffe-Blob-1/">caffe-Blob-(1)</a>
          </li>
        
          <li>
            <a href="/2020/06/04/caffe-%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/">caffe-所使用的数据格式</a>
          </li>
        
          <li>
            <a href="/2020/06/03/caffe-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8Epython%E6%8E%A5%E5%8F%A3/">caffe 命令行与python接口</a>
          </li>
        
          <li>
            <a href="/2020/06/03/cpp-lambda-function/">cpp-lambda function</a>
          </li>
        
          <li>
            <a href="/2020/06/03/pip%E4%B8%8B%E8%BD%BD%E5%8A%A0%E9%80%9F/">pip下载加速</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>