<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>CUDA-Performance Metrics | Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Implement Performance Metrics in CUDA“Before we jump into these performance measurement techniques, we need to discuss how to synchronize execution between the host and device.”why? 因为有些指令同步执行，有些指令异步执">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA-Performance Metrics">
<meta property="og:url" content="http://yoursite.com/2019/11/27/CUDA-Performance-Metrics/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:description" content="Implement Performance Metrics in CUDA“Before we jump into these performance measurement techniques, we need to discuss how to synchronize execution between the host and device.”why? 因为有些指令同步执行，有些指令异步执">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-11-27T14:46:48.000Z">
<meta property="article:modified_time" content="2020-01-12T14:53:12.988Z">
<meta property="article:author" content="Junhui">
<meta property="article:tag" content="CUDA">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-CUDA-Performance-Metrics" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/27/CUDA-Performance-Metrics/" class="article-date">
  <time datetime="2019-11-27T14:46:48.000Z" itemprop="datePublished">2019-11-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      CUDA-Performance Metrics
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Implement-Performance-Metrics-in-CUDA"><a href="#Implement-Performance-Metrics-in-CUDA" class="headerlink" title="Implement Performance Metrics in CUDA"></a>Implement Performance Metrics in CUDA</h1><p>“Before we jump into these performance measurement techniques, we need to discuss how to synchronize execution between the host and device.”<br>why? 因为有些指令同步执行，有些指令异步执行。只有知道了区别才可以正确测量性能。</p>
<p>遇到<code>cudaMemcpy()</code> 执行变成同步的，也就是说，所有指令必须等待其他指令执行到此，才可以一起向下继续执行。如果没有<code>cudaMemcpy()</code>，可以使用<code>cudaDeviceSynchronize()</code>实现同步。</p>
<h2 id="使用CPU的timer"><a href="#使用CPU的timer" class="headerlink" title="使用CPU的timer"></a>使用CPU的timer</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(d_x, x, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">cudaMemcpy(d_y, y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">t1 = myCPUTimer();</span><br><span class="line">saxpy&lt;&lt;&lt;(N+<span class="number">255</span>)/<span class="number">256</span>, <span class="number">256</span>&gt;&gt;&gt;(N, <span class="number">2.0</span>, d_x, d_y);</span><br><span class="line">cudaDeviceSynchronize();   <span class="comment">// </span></span><br><span class="line">t2 = myCPUTimer();</span><br><span class="line"></span><br><span class="line">cudaMemcpy(y, d_y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>

<p>“we use the explicit synchronization barrier <code>cudaDeviceSynchronize()</code> to block CPU execution until all previously issued commands on the device have completed. Without this barrier, this code would measure the kernel launch time and not the kernel execution time.” 在这里犯过错，CPU负责控制，当执行到kernel函数时，是CPU调用kernel函数，但是在GPU上执行，CPU调用之后，马上执行下面的语句，如果没有<code>cudaDeviceSynchronize()</code>，CPU会执行t2，如此一来t2-t1测的是调用kernel的时间，而非kernel执行的时间。也就是说，CPU与GPU是异步的，只有加上<code>cudaDeviceSynchronize()</code>，告诉CPU等待GPU把kernel执行完毕，后一同执行t2. </p>
<h2 id="Timing-using-CUDA-Events"><a href="#Timing-using-CUDA-Events" class="headerlink" title="Timing using CUDA Events"></a>Timing using CUDA Events</h2><p>A problem with using host-device synchronization points, such as <code>cudaDeviceSynchronize()</code>, is that they stall the GPU pipeline. For this reason, CUDA offers a relatively light-weight alternative to CPU timers via the CUDA event API. The CUDA event API includes calls to create and destroy events, record events, and compute the elapsed time in milliseconds between two recorded events.</p>
<p> A CUDA stream is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped—a property that can be used to hide data transfers between the host and the device </p>
<p>默认使用的stream 0，<br> Up to now, all operations on the GPU have occurred in the default stream, or stream 0 (also called the “Null Stream”).</p>
<p> here is an example:</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, stop;</span><br><span class="line">cudaEventCreate(&amp;start);</span><br><span class="line">cudaEventCreate(&amp;stop);</span><br><span class="line"></span><br><span class="line">cudaMemcpy(d_x, x, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">cudaMemcpy(d_y, y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">cudaEventRecord(start);</span><br><span class="line">saxpy&lt;&lt;&lt;(N+<span class="number">255</span>)/<span class="number">256</span>, <span class="number">256</span>&gt;&gt;&gt;(N, <span class="number">2.0f</span>, d_x, d_y);</span><br><span class="line">cudaEventRecord(stop);</span><br><span class="line"></span><br><span class="line">cudaMemcpy(y, d_y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">cudaEventSynchronize(stop);</span><br><span class="line"><span class="keyword">float</span> milliseconds = <span class="number">0</span>;</span><br><span class="line">cudaEventElapsedTime(&amp;milliseconds, start, stop);</span><br></pre></td></tr></table></figure>

<p> 这个计时器记录的是核函数的执行时间。</p>
<p> CUDA events are of type <code>cudaEvent_t</code> and are created and destroyed with <code>cudaEventCreate()</code> and <code>cudaEventDestroy()</code>. In the above code <code>cudaEventRecord()</code> places the start and stop events into the default stream, <code>stream 0</code>. The device will record a time stamp for the event when it reaches that event in the stream. The function <code>cudaEventSynchronize()</code> blocks CPU execution until the specified event is recorded. The <code>cudaEventElapsedTime()</code> function returns in the first argument the number of milliseconds time elapsed between the recording of start and stop. This value has a resolution of approximately one half microsecond.</p>
<h2 id="Memory-Bandwidth"><a href="#Memory-Bandwidth" class="headerlink" title="Memory Bandwidth"></a>Memory Bandwidth</h2><p> 我们需要知道极限带宽，和实际带宽。</p>
<h3 id="极限带宽（理论带宽）"><a href="#极限带宽（理论带宽）" class="headerlink" title="极限带宽（理论带宽）"></a>极限带宽（理论带宽）</h3><p> Theoretical bandwidth can be calculated using hardware specifications available in the product literature. For example, the NVIDIA Tesla M2050 GPU uses DDR (double data rate) RAM with a memory clock rate of <code>1,546 MHz</code> and a <code>384-bit</code> wide memory interface. Using these data items, the peak theoretical memory bandwidth of this GPU can be computed using the following:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BWTheoretical &#x3D; 1546 * 10^6 * (384&#x2F;8) * 2 &#x2F; 10^9 &#x3D; 148 GB&#x2F;s</span><br></pre></td></tr></table></figure>

<p>解释：In this calculation, we convert the memory clock rate to Hz, multiply it by the interface width (divided by 8, to convert bits to bytes) and multiply by 2 due to the double data rate. Finally, we divide by 109 to convert the result to GB/s.</p>
<h3 id="实际带宽"><a href="#实际带宽" class="headerlink" title="实际带宽"></a>实际带宽</h3><p>We calculate effective bandwidth by timing specific program activities and by knowing how our program accesses data. We use the following equation.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BWEffective &#x3D; (RB + WB) &#x2F; (t * 10^9)</span><br></pre></td></tr></table></figure>

<p>Here, <code>BWEffective</code> is the effective bandwidth in units of GB/s, <code>RB</code> is the number of bytes read per kernel, <code>WB</code> is the number of bytes written per kernel, and <code>t</code> is the elapsed time given in seconds.</p>
<p>实例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">__global__</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">saxpy</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">float</span> a, <span class="keyword">float</span> *x, <span class="keyword">float</span> *y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = blockIdx.x*blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; n) y[i] = a*x[i] + y[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N = <span class="number">20</span> * (<span class="number">1</span> &lt;&lt; <span class="number">20</span>);</span><br><span class="line">    <span class="keyword">float</span> *x, *y, *d_x, *d_y;</span><br><span class="line">    x = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">    y = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">    cudaMalloc(&amp;d_x, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)); </span><br><span class="line">    cudaMalloc(&amp;d_y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        x[i] = <span class="number">1.0f</span>;</span><br><span class="line">        y[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    cudaEventCreate(&amp;start);</span><br><span class="line">    cudaEventCreate(&amp;stop);</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(d_x, x, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_y, y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    cudaEventRecord(start);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Perform SAXPY on 1M elements</span></span><br><span class="line">    saxpy&lt;&lt;&lt;(N+<span class="number">511</span>)/<span class="number">512</span>, <span class="number">512</span>&gt;&gt;&gt;(N, <span class="number">2.0f</span>, d_x, d_y);</span><br><span class="line"></span><br><span class="line">    cudaEventRecord(stop);</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(y, d_y, N*<span class="keyword">sizeof</span>(<span class="keyword">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    cudaEventSynchronize(stop);</span><br><span class="line">    <span class="keyword">float</span> milliseconds = <span class="number">0</span>;</span><br><span class="line">    cudaEventElapsedTime(&amp;milliseconds, start, stop);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> maxError = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        maxError = max(maxError, <span class="built_in">abs</span>(y[i]<span class="number">-4.0f</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Max error: %fn"</span>, maxError);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Effective Bandwidth (GB/s): %fn"</span>, N*<span class="number">4</span>*<span class="number">3</span>/milliseconds/<span class="number">1e6</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>In the bandwidth calculation, N*4 is the number of bytes transferred per array read or write, and the factor of three represents the reading of x and the reading and writing of y. The elapsed time is stored in the variable milliseconds to make units clear. Note that in addition to adding the functionality needed for the bandwidth calculation, we have also changed the array size and the thread-block size.</p>
<p>CUDA events use the GPU timer and therefore avoid the problems associated with host-device synchronization</p>
<p>原文作者Mark Harris<br>原文<a href="https://devblogs.nvidia.com/how-implement-performance-metrics-cuda-cc/" target="_blank" rel="noopener">链接</a></p>
<hr>
<p><span style="font-family:Papyrus; font-size:2em;">CUDA</span></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/27/CUDA-Performance-Metrics/" data-id="ck71en4j4000fhefz6ot2hf0m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/11/28/CUDA-overlap-data-transfer/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          CUDA-overlap data transfer
        
      </div>
    </a>
  
  
    <a href="/2019/11/27/CUDA-Unified-Memory/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CUDA-Unified Memory</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning-Algorithms/">Deep Learning Algorithms</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Net-Algorithms/">Deep Net Algorithms</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">10</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">30</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hardware/" rel="tag">hardware</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 15px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 20px;">CUDA</a> <a href="/tags/hardware/" style="font-size: 10px;">hardware</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-stack/">LeetCode-方法论-stack</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%9D%82%E8%AE%B0%E5%BE%85%E5%BD%92%E7%B1%BB/">CUDA-杂记待归类</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95/">CUDA-扫描算法</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E5%86%8D%E7%9C%8B%E8%A7%84%E7%BA%A6-%E4%B8%80%E6%AE%B5%E8%A7%84%E7%BA%A6/">CUDA-再看规约-一段规约</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E5%86%8D%E7%9C%8B%E8%A7%84%E7%BA%A6-%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80/">CUDA-再看规约-循环展开</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>