<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Junhui&#39;s Journal">
<meta property="og:url" content="http://yoursite.com/page/7/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junhui">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-集成学习-ensemble-learning-二" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/" class="article-date">
  <time datetime="2019-08-20T15:43:46.000Z" itemprop="datePublished">2019-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/">集成学习-(二)-Bagging</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>集成学习可以分为两大类：</p>
<ul>
<li>学习器之间存在依赖关系，必须串行生成序列化方法。</li>
<li>学习器之间不存在依赖关系，可以同时生成的并行化方法。</li>
</ul>
<p>前者的代表是“Boosting”，后者的代表是“Bagging”和“随机森林(Random Forest)”.</p>
<p>得到泛化能力强的集成，每个学习器要尽量不同，如何做到不同。一个方法是由训练集产生多个不同的子集，在每个子集上训练学习器。如5000个样本，用5个学习器分别学习1000个样本集。如此产生5个不同的模型。但是如此一来，每个模型的性能会有所下降，同时，集成学习的一个优势是每个学习器并不需要具有很强的性能。</p>
<p>但是，“好而不同”毕竟每个学习器要“好”。所以根据每个学习器只使用数据的一部分，产生两种采样方式：</p>
<ul>
<li>有放回抽样 自助采样(Bootstrap Sampling)</li>
<li>无放回抽样</li>
</ul>
<p>自助采样：假设原始数据集由m个样本，对于第一个学习器，随机采样一个样本，放入采样集中，后把该样本放回原数据集。如此采样m次便得到含有m个样本的子集来训练学习器#1。对于其他学习器，采用同样的方式得到训练集。如此得到的不同子集中一定存在重复的元素。最后基于每一个子集训练学习器，后集成。此过程成为Bagging。</p>
<p>对于无放回抽样，就如上述所述，5000个样本是数据集，若分给5个学习器，每个得到1000个样本；10 个学习器，每个得到500个样本。这种方法成为Pasting。其局限性很显然，学习器个数与其子集样本数成反比。</p>
<p>Bagging常用。例：<br>所使用数据即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x, y = datasets.make_moons(n_samples=<span class="number">1000</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">111</span>)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">111</span>)</span><br></pre></td></tr></table></figure>
<p>使用决策树为基学习器，设置5个，采样为放回采样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"></span><br><span class="line">bagg = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                         n_estimators=<span class="number">5</span>, max_samples=<span class="number">100</span>, bootstrap=<span class="literal">True</span>)</span><br><span class="line">bagg.fit(x_train, y_train)</span><br><span class="line">print(bagg.score(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p>测试集的正确率为：94.4%</p>
<p>设置500个基学习器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bagg2 = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                         n_estimators=<span class="number">500</span>, max_samples=x_train.shape[<span class="number">0</span>], bootstrap=<span class="literal">True</span>)</span><br><span class="line">bagg2.fit(x_train, y_train)</span><br><span class="line">print(bagg2.score(x_test, y_test))</span><br></pre></td></tr></table></figure>

<p>测试集准确率： 96.0%</p>
<h2 id="Out-of-bag-Estimate"><a href="#Out-of-bag-Estimate" class="headerlink" title="Out-of-bag Estimate"></a>Out-of-bag Estimate</h2><p>可以计算，通过自助采样，原始数据集中大约有36.8%的样本未出现在每个学习器的样本子集中。所以对于Bagging，天然的，在原始数据集中就有测试集了，即那剩下的原始数据集中的36.7%。</p>
<p>使用sklearn 中的oob_score_ 实现oob：<br>在放回采样过程中，记录那些样本没有被取到，这些未被取到的作为验证集或测试集，这个过程由oob_score=True 实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bagg3 = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                          n_estimators=<span class="number">500</span>, max_samples=x.shape[<span class="number">0</span>],</span><br><span class="line">                          bootstrap=<span class="literal">True</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">bagg3.fit(x, y)</span><br><span class="line">print(bagg3.oob_score_)</span><br></pre></td></tr></table></figure>

<p>结果0.958。</p>
<h2 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h2><p>由于每个学习器没有相互影响，所以所有学习器可以同时学习：<br>指定n_jobs的值，当其为-1时，表示使用计算机所有物理核心：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 并行执行，使用所有核心</span></span><br><span class="line">bagg4 = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                          n_estimators=<span class="number">500</span>, max_samples=x.shape[<span class="number">0</span>],</span><br><span class="line">                          bootstrap=<span class="literal">True</span>,</span><br><span class="line">                          oob_score=<span class="literal">True</span>,</span><br><span class="line">                          n_jobs=<span class="number">-1</span>)</span><br><span class="line">start2 = clock()</span><br><span class="line">bagg4.fit(x, y)</span><br><span class="line">end2 = clock()</span><br><span class="line">print(bagg4.oob_score_)</span><br><span class="line">print(end2 - start2)</span><br></pre></td></tr></table></figure>
<p>计时得到最终执行时间：0.1591450000000001<br>而使用一个核心的执行时间为：0.6764169999999998</p>
<h2 id="更多采样方式"><a href="#更多采样方式" class="headerlink" title="更多采样方式"></a>更多采样方式</h2><p>当数据集的特征较多时，可是对特征进行随机采样：Random Subspaces。只在列上随机采样。如下左图。<br><br>另一种采样方式，既对样本随机采样，又对特征的随机采样：Random Patches。既在行又在列上随机采样。如下右图：</p>
<div align="center"><img src="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/sampling.png" width="600"></div>

<p>既然有对样本数量的自助采样(bootstrap sampling)，也有对样本特征的自助采样。如此得到的特征为bootstrap features：</p>
<p>Random Subspaces，只对特征进行随机采样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭样数量的随机采样</span></span><br><span class="line">random_subspaces_bag = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                                        n_estimators=<span class="number">500</span>,</span><br><span class="line">                                        max_samples=x.shape[<span class="number">0</span>],  </span><br><span class="line">                                        max_features=<span class="number">1</span>,</span><br><span class="line">                                        bootstrap=<span class="literal">True</span>,</span><br><span class="line">                                        bootstrap_features=<span class="literal">True</span>,</span><br><span class="line">                                        oob_score=<span class="literal">True</span>,</span><br><span class="line">                                        n_jobs=<span class="number">-1</span>)</span><br><span class="line">start3 = clock()</span><br><span class="line">random_subspaces_bag.fit(x, y)</span><br><span class="line">end3 = clock()</span><br><span class="line">print(random_subspaces_bag.oob_score_)</span><br><span class="line">print(end3 - start3)</span><br></pre></td></tr></table></figure>
<p>结果为： 0.835<br>运行时间：0.1876460000000002</p>
<p>Random Patches，既对样本数据量采样，又对特征进行采样：<br>因为原始数据只有2个特征，所以此处指采样一个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## random patches 既有样本数的随机采样， 又有特征的随机采样：</span></span><br><span class="line">random_patches_bag = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                                          n_estimators=<span class="number">500</span>,</span><br><span class="line">                                          max_samples=<span class="number">200</span>,</span><br><span class="line">                                          max_features=<span class="number">1</span>,</span><br><span class="line">                                          bootstrap=<span class="literal">True</span>,</span><br><span class="line">                                          bootstrap_features=<span class="literal">True</span>,</span><br><span class="line">                                          oob_score=<span class="literal">True</span>,</span><br><span class="line">                                          n_jobs=<span class="number">-1</span>)</span><br><span class="line">start3 = clock()</span><br><span class="line">random_patches_bag.fit(x, y)</span><br><span class="line">end3 = clock()</span><br><span class="line">print(random_patches_bag.oob_score_)</span><br><span class="line">print(end3 - start3)</span><br></pre></td></tr></table></figure>
<p>结果为：0.897<br>运行时间： 0.16168400000000016</p>
<p>对于图像信息，除了pooling采样操作，还可以使用这两种采样操作。</p>
<p><font color="gree" size="5">敲黑板</font></p>
<pre><code>”好而不同“是集成学习的核心。
”不同“的实现方式之一是”采样“：对样本数量采样，对样本特征采样。
即，使用一部分数据训练基学习器。</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/" data-id="ck7bjjf60000vi5fzczqjd075" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-集成学习-ensemble-learning-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-20T14:28:32.000Z" itemprop="datePublished">2019-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%B8%80/">集成学习(ensemble-learning)-(一)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>类似，生病了找多名专家从不同方面一起确诊。</li>
<li>集成学习将多个学习器进行结合，通常可以获得比单一的学习器更优的泛化性能。</li>
<li>集成学习可以得带3中不同结果：提升性能，不起作用，起负作用。</li>
<li>所以要想获得好的集成结果，每个学习器应该“好而不同”，即每个学习器要有一定的准确性，同时，每个学习器各不相同。即“多样性”。</li>
<li>实际上，如何产生“好而不同”的基学习器，是集成学习的核心。</li>
</ul>
<h1 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h1><p>使用数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x, y = datasets.make_moons(n_samples=<span class="number">500</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">321</span>)</span><br></pre></td></tr></table></figure>
<p>构建三个基学习器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1） 使用逻辑回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log = LogisticRegression()</span><br><span class="line">log.fit(x_train, y_train)</span><br><span class="line">print(log.score(x_test, y_test))   <span class="comment">#  0.896</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2) 使用SVM</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(x_train, y_train)</span><br><span class="line">print(svm.score(x_test, y_test))   <span class="comment">#  0.952</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3) 使用决策树分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">tree = DecisionTreeClassifier()</span><br><span class="line">tree.fit(x_train, y_train)</span><br><span class="line">print(tree.score(x_test, y_test))  <span class="comment">#  0.944</span></span><br></pre></td></tr></table></figure>
<p>结果分别为 0.896, 0.952, 0.944.</p>
<p>投票操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># voting</span></span><br><span class="line">y_pre1 = log.predict(x_test)</span><br><span class="line">y_pre2 = svm.predict(x_test)</span><br><span class="line">y_pre3 = tree.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 少数服从多数</span></span><br><span class="line"><span class="comment"># 对于三个模型的预测值，至少2个模型判断它为1，我才认为它是1：</span></span><br><span class="line">y_p = np.asarray((y_pre1 + y_pre2 + y_pre3) &gt;= <span class="number">2</span>, dtype=int)</span><br><span class="line">print(y_pre1[:<span class="number">10</span>])   <span class="comment">#  [1 1 1 1 1 0 1 0 0 0]</span></span><br><span class="line">print(y_pre2[:<span class="number">10</span>])   <span class="comment">#  [1 1 1 1 1 0 1 0 0 1]</span></span><br><span class="line">print(y_pre3[:<span class="number">10</span>])   <span class="comment">#  [1 1 1 1 1 0 1 0 0 1]</span></span><br><span class="line">print(y_p[:<span class="number">10</span>])      <span class="comment">#  [1 1 1 1 1 0 1 0 0 1]</span></span><br></pre></td></tr></table></figure>

<p>判断集成效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">acc = accuracy_score(y_test, y_p)</span><br><span class="line">print(acc)     <span class="comment">#  0.96</span></span><br></pre></td></tr></table></figure>
<p>最终集成性能是0.96。</p>
<h2 id="Hard-voting"><a href="#Hard-voting" class="headerlink" title="Hard voting"></a>Hard voting</h2><p>hard voting 即少数服从多数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"></span><br><span class="line">voting = VotingClassifier(estimators=[</span><br><span class="line">    (<span class="string">'log'</span>, LogisticRegression()),</span><br><span class="line">    (<span class="string">'svm'</span>, SVC()),</span><br><span class="line">    (<span class="string">'tree'</span>, DecisionTreeClassifier())</span><br><span class="line">], voting=<span class="string">'hard'</span>)</span><br><span class="line"></span><br><span class="line">voting.fit(x_train, y_train)</span><br><span class="line">print(voting.score(x_test, y_test))    <span class="comment">#  0.96</span></span><br></pre></td></tr></table></figure>

<p>实际中，通常把基学习器调参到最优，后再集成。</p>
<h2 id="Soft-voting"><a href="#Soft-voting" class="headerlink" title="Soft voting"></a>Soft voting</h2><p>hard voting 其实是“少数服从多数”，而 soft voting 带权投票，如歌唱的专业评审团的投票权重就大。如：</p>
<p>5个学习器把同一个样本分为A类或B类的概率分别如下：</p>
<pre><code>            A类    B类
学习器#0   99.0%  1.0%
学习器#1   49.0%  51.0%
学习器#2   43.0%  57.0%
学习器#3   98.0%  2.0%
学习器#4   34.0%  64.0%</code></pre><p>如果使用hard voting 该样本最终本分为B类。</p>
<p>但是，显然学习器#0和#3有很大的把握认为该样本属于A类，而其他三个学习器并没有很确定该样本的类别。所以“少数服从多数”不合适。使用Soft voting 计算<br>属于A类：(99.0%+49.0%+43.0%+98.0%+34.0%) / 5 = 64.6%. <br>属于B类：(1.0%+51.0%+57.0%+2.0%+64.0%) / 5 = 35.0%.<br> 所以该样本应该被分类为A。</p>
<p>使用 soft voting 的基学习器要求都应该估计概率，即可以调用 predict_proba 函数。SVM模型把probability设为true，便可以进行概率估计。调用VotingClassifier时指明voting方式为soft即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line">voting2 = VotingClassifier(estimators=[</span><br><span class="line">    (<span class="string">'log'</span>, LogisticRegression()),</span><br><span class="line">    (<span class="string">'svm'</span>, SVC(probability=<span class="literal">True</span>)),</span><br><span class="line">    (<span class="string">'tree'</span>, DecisionTreeClassifier())</span><br><span class="line">], voting=<span class="string">'soft'</span>)</span><br><span class="line"></span><br><span class="line">voting2.fit(x_train, y_train)</span><br><span class="line">print(voting2.score(x_test, y_test))   <span class="comment">#  0.976</span></span><br></pre></td></tr></table></figure>

<p>所以，当基学习器可以求出样本概率估计时，Soft voting 比 hard voting 性能优。 </p>
<p><font color="green" size="5">敲黑板</font>理解当前问题，才能找到已有方法的不足，才能找到更合适的方法。就如本文表达的用Soft voting 替代已有的Hard voting。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%B8%80/" data-id="ck7bjjf60000si5fz59f5drtq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回顾决策树-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-19T16:00:11.000Z" itemprop="datePublished">2019-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/">回顾决策树(一)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>决策树是非参数学习算法</li>
<li>天然解决多分类问题</li>
<li>有很好的可解释性</li>
<li>关键问题是使用哪个特征做为根节点</li>
<li>对于连续值的特征，在哪个值上做划分</li>
</ul>
<p>使用iris数据集，调用sklearn的Decision Tree 模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只是用数据2个特征</span></span><br><span class="line">x = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">tree_clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>, criterion=<span class="string">"entropy"</span>)</span><br><span class="line">tree_clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(tree_clf, axis=[<span class="number">0.5</span>, <span class="number">7.5</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">0</span>, <span class="number">0</span>], x[y == <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">2</span>, <span class="number">0</span>], x[y == <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">1</span>, <span class="number">0</span>], x[y == <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align="center"><img src="/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/tree1.png" width="500"></div>

<p>划分后使得系统的熵，即不确定性降低。所以对于一个划分，如果划分后的系统信息熵比其他划分后的熵都要小，则当前就是用这个划分。根据这个原理，可以就特征为连续值的数据集进行划分：</p>
<p>已知特征d，和value， 对x进行划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(x, y, d, value)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    index_a = (x[:, d] &lt;= value)</span><br><span class="line">    index_b = (x[:, d] &gt; value)</span><br><span class="line">    <span class="keyword">return</span> x[index_a], x[index_b], y[index_a], y[index_b]</span><br></pre></td></tr></table></figure>

<p>传入label列表，求此时的系统信息熵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_entropy</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="comment"># counter 为字典，[类别，这个类别所含样本数]</span></span><br><span class="line">    counter = Counter(y)  </span><br><span class="line">    res = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num / len(y)</span><br><span class="line">        res += -p * log2(p)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>定义一次划分，即，搜索找到是熵最小的特征d 和value：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_split</span><span class="params">(x, y)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始值最大</span></span><br><span class="line">    best_entropy = float(<span class="string">'inf'</span>)  </span><br><span class="line">    <span class="comment"># best_e_l, best_e_r = -1, -1</span></span><br><span class="line">    <span class="comment"># 初始化d 和 value</span></span><br><span class="line">    best_d, best_v = <span class="number">-1</span>, <span class="number">-1</span>   </span><br><span class="line">    <span class="comment"># 在x 的所有维度（特征）搜索d：</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="comment"># 在特征d的哪个值上划分：先排序，后找相邻的样本在特征d上的中间值是多少</span></span><br><span class="line">        sorted_index = np.argsort(x[:, d])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(x)):  <span class="comment"># 遍历所有样本</span></span><br><span class="line">            <span class="keyword">if</span> x[sorted_index[i<span class="number">-1</span>], d] != x[sorted_index[i], d]:</span><br><span class="line">                v = (x[sorted_index[i<span class="number">-1</span>], d] + x[sorted_index[i], d]) / <span class="number">2</span></span><br><span class="line">                <span class="comment"># 有了 d 和 v， split：</span></span><br><span class="line">                x_l, x_r, y_l, y_r = split(x, y, d, v) </span><br><span class="line">                 <span class="comment"># 此时系统熵是多少</span></span><br><span class="line">                e = cal_entropy(y_l) + cal_entropy(y_r)</span><br><span class="line">                <span class="comment"># e_l = cal_entropy(y_l)</span></span><br><span class="line">                <span class="comment"># e_r = cal_entropy(y_r)</span></span><br><span class="line">                <span class="comment"># 判断新的熵e是否小于best_entropy，更新best_entropy</span></span><br><span class="line">                <span class="keyword">if</span> e &lt; best_entropy:</span><br><span class="line">                    best_entropy, best_v, best_d = e, v, d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_entropy, best_d, best_v</span><br></pre></td></tr></table></figure>

<p>调用函数得第一次划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一次划分：</span></span><br><span class="line">best_entropy, best_d, best_v = try_split(x, y)</span><br><span class="line">x1_l, x1_r, y1_l, y1_r = split(x, y, best_d, best_v)</span><br><span class="line"></span><br><span class="line">print(cal_entropy(y1_l))  <span class="comment"># 0.0  </span></span><br><span class="line">print(cal_entropy(y1_r))  <span class="comment"># 1.0</span></span><br></pre></td></tr></table></figure>
<p>左子树的熵为0，右子树的熵大于0，所以对右子树进行第二次划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于右边再次划分：</span></span><br><span class="line">best2_entropy, best2_d, best2_v = try_split(x1_r, y1_r)</span><br><span class="line">x2_l, x2_r, y2_l, y2_r = split(x1_r, y1_r, best2_d, best2_v)</span><br><span class="line"></span><br><span class="line">print(cal_entropy(y2_l))  <span class="comment"># 0.44506485705083865</span></span><br><span class="line">print(cal_entropy(y2_r))  <span class="comment"># 0.15109697051711368</span></span><br></pre></td></tr></table></figure>
<p>可以继续深入划分。</p>
<p>由于数据集各个特征值是连续的，所以，上述过程通过遍历找到所划分的特征，再遍历这个特征的所有样本，排序后求相邻两个样本的均值作为这个特征的具体划分值，时间复杂度为O(mxn)，即样本个数乘以特征个数。</p>
<p>对于各个特征值为离散的，使用<strong>信息增益</strong>来找到消除不确定性最强的特征。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/" data-id="ck7bjjf5t0009i5fzhp0123x1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回看SVM-三" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/" class="article-date">
  <time datetime="2019-08-18T14:42:57.000Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/">回看SVM-(三)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在现实任务中，样本空间内很可能不存在一个能将样本正确划分的超平面。此时，可以将样本从原始空间映射到一个更高维的样本空间，使得样本在这个高维空间内线性可分。如果原始空间是有限维的，也就是说，样本的属性个数有限，那么一定存在一个更高的样本维度使样本线性可分。</p>
<p>上述涉及到每一个特征映射到高维空间后之间的内积，由于特征空间的维度可能很高，甚至是无穷维的，因此先映射到高维空间后计算，是非常困难的。即，核函数可以等价代替<strong>样本映射后的特征向量间的内积</strong>。使用Kernel Trick 避开这个障碍。如此用一个近似的计算来取代映射到高维特征向量。好处：减少了计算量，节省了内存，Kernel Trick是一个可以应用与任何算法的技巧。</p>
<h1 id="RBF核"><a href="#RBF核" class="headerlink" title="RBF核"></a>RBF核</h1><p>加入原始数据只有一维信息，如果有两个landmark，就将数据变化为2维数据。每个样本的取值变化为x-&gt;( ，)，即含有2个特征。下面模拟该过程： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维数据x</span></span><br><span class="line">x = np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应的lable</span></span><br><span class="line">y = np.asarray((x &gt;= <span class="number">-3</span>) &amp; (x &lt;= <span class="number">3</span>), dtype=int)</span><br><span class="line"></span><br><span class="line">plt.scatter(x[y == <span class="number">0</span>], <span class="number">0</span>*(y[y == <span class="number">0</span>]))</span><br><span class="line">plt.scatter(x[y == <span class="number">1</span>], <span class="number">0</span>*(y[y == <span class="number">1</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>如图：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-1.png" width="500"></div>

<p>定义一个函数，每个样本跟landmark进行计算，得到新的x：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, l)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    每个x 跟l计算得到新的x</span></span><br><span class="line"><span class="string">    :param x: 原来x</span></span><br><span class="line"><span class="string">    :param l: x跟谁计算</span></span><br><span class="line"><span class="string">    :return: 新的x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    gamma = <span class="number">0.05</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(-gamma * (x - l)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>对x的每个样本进行升维处理，得到新的x：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.empty((len(x), <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(x):</span><br><span class="line">    <span class="comment"># x_new 的第一个特征：</span></span><br><span class="line">    x_new[i, <span class="number">0</span>] = gaussian(data, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># x_new 的第二个特征：</span></span><br><span class="line">    x_new[i, <span class="number">1</span>] = gaussian(data, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(x_new[y == <span class="number">0</span>, <span class="number">0</span>], x_new[y == <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x_new[y == <span class="number">1</span>, <span class="number">0</span>], x_new[y == <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>如图：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-2.png" width="500"></div>

<p>此2维数据显然是线性可分的。本例是使用l1和l2 两个landmark，即，只将数据变化为2维，原本20x1的数据映射成20x2。BRF高斯核实际上是使用每一个数据点作为landmark，即原本mxn的数据映射成mxm的数据。当m远远大于n时(一般合格的数据集)，数据特征由n增加到m。当然，对于一个m小于n的数据集(如NLP等情境下的数据)，依然变换为mxm，其实是降维处理。</p>
<h1 id="RBF核gamma"><a href="#RBF核gamma" class="headerlink" title="RBF核gamma"></a>RBF核gamma</h1><p>RBF中的参数gamma与高斯分布有关：gamma越大，分布越窄越高。<br>实验如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用moon数据：</span></span><br><span class="line">x, y = datasets.make_moons(noise=<span class="number">0.15</span>, random_state=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RBFKernel</span><span class="params">(gamma=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([(<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">                     (<span class="string">"svc"</span>, SVC(kernel=<span class="string">"rbf"</span>, gamma=gamma))])</span><br></pre></td></tr></table></figure>
<p>指定gamma，并学习，当gamma=100时：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">svc = RBFKernel(<span class="number">100</span>)</span><br><span class="line">svc.fit(x, y)</span><br><span class="line">plot_decision_boundary(svc, axis=[<span class="number">-1.5</span>, <span class="number">2.5</span>, <span class="number">-1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">0</span>, <span class="number">0</span>], x[y == <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">1</span>, <span class="number">0</span>], x[y == <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>绘图：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-3.png" width="500"></div>

<p>当gamma=100，很大， 对应高斯分布越高越窄， 反映在图中就是每一个橘色样本点周围的小区域为一个高斯分布，橘色样本点本身为分布的顶点，即，模型判断，样本点只在这样一个区域内，才被判定为橘色点。 此情况显然过拟合。</p>
<p>当gamma=10， 上述的分布区域变大了：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-4.png" width="500"></div>

<p>当gamma=0.1，很小时，不拟合：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-5.png" width="500"></div>


<p>所以可以说，gamma值在控制模型的复杂度，gamma越小，模型复杂度越小，<br>所以要找到合适的gamma。</p>
<h1 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h1><p>使用SVM解决回归问题。SVR 使得在margin范围里的样本点越多越好，表示这个范围可以较好地表示样本点。此时取中间的线为回归曲线。</p>
<p>或者说，SVR解决的问题与SVM相反，soft SVM要使得margin间的点越少越好，而SVR要使得margin间的点越多越好。</p>
<p>又或者说，SVR目的与SVM相同，都是最大化margin间的距离。SVR假设我们可以容忍f(x)与y之间最多有epislon的偏差，即，<strong>当且仅当f(x)与y之间的距离绝对值大于epsilon时，才计算损失，损失仍是最小化margin间的距离</strong>。这个过程相当于构建了宽度为2×epsilon的间隔带，<strong>如果样本点在间隔带之间，则认为是被预测正确的</strong>。</p>
<p>sklearn中的使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVR</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR  <span class="comment"># 指明核函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">StandardLinearSVR</span><span class="params">(epsilon=<span class="number">0.1</span>, c=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    指明参数 epsilon 和 C 等</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"LinearSVR"</span>, LinearSVR(epsilon=epsilon, C=c))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/" data-id="ck7bjjf5n0003i5fzhepb19ry" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回看SVM-二" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/" class="article-date">
  <time datetime="2019-08-18T13:18:45.000Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/">回看SVM-(二)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>对于线性不可分而非线性可分的数据，也可以使用线性SVM。只需要将数据转换为高维的含有多项式项的数据。</p>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>先找一个，分线性可分数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">x, y = datasets.make_moons(noise=<span class="number">0.15</span>, random_state=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制出结果：</span></span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>, <span class="number">0</span>], x[y==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>, <span class="number">0</span>], x[y==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/svm-moon.png" width="500"></div>


<h1 id="含多项式特征的LinearSVM"><a href="#含多项式特征的LinearSVM" class="headerlink" title="含多项式特征的LinearSVM"></a>含多项式特征的LinearSVM</h1><p>指定多形式的阶数，将数据转化成高维的含有多项式项上午数据，后传入LinearSVM：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomialSVC</span><span class="params">(degree, C=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">"Poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">        (<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"LinearSVC"</span>, LinearSVC(C=C))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<p>使用Pipline可以顺序执行各部分。实例化模型，并查看模型信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poly_svc = PolynomialSVC(<span class="number">3</span>)</span><br><span class="line">poly_svc.fit(x, y)</span><br><span class="line">print(poly_svc)</span><br></pre></td></tr></table></figure>
<p>模型信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Pipeline(memory=<span class="literal">None</span>,</span><br><span class="line">     steps=[(<span class="string">'Poly'</span>, PolynomialFeatures(degree=<span class="number">3</span>, include_bias=<span class="literal">True</span>, interaction_only=<span class="literal">False</span>)), (<span class="string">'standard'</span>, StandardScaler(copy=<span class="literal">True</span>, with_mean=<span class="literal">True</span>, with_std=<span class="literal">True</span>)), (<span class="string">'LinearSVC'</span>, LinearSVC(C=<span class="number">0.1</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">True</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">     intercept_scaling=<span class="number">1</span>, loss=<span class="string">'squared_hinge'</span>, max_iter=<span class="number">1000</span>,</span><br><span class="line">     multi_class=<span class="string">'ovr'</span>, penalty=<span class="string">'l2'</span>, random_state=<span class="literal">None</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">     verbose=<span class="number">0</span>))])</span><br></pre></td></tr></table></figure>
<p>绘出分类边界：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_boundary(poly_svc, axis=[<span class="number">-1.5</span>, <span class="number">2.5</span>, <span class="number">-1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>, <span class="number">0</span>], x[y==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>, <span class="number">0</span>], x[y==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果为分线性边界：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/svm-moon-2.png" width="500"></div>

<h1 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h1><p>SVM可以直接使用数据的多项式特征，即多项式核函数，所以使用SVC而非LinearSVC。此时Pipline中不需先得到多项式特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Polynomial_KernelSVC</span><span class="params">(degree, C=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"kernelSVC"</span>, SVC(kernel=<span class="string">"poly"</span>, degree=degree, C=C))  </span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<p>传入kernel参数，指定使用什么样的核函数。实例化模型，绘制分类边界：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">poly_kernel_svc = Polynomial_KernelSVC(degree=<span class="number">5</span>)</span><br><span class="line">poly_kernel_svc.fit(x, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(poly_kernel_svc, axis=[<span class="number">-1.5</span>, <span class="number">2.5</span>, <span class="number">-1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>, <span class="number">0</span>], x[y==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>, <span class="number">0</span>], x[y==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>边界：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/svm-moon-3.png" width="500"></div>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/" data-id="ck7bjjf5l0001i5fz65869xyx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-信息论-信息量与二叉树" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/" class="article-date">
  <time datetime="2019-08-18T09:46:24.000Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/">信息论-信息量与二叉树</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>香浓认为，对于一条信息，重要的是找出这条信息中含有多少信息量，要搞清楚信息量，就要找到一个量化度量信息的单位。香浓的最大贡献就是找到了这个单位“比特(bit)”。<br>比特是度量信息量的基本单位。它可以被这样定义：<em>如果一个黑盒子里由A和B两种可能，而且这两种可能出现的概率相同。那么要搞清楚黑盒子中到底是A还是B，所需要的信息量就是一个比特(bit)</em>。<br>当A和B的出现的概率不相同时，确定他们谁出现所需的信息量就不到一个比特。</p>
<p>比如，设抛一枚两面质地均匀的硬币为一个系统，状态A和B分别是正面朝上和反面朝上。则确定一次投掷所需要的信息量为一个比特。<br>可以这样计算：</p>
<div align="center"><img src="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/bit-1.png" width="400"></div>

<p>又比如，做一道4(有A,B,C,D4中状态)选一个的选择题，需要多少信息量才能找到正确答案。2bits。先使用1bit信息确定比如是否在A或B中，若是，再使用1bit信息量确定是否是A，若否，则最终答案是B。(这只是一种判断出结果的方法，但每种方法所需的信息量都是2bits)：</p>
<div align="center"><img src="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/bit-2.png" width="400"></div>

<p>所以，如果由32个足球队参加世界杯，最终需要5bits的信息量确定最终哪支球队为冠军。</p>
<p>可以看出的规律：一棵树，以所有可能情况为根，<strong>平均</strong>分所有情况给左子右子，如此递归，直到每个叶节点只有一种情况。此时得到树的深度即为确定做种是哪种情况所需的信息量。以选择题为例：</p>
<div align="center"><img src="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/bit-3.png" width="400"></div>

<p>树的深度为2，从树根到树叶需要2步走，即需要2bits信息确定究竟是哪一个。</p>
<p>我们把充满不确定性的盒子称作“信息源”，它里面的不确定性称作“信息熵”，“信息”是用来消除这些不确定性的(信息熵)。所以要消除黑盒子里的不确定性，需要的“信息量”等于“信息熵”。“熵”是热力学的概念，它表示混乱程度，同样的，信息论中的信息熵表示一个系统中的不确定性。</p>
<p>一个系统中的状态数量也叫做<strong>可能性数量，越多，系统不确定性越大；当状态数量确定时，各个状态的可能性相同，熵达到最大；相反，如果只要概率有不相同，系统不确定性就会减小，极限情况下，当状态A发生的概率为99.9%，而其他所有状态发生的概率和为0.01%时，系统的不确定性很小：很大的概率上A是会发生的</strong>。香浓用以下公式可以计算出系统信息熵：</p>
<img src="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/bit-4.png">

<p>以二分类为例，下图体现了一个状态的概率与系统的信息熵的关系：</p>
<div align="center"><img src="/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/bit-5.png" width="500"></div>

<p>当状态概率为0.5时，系统信息熵，即不确定性达最大值。实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据上述信息熵计算公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(p)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -p * np.log2(p)-(<span class="number">1</span>-p)*np.log2(<span class="number">1</span>-p)</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0.01</span>, <span class="number">0.99</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, entropy(x))</span><br><span class="line"><span class="comment"># 绘出一个点</span></span><br><span class="line">plt.scatter([<span class="number">0.5</span>], [<span class="number">1</span>], marker=<span class="string">'o'</span>, edgecolors=<span class="string">'r'</span>)   </span><br><span class="line"><span class="comment"># 添加注释 </span></span><br><span class="line">plt.text(<span class="number">0.5</span>, <span class="number">1</span>, (<span class="number">0.5</span>, <span class="number">1</span>), ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>, fontsize=<span class="number">10</span>)   </span><br><span class="line">plt.xlabel(<span class="string">'Pr(x)'</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Entropy'</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>在构建ID3决策树时，就是根据信息熵公式，得到各种划分后的信息熵，选取信息熵最小的划分。即这个划分使得系统不确定性变得最小！</p>
<p>如四选一选择题，假如，每种选项的概率均为0.25，根据公式，消除选择题的不确定性所需信息量为2bits。<br>如果4中选项的概率不相同，不论哪种情况，信息熵均小于2bits。</p>
<p>通过平衡二叉树来理解信息量单位比特。如果一个分类模型在一个数据集上的正确率为0.5，那么可以说，这个模型是最糟糕的模型，它对减少系统信息熵没有贡献。</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>热力学熵和信息熵的物理意义，数学形式，完全一样，不存在本质区别，只是应用领域不同。克劳修斯形式(熵的宏观形式)用于内燃机研究，进入量子时代后主要使用玻尔兹曼形式(熵的微观形式)。<br>玻尔兹曼熵与信息熵的唯一不同只是前者计算对数用e为底数得到单位为nats，后者计算使用2为底数得到单位为bit。<br><br>另外，取10为底，得到单位是bans。其实，在决策树的计算中，求信息熵的对数底可以使任意的，因为具体在划分的时候，不管底取多少，H这个函数的性质是不会变的。模型其实在意熵的相对值，而非绝对值。</p>
<p>熵增原理：任何孤立的系统，总熵不会减少。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/18/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E4%BF%A1%E6%81%AF%E9%87%8F%E4%B8%8E%E4%BA%8C%E5%8F%89%E6%A0%91/" data-id="ck71en4kc003ehefz8qga0gv2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Style-Transfer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/Style-Transfer/" class="article-date">
  <time datetime="2019-08-12T17:47:38.000Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/13/Style-Transfer/">Style-Transfer</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本笔记是风格转换应用的概述。</p>
<p>本应用需要使用使用已训练好的VGG16网络的结构及参数。VGG16.npy文件中保存了网络的结构和参数，这些参数可以通过解析网络得到，之后便可以重构整个网络，包括每一个conv layer，每一个pooling layer，每一个fully connected layer和flatten layer，以及每一层所需的w，b。如下图：</p>
<div align="center"><img src></div>

<p>有了每一层的参数，可以构建若干conv layer 和 pooling layer。 由于本应用不使用flatten layer 和 fc layer 的输出结果，而且VGG16 大部分的参数集中在fc layer，所以不用构建此二层。</p>
<p>当含有训练好参数的网络构建好后，或者说函数<code>y=f(x)</code>及所有参数都定义好了。此时当输入<code>f(x)</code>一个<code>x</code>，便可以得到一个对应的<code>y</code>。</p>
<p>本应用需要使用三个一模一样网络y=f(x)，一个传入content_img，可以得到f(p)每一层的输出；第二个网络传入style_img，可以得到f(a)的每一层输出；第三个网络传入自变量x，f(x)。x是一个2D向量，其初始值为一个随机初始化的向量。本应用就是使随机化的x通过学习逐渐变成一个混合content_img和style_img的图像。</p>
<p>这个应用实际上也是个学习的过程，此过程需要3个Loss值：<code>Lcontent</code>，<code>Lstyle</code>，<code>Ltotal</code>。其中Lcontent是p传入f在M层的输出与x传入f在同一层的输出做平方差；Lstyle是a传入f在N层的输出的Gram矩阵与x传入同一层的输出的Gram矩阵的平方差；最后得到整个学习过程的Loss函数Ltotal：<code>Ltotal=alpha*Lcontent+beta*Lstyle</code>。Ltotal是一个以x为参数的函数，通过不断更新x的值来最小化Ltotal。同样是学习过程，不同于CNN应用在分类问题，Loss是以x为学习参数，w和b是定值，每次学习都是更新x。</p>
<p><font color="Blue">迭代数学公式</font></p>
<p>实验通过改变alpha 和 beta的值来改变风格转化的程度。特别的，当alpha！=0，beta=0时，Ltotal只与Lcontent有关，这种情况下，学习过程变成了：x通过学习逐渐变成p(content img)；当alpha=0，beta！=0时，x通过学习逐渐变成a(style img)。</p>
<p>有趣的是，要想使x学习成p，需要使用f(p)在浅层的输出值；而要想是x学习成a，需要使用f(a)早深层的输出值。所以最终为了使混合图像的效果好，f(p)使用浅层输出，f(a)使用深层输出。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/13/Style-Transfer/" data-id="ck74tgkug0004i2fz1eoh76db" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-更新线程ID" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/12/CUDA-%E6%9B%B4%E6%96%B0%E7%BA%BF%E7%A8%8BID/" class="article-date">
  <time datetime="2019-08-12T06:13:44.000Z" itemprop="datePublished">2019-08-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/12/CUDA-%E6%9B%B4%E6%96%B0%E7%BA%BF%E7%A8%8BID/">CUDA-更新线程ID</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="更新-thread-ID"><a href="#更新-thread-ID" class="headerlink" title="更新 thread ID"></a>更新 thread ID</h1><p>当数据个数 <code>≥ CUDA core</code> 个数时，<code>thread id</code> 不用更新，一次同时习性完。</p>
<p>如我有2024个数据，而我的GPU每次可分配2048个threads。我的<code>kernel</code>配置<code>&lt;&lt;&lt;1, 2048&gt;&gt;&gt;</code>，那么2024个数据可以被2024个threads一次执行完毕，其中有24个threads空闲，因为因为没有多余的数据去处理。</p>
<p>如果还是2024个数据，而我的GPU只允许用户一次配置512个threads。我的kernel配置是&lt;&lt;&lt;1, 512&gt;&gt;&gt;，一次是不能把数据全部处理完的。当第一次处理完后，还要3次threads的id更新，最后一次有24个threads空闲，因为没有多余出具需要被处理。</p>
<p>kernel代码片段如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> id = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line"><span class="keyword">while</span>(id &lt; N)&#123;</span><br><span class="line">    <span class="comment">// TODO excute operation</span></span><br><span class="line">    id += blockDIm.x * gridDim.x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中<code>while()</code>判断当前thread的id需要更新多少次。</p>
<ul>
<li><p>对于<code>&lt;&lt;&lt;1, 2048&gt;&gt;&gt;</code>的kernel，处理2024个数据，其中<code>N=2024</code>。假如其中一个thread的起始<code>id</code>为0，干完活后，判断<code>0＜2024</code>，所以这个thread的<code>id</code>会被跟新为2048，此时再判断<code>2048＜2024</code>，返回<code>false</code>，这个thread的工作结束。thread的id未被更新。</p>
</li>
<li><p>对于&lt;&lt;&lt;1, 512&gt;&gt;&gt;的kernel，处理2024个数据， 其中N=2024。<br>仍假如有一个thread是起始id为0，判断0＜2024，执行操作。所以跟新id为512；<br>判断512＜2024，执行操作，再更新id为1024；<br>判断1024＜2024，执行操作，再更新id为1536；<br>判断1536＜2024，执行操作，再更新id为2048；<br>判断2048＜2024，返回false。该thread的工作结束。</p>
</li>
</ul>
<p>期间这个thread的id被更新了4次，第4次更新玩后，并无操作。</p>
<h1 id="Matrix-Transpose-理解id更新"><a href="#Matrix-Transpose-理解id更新" class="headerlink" title="Matrix Transpose 理解id更新"></a>Matrix Transpose 理解id更新</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/12/CUDA-%E6%9B%B4%E6%96%B0%E7%BA%BF%E7%A8%8BID/" data-id="ck71en4jk001ihefzgf0a7i8d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-基本步骤-逻辑概念-物理概念" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/" class="article-date">
  <time datetime="2019-08-10T13:03:23.000Z" itemprop="datePublished">2019-08-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/">CUDA-基本步骤-逻辑概念-物理概念</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一个实例"><a href="#一个实例" class="headerlink" title="一个实例"></a>一个实例</h2><p>检查环境：</p>
<ul>
<li><code>nvcc -V</code>：CUDA编译器是否安装</li>
<li><code>nvidia-smi</code>：显卡驱动是否安装</li>
</ul>
<p>cuda代码文件以<code>.cu</code>结尾，当写好一个文件后，使用NVIDIA 的编译器编译 <code>nvcc FILE-NAME.cu</code>，后<code>./FILE-NAME</code>执行。</p>
<p>从一个实例讲起：<br>两个向量相加，结果存入另一个向量。<br>代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 1&lt;&lt;10  </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ARRAY_BYTES N*sizeof(float)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) add __global__ to kernel, AKA device code</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">float</span>* x, <span class="keyword">float</span>* y, <span class="keyword">float</span>* z)</span></span>&#123;  </span><br><span class="line">	<span class="keyword">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">	<span class="keyword">if</span> (tid &lt; N)&#123;</span><br><span class="line">		z[tid] = x[tid] + y[tid];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//host code (runs on cpu)</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//allocate mem on steak for a,b,c </span></span><br><span class="line">	<span class="keyword">float</span> h_a[N], h_b[N], h_c[N];</span><br><span class="line"></span><br><span class="line">	<span class="comment">//declare pointers in gpu   </span></span><br><span class="line">	<span class="keyword">float</span> *dev_a, *dev_b, *dev_c;  </span><br><span class="line"></span><br><span class="line">	<span class="comment">//allocate mem in gpu </span></span><br><span class="line">	cudaMalloc((<span class="keyword">void</span>**)&amp;dev_a, ARRAY_BYTES);</span><br><span class="line">	cudaMalloc((<span class="keyword">void</span>**)&amp;dev_b, ARRAY_BYTES);</span><br><span class="line">	cudaMalloc((<span class="keyword">void</span>**)&amp;dev_c, ARRAY_BYTES);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">//initialize a, b  arrays in the host</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;N;i++)&#123;</span><br><span class="line">		h_a[i] = i * <span class="number">1.0f</span>;</span><br><span class="line">		h_b[i] = i * <span class="number">2.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//copy a, b to gpu</span></span><br><span class="line">	cudaMemcpy(dev_a, h_a, ARRAY_BYTES, cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(dev_b, h_b, ARRAY_BYTES, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">	<span class="comment">//run kernel on 1M elements on the CPU</span></span><br><span class="line">	add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1024</span>&gt;&gt;&gt;(dev_a, dev_b, dev_c);  <span class="comment">//one block and 1024 threads</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//copy c back from gpu</span></span><br><span class="line">	cudaMemcpy(h_c, dev_c ,ARRAY_BYTES, cudaMemcpyDeviceToHost);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//display results</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%f + %f = %f \n"</span>, h_a[i], h_b[i], h_c[i]);</span><br><span class="line"></span><br><span class="line">	<span class="comment">//2) free memory</span></span><br><span class="line">	cudaFree(dev_a);</span><br><span class="line">	cudaFree(dev_b);</span><br><span class="line">	cudaFree(dev_c);</span><br><span class="line"></span><br><span class="line">	<span class="comment">//new &amp; delete go together. </span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码包含了CUDA代码的一般步骤：</p>
<ul>
<li><p>1）声明所需指针在GPU，并且在GPU上开辟空间， 使用函数<code>cudaMalloc()</code></p>
</li>
<li><p>2）从CPU拷贝所需内容到GPU的内存中，使用函数<code>cudaMemcpy()</code></p>
</li>
<li><p>3）配置核函数，并执行操作。该函数在main函数之外，以<code>__global__</code>开头</p>
</li>
<li><p>4）把GPU上计算得到的结果拷贝回RAM，使用函数<code>cudaMemcpy()</code></p>
</li>
<li><p>5）释放VRAM中的空间</p>
  <div align="center"><img src="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/main-process.png" width="600"></div>


</li>
</ul>
<pre><code>从两个维度理解CUDA基本概念</code></pre><ul>
<li><strong>物理层</strong></li>
<li><strong>逻辑层</strong></li>
</ul>
<h2 id="物理概念"><a href="#物理概念" class="headerlink" title="物理概念"></a>物理概念</h2><p>CUDA中的两个对象：Host，Device</p>
<ul>
<li>Host 包括 CPU 和内存 DRAM</li>
<li>Device 包括 GPU 和存在与其上的存储 VRAM</li>
</ul>
<p>VRAM 是 off-chip memory，即不在芯片上。由三部分组成：Global Memory，Texture Memory 和 Constant Memory。其中后二者为 read-only。<br>GPU 芯片上的 memory 包括 Registers，l1-cache, 和 Shared Memory。</p>
<p>每个 GPU 芯片拥有一组不同的 memory，如上述。其中最重要的两个是 Global Memory 和 Shared Memory。<br>Global Memory 类似CPU系统的 RAM，Shared Memory 相当于CPU的片内缓存。</p>
<p>Host 和 Device 由北桥芯片连接：</p>
<div align="center"><img src="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/host-device.png" width="500"></div>

<p>对于CUDA编程，你需要负责以下内容：</p>
<p>1) 在 GPU memory 上开辟空间<br>2) 拷贝数据从CPU上到GPU上<br>3) 在GPU上执行 kernel 代码<br>4) 再把结果从GPU上考回CPU<br>5) 协调Host 和 Device中的操作</p>
<p>使用GPU编程时，要从 MIMD(Multiple Instructions Multiple Data) 的思考形式转变到 SIMD(Single Instruction Multiple Data)，在CUDA 中，每个核心执行的代码指令都是一样的，所以说是Single。</p>
<p>CUDA 提供的重要功能：组织线程，memory access。</p>
<p>与CUDA并行编程的代码分为两部分：</p>
<ul>
<li>Host 部分代码由 ANSI C 来完成</li>
<li>Device 部分由 CUDA C 来完成</li>
</ul>
<p>知道怎样组织 threads 在使用 CUDA 是十分重要的。</p>
<h2 id="逻辑概念"><a href="#逻辑概念" class="headerlink" title="逻辑概念"></a>逻辑概念</h2><p>在一个 grid 中所有的 thread 共享相同的 Global Memory。来自不同 block 的 threads 不能相互交流。即属于同一个 block 的 thread 可以相互交流。   </p>
<ul>
<li>threadIdx.x: 每个 block 中 x 方向 thread 的 id</li>
<li>threadIdx.y: 每个 block 中 y 方向 thread 的 id</li>
<li>threadIdx.z: 每个 block 中 z 方向 thread 的 id</li>
</ul>
<ul>
<li>blockIdx.x: 每个 block 的 x 方向上所含的 id</li>
<li>blockIdx.y: 每个 block 的 y 方向上所含的 id</li>
<li>blockIdx.z: 每个 block 的 z 方向上所含的 id</li>
</ul>
<ul>
<li>blockDim.x: 每个 block 的 x 方向上所含的 thread 数</li>
<li>blockDim.y: 每个 block 的 y 方向上所含的 thread 数</li>
<li>blockDim.z: 每个 block 的 z 方向上所含的 thread 数</li>
</ul>
<ul>
<li>gridDim.x: 每个 grid 的 x 方向上的 block 数</li>
<li>gridDim.y: 每个 grid 的 y 方向上的 block 数</li>
<li>gridDim.z: 每个 grid 的 z 方向上的 block 数</li>
</ul>
<p>grids 和 blocks 使用 dim3 数据类型。 当给了数据的大小，如何决定 grid &amp; block 的维度。</p>
<ul>
<li>1）先决定 block 大小，即每个 block 由多少 threads，</li>
<li>2）然后根据数据大小和 block 大小，计算 grid dim。</li>
</ul>
<p>为了得到 block dim，考虑两点：</p>
<ul>
<li>1）kernel 的性能特点</li>
<li>2）GPU的物理极限<ul>
<li>我的芯片的数据：</li>
</ul>
</li>
</ul>
<p>func&lt;&lt;&lt;32, 1024&gt;&gt;&gt;()：</p>
<ul>
<li>32：block 的数量为32个</li>
<li>1024：每一个 block 的 thread 数为1024个</li>
</ul>
<p>为了配置 kernel 你需要知道：</p>
<ul>
<li>1）kernel 的 thread 总数</li>
<li>2）这些 threads 的分布：block &amp; grid 的维数，每个 block 由多少 threads</li>
</ul>
<p>举例子：<br>假如我想使用32个threads，我想 配置一个<em>1D grid 1D block</em> kernel func&lt;&lt;&lt;4, 8&gt;&gt;&gt;()，其 thread 分布是：</p>
<div align="center"><img src="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/1d.png"></div>

<p><strong>32个 threads 决定了会有32 份func()的拷贝，每一份由一个thread 执行，唯一不同的是每一个thread 的ID</strong>，这样计算：<br><code>idx=threadIdx.x + blockDim.x * blockIdx.x</code>。<br>如第二个block的第一个thread 的ID是 <code>0+8*1=8</code>，最后一个thread的ID是<code>7+8*3=31</code>，所以，这个配置中的所有threads由唯一的ID：<code>0~31</code>。</p>
<p>再如：<em>2D grid，2D block</em> kernel：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">threads</span><span class="params">(<span class="number">2</span>, <span class="number">4</span>)</span></span></span><br><span class="line"><span class="function">dim3 <span class="title">blocks</span><span class="params">(<span class="number">2</span>, <span class="number">4</span>)</span></span></span><br><span class="line">func&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>下图是所有相关的参数，及怎样得到每个 thread 的 ID：</p>
<div align="center"><img src="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/2d-2d.png"></div>
其中每个矩形代表一个 block：blockDim.x=4，blockDim.y=2。每个 block 中的 thread 的组织是4行2列。

<p>更多kernel的配置：</p>
<div align="center"><img src="/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/more-configure.png" width="600"></div>

<p>其中矩形表示一个block，相邻blocks组织成grid。</p>
<p>核心概念：<br>grid 由 block 组成，可以是1D，2D，和3D。<br>block 由 thread 组成，也可以是1D，2D，和3D。<br>根据具体问题，选择 block/grid 的维度。一般来说：<br>1D 适用于 vector 操作<br>2D 适用于 images<br>3D 适用于 3D space</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/10/CUDA-%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4-%E9%80%BB%E8%BE%91%E6%A6%82%E5%BF%B5-%E7%89%A9%E7%90%86%E6%A6%82%E5%BF%B5/" data-id="ck71en4jf0019hefz7qmiffz1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Deep-Learning-知识点" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Deep-Learning-%E7%9F%A5%E8%AF%86%E7%82%B9/" class="article-date">
  <time datetime="2019-08-08T14:48:42.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Deep-Learning-%E7%9F%A5%E8%AF%86%E7%82%B9/">Deep Learning 知识点</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>杂记：</p>
<h1 id="Loss-Fucntion与训练"><a href="#Loss-Fucntion与训练" class="headerlink" title="Loss Fucntion与训练"></a>Loss Fucntion与训练</h1><ul>
<li><p>为啥会有Loss Function目标函数：如果要调整一个东西，让它符合一个事件的话，首先需要定义需要符合的事件是啥，这就是目标函数。也就是说，要基于什么样的目标来调整这个东西（模型）。 </p>
</li>
<li><p>常使用的目标函数：</p>
<ul>
<li>平方差损失函数。样本标签值与预测值的距离之和</li>
<li>交叉熵损失函数。衡量两个分布间的差距</li>
</ul>
</li>
<li><p>训练目的：调整参数，使得模型在训练集上的损失函数值最小。</p>
</li>
<li><p>如何训练：直接解方程？不得行。所以有了以GD为代表的学习算法。</p>
</li>
<li><p>使用<code>Mini-batch梯度下降</code>。如果每一次都在整个数据集上计算梯度，计算量巨大，可能内存不够。如果使用随机梯度(<code>SGD</code>)下降，即每使用一个样本就计算一次梯度，在一个样本上得到的梯度不能反应整个数据集的梯度方向，所以收敛速度慢。所以一般使用<code>Mini-batch梯度下降</code>。</p>
</li>
<li><p>上述三种梯度下降法都存在一个缺点：会陷入局部最小值和鞍点。可以使用动量梯度下降(<code>SGD+Momentum</code>)。其特点是,如果当前步的方向与上一步的方向呈锐角，实际方向在二者之间，且步长较大，若是钝角，则步长小。也就是说:</p>
<ul>
<li>刚开始积累动量，加速学习。</li>
<li>局部震荡时，梯度为0，由于动量的存在，跳出局部最优。</li>
<li>梯度方向改变时，动量缓解震荡。</li>
</ul>
</li>
</ul>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><ul>
<li><p>为啥有CNN。在传统的NN处理图像时，由于全连接，导致</p>
<ol>
<li><p>计算量大</p>
</li>
<li><p>参数过多，使得过拟合</p>
<p>CNN灵感来与生物的视觉感受野，其特点是局部连接，每个神经元在图片上移动时，其卷积核参数不变（参数共享），而不同的神经元间的卷积核参数不同。相比较于全连接，参数量减少了很多。参数共享：一个卷积核提取一个特征。所以CNN解决上述两个问题：</p>
</li>
<li><p>局部连接</p>
</li>
<li><p>参数共享</p>
</li>
</ol>
</li>
<li><p>一个卷积核3通道分别与输入的3通道计算，得到的3通道计算结果相加作为输出的一个通道。如下图：</p>
<ul>
<li><p>即一个卷积核得到一个输出通道，所以n个卷积核得到n个输出通道。</p>
</li>
<li><p>一个卷积核提取一个特征，所以6个卷积核提取6个特征。</p>
</li>
<li><p>其中<code>28 =（32-5)/1+1</code>，即<code>输出长=(输入长-卷积核长)/stride步长+1</code>(步长为1)。<br></p>
<div align="center"><img src="/2019/08/08/Deep-Learning-%E7%9F%A5%E8%AF%86%E7%82%B9/conv.png" width="600"></div>

<p>注意图中的参数个数和计算量的计算与输入尺寸无关。参数计算=（核长×核宽）×（输入通道数×输出通道数）。与图中一致。</p>
</li>
</ul>
</li>
</ul>
<h1 id="激活函数选择（添加图示）"><a href="#激活函数选择（添加图示）" class="headerlink" title="激活函数选择（添加图示）"></a>激活函数选择（添加图示）</h1><ul>
<li>线性激活函数，对于网络无效，因为是全等变换，无论网络由多少层，都相当于只有一层。我们说激活函数的作用是非线性变换，而现行变换无效果。</li>
<li>深层网络不适用<code>sigmoid</code>，因其计算复杂，输出均值非0，且会发生梯度消失，即更深的层参数得不到更新。</li>
<li><code>tanh</code>虽然输出均值为0，但计算复杂，有和<code>sigmoid</code>相同的问题。</li>
<li><code>ReLU</code>首次在AlexNet中使用，计算简单，但输出均值非0，会存在dead 神经元。</li>
<li><code>Leaky ReLU</code>解决了<code>ReLU</code>的dead 神经源问题。</li>
<li><code>ELU</code>输出均值接近于0，</li>
<li><code>MaxOut</code>是<code>ReLU</code>的泛华版本，没有dead神经元，但参数会翻倍。</li>
</ul>
<p>总结：一般使用<code>Leaky ReLU</code>，<code>ELU</code>，<code>MaxOut</code>。</p>
<h1 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h1><p>max-pooling,average-pooling, 特点：</p>
<ul>
<li>池化核移动不重叠，</li>
<li>不补零，</li>
<li>无可训练过参数，</li>
<li>作用：降采样，为下一层减少尺寸，减少计算量，减少训练参数，</li>
<li>平移鲁棒性</li>
</ul>
<h1 id="全连接层FC"><a href="#全连接层FC" class="headerlink" title="全连接层FC"></a>全连接层FC</h1><p>将上一层输出展开并连接到每一个神经元上。<code>2D</code>变<code>1D</code>，没有了<code>2D</code>信息，所以不能再加卷基层。实际上就是普通的<code>NN</code>。全连接层的参数很多，一般占整个模型参数量的60%~80%。</p>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><ul>
<li><p>SGD：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> true:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    x += lr * dx</span><br></pre></td></tr></table></figure>
</li>
<li><p>动量SGD，解决鞍点和局部最优值，问题，每个维度的学习速率都一样：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Vx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> true:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    Vx = rho * Vx + dx</span><br><span class="line">    x += lr * Vx</span><br></pre></td></tr></table></figure>
</li>
<li><p>AdaBrad：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> true:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    grad_sqaured += dx * dx    <span class="comment"># 积累平方梯度</span></span><br><span class="line">    x -= lr * dx / (np.sqrt(grad_sqaured)+<span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>

<p>  缺点：当learning rate较大时，分母敏感，使梯度爆炸。后期分母变大，使更新变得很小，提前结束训练。</p>
</li>
<li><p>RMSProp：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> true:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    grad_sqaured = delay_rate * grad_sqaured + (<span class="number">1</span>-delay_rate) * dx * dx    <span class="comment"># 平均平方梯度</span></span><br><span class="line">    x -= lr * dx / (np.sqrt(grad_sqaured)+<span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>

<p>  解决了后期训练提前结束的问题。</p>
</li>
<li><p>Adam：</p>
<p>  结合了动量SGD和RMSProp优点</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">frst_moment = <span class="number">0</span></span><br><span class="line">scnd_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> true:</span><br><span class="line">    dx = compute_gradient(x)</span><br><span class="line">    frst_moment = beta1 * frst_moment + (<span class="number">1</span>-beta1) * dx</span><br><span class="line">    scnd_moment = beta2 * scnd_moment + (<span class="number">1</span>-beta2) * dx * dx</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 校准</span></span><br><span class="line">    frst_unbias = frst_moment/(<span class="number">1</span>-beta1**t)</span><br><span class="line">    scnd_unbias = scnd_moment/(<span class="number">1</span>-beta2**t)</span><br><span class="line">    x -= lr * frst_unbias / (np.sqrt(scnd_unbias)+<span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>

<p>  使得开始训练时frst_moment和scnd_moment变大来加速训练。一般使用Adam算法，从经验来讲，beta1=0.9， beta2=0.999，lr=1e-3。</p>
</li>
<li><p>lr自适应方法：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Exponetial Decay:</span></span><br><span class="line">lr = lr * e**(-kt)</span><br><span class="line"><span class="comment"># 1/t Decay:</span></span><br><span class="line">lr = lr / (<span class="number">1</span>+kt)</span><br></pre></td></tr></table></figure>
<p>  SGD训练时间长但效果好，不过需要好的初始化，和lr自适应，如果不能找到好的初始化和lr自适应，用Adam。如果要训练更深更复杂的网络，且要求收敛速度快，推荐使用Adam。</p>
</li>
</ul>
<h1 id="网络需要初始化-添加图示"><a href="#网络需要初始化-添加图示" class="headerlink" title="网络需要初始化 (添加图示)"></a>网络需要初始化 (添加图示)</h1><p>好的初始化使训练速度快，且达到一个好的结果。多层网络不适用0来初始化。</p>
<p>如何判断初始化的好坏？查看初始化后各层的激活函数的分布，如果分布在一个区间内，好；如果输出集中在某个值上，不好。<br>一般使用均值为0方差为0.02的正态分布来初始化。<br>对于<code>tanh</code>使用<code>Xavior</code>初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.rand(channel_in, channel_out)/np.sqrt(channel_in)</span><br></pre></td></tr></table></figure>

<p>对于ReLU使用<code>He-ReLU</code>初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.rand(channel_in, channel_out)/np.sqrt(channel_in/<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>其网络每一层分布都相近。</p>
<h1 id="批归一化"><a href="#批归一化" class="headerlink" title="批归一化"></a>批归一化</h1><p>通用的归一化方法：</p>
<ul>
<li><p>为了使得每层激活函数的分布一致，在得到激活函数值后，把输出做归一化处理：减去均值，除以方差，使得其值分布在均值为0，方差为1 的分分布上。</p>
</li>
<li><p>缺点：表示在每一批上做归一化，但是，一批的分布不能反应整个数据集的分布，结果是，得到一个特征，批归一化后，这个特征不能在批不批间区分样本了，即特征无效了。</p>
</li>
<li><p>解决：设置alpha&amp;beta两个参数做逆归一化：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数学公式 填坑</span><br></pre></td></tr></table></figure>

<h1 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h1><p>多尺度剪裁。</p>
<h1 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h1><p>在已经训练好的网络上进行微调。使用已经微调好的模型来初始化。见Fine-tuning博客。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Deep-Learning-%E7%9F%A5%E8%AF%86%E7%82%B9/" data-id="ck79dofgw0000vhfz6a0c6h8i" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/6/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/8/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">38</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hardware/" rel="tag">hardware</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 16.67px;">CUDA</a> <a href="/tags/Test-Analysis/" style="font-size: 13.33px;">Test Analysis</a> <a href="/tags/hardware/" style="font-size: 10px;">hardware</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/28/CUDA-%E5%B9%B6%E8%A1%8C%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF/">CUDA-并行一维卷积</a>
          </li>
        
          <li>
            <a href="/2020/02/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-stack/">LeetCode-方法论-stack</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%9D%82%E8%AE%B0%E5%BE%85%E5%BD%92%E7%B1%BB/">CUDA-杂记待归类</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95/">CUDA-扫描算法</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E5%86%8D%E7%9C%8B%E8%A7%84%E7%BA%A6-%E4%B8%80%E6%AE%B5%E8%A7%84%E7%BA%A6/">CUDA-再看规约-一段规约</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>