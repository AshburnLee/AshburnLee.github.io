<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Junhui&#39;s Journal">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junhui">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-LeetCode-求一个数的平方根sqrt-x" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9sqrt-x/" class="article-date">
  <time datetime="2020-06-12T13:44:55.000Z" itemprop="datePublished">2020-06-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LeetCode/">LeetCode</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9sqrt-x/">LeetCode-求一个数的平方根sqrt(x)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>描述</p>
<p>  如题</p>
</li>
<li><p>思路</p>
<p>  使用基础算法，二分查找容易求出<code>sqrt(x)</code>。如果对精度有要求，即要求精确到小数点后n位：<code>sqrt(x, n)</code>，只不过此时二分查找的<code>head</code>和<code>tail</code>的偏移不再是1，而是与精度有关。</p>
</li>
<li><p>实现</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 没有精度的要求</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">//int compute_count=0;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// my sqrt without accuracy</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> head = <span class="number">0</span>, tail=x;</span><br><span class="line">        <span class="keyword">while</span>(head&lt;=tail)&#123;</span><br><span class="line">            <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> middle = head + (tail-head)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(middle * middle == x)</span><br><span class="line">                <span class="keyword">return</span> middle;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(middle * middle &lt; x)</span><br><span class="line">                head = middle+<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                tail = middle<span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tail;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// my sqrt with accuracy</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">int</span> acc)</span></span>&#123;</span><br><span class="line">        <span class="keyword">double</span> head = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> offset = <span class="built_in">pow</span>(<span class="number">10</span>, -acc);</span><br><span class="line">        <span class="keyword">double</span> tail = x - offset;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"Accuracy: "</span>&lt;&lt;offset&lt;&lt;<span class="string">" result: "</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(tail&gt;=head)&#123;</span><br><span class="line">            <span class="comment">//compute_count++;</span></span><br><span class="line">            <span class="keyword">double</span> mid = head + (tail-head)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(mid * mid == x)</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(mid * mid &lt; x)</span><br><span class="line">                head = mid + offset;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                tail = mid - offset;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">return</span> offset!=<span class="number">1</span>?min(tail,head):tail;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Solution sol;</span><br><span class="line">    <span class="keyword">double</span> in;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;in;</span><br><span class="line">    <span class="comment">// 调用无精度要求的函数</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(in))&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// 不同的精度要求</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">0</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">1</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">2</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">3</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">4</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">5</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;sol.mySqrt(in,<span class="number">6</span>)&lt;&lt;<span class="string">" | true: "</span>&lt;&lt;<span class="built_in">sqrt</span>(in)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  返回如下，可以看出，结果的精度在随要求精度提高：</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">87 <span class="comment"># 输入87</span></span><br><span class="line">9</span><br><span class="line">Accuracy: 1 result: 9</span><br><span class="line">Accuracy: 0.1 result: 9.32656</span><br><span class="line">Accuracy: 0.01 result: 9.32678</span><br><span class="line">Accuracy: 0.001 result: 9.32655</span><br><span class="line">Accuracy: 0.0001 result: 9.32743</span><br><span class="line">Accuracy: 1e-05 result: 9.32738</span><br><span class="line">Accuracy: 1e-06 result: 9.32738 | <span class="literal">true</span>: 9.32738</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><font color="gree" size="5">敲黑板</font>：注意边界条件，注意特殊情况。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9sqrt-x/" data-id="ckbcav8uq00007nfzapsu843x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-LeetCode-求一个数的n次幂" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84n%E6%AC%A1%E5%B9%82/" class="article-date">
  <time datetime="2020-06-12T13:04:34.000Z" itemprop="datePublished">2020-06-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LeetCode/">LeetCode</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84n%E6%AC%A1%E5%B9%82/">LeetCode-求一个数的n次幂</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><p>问题描述：</p>
<p>  求x的n次方<code>power(x, n)</code>，如<code>power(2,4) = 16</code>。</p>
</li>
<li><p>思路：</p>
<p>  很直接的思路，x与自己相乘n次的结果，事假复杂度为O(N)。这个方法有个问题：很多的重复计算。以power(2,4)为例，power(2,4) = (2x2)x(2x2)。(2x2)被计算了两次，当n很大时，就会由更多的重复计算。其实power(2,4) = power(2,2)xpower(2,2)。所以可以很容易写出递归的主体：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">double</span> half = power(x, n/<span class="number">2</span>);</span><br><span class="line"><span class="keyword">double</span> res = half*half;</span><br></pre></td></tr></table></figure>
<p>  时间复杂度为O(logN)。而递归终止条件和其他细节可从下图中自然得到：</p>
<p>  【过程图】</p>
</li>
<li><p>实现：</p>
<p>  由过程图可以实现如下</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">myPow3</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">int</span> n)</span></span>&#123; </span><br><span class="line">    <span class="comment">// 递归终止条件：</span></span><br><span class="line">    <span class="keyword">if</span> (n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1.0</span>;</span><br><span class="line">    <span class="comment">// 特殊情况：</span></span><br><span class="line">    <span class="keyword">if</span> (x==<span class="number">0.0</span>) <span class="keyword">return</span> <span class="number">1.0</span>;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> nn=n;</span><br><span class="line">    <span class="comment">// 当x为负数</span></span><br><span class="line">    <span class="keyword">if</span>(nn&lt;<span class="number">0</span>)&#123;</span><br><span class="line">        nn=-nn;</span><br><span class="line">        x=<span class="number">1</span>/x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 递归主体：</span></span><br><span class="line">    <span class="keyword">double</span> half = myPow3(x, nn/<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">double</span>  ans = half*half;</span><br><span class="line">    <span class="comment">// 这个细节在过程图中很容易看出：</span></span><br><span class="line">    <span class="keyword">return</span> n%<span class="number">2</span>==<span class="number">0</span> ? ans : ans*x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><font color="gree" size="5">敲黑板</font>：递归直接写很容易出错，把过程图画出来，一些细节就会很容易发现。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84n%E6%AC%A1%E5%B9%82/" data-id="ckbc9fx290000infzgxddhof1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-阅读Log运行日志" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/11/caffe-%E9%98%85%E8%AF%BBLog%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97/" class="article-date">
  <time datetime="2020-06-10T19:14:11.000Z" itemprop="datePublished">2020-06-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/11/caffe-%E9%98%85%E8%AF%BBLog%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97/">caffe-阅读Log运行日志</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>接着笔记<code>caffe-数据&amp;模型-模型输出log</code>，继续阅读Log。是关于内存磁盘间的通信。</p>
<h1 id="从prototxt描述转换城内存表示方式"><a href="#从prototxt描述转换城内存表示方式" class="headerlink" title="从prototxt描述转换城内存表示方式"></a>从prototxt描述转换城内存表示方式</h1><p>要构建一个Net，就需要解析两个超参数文件。对于模型结构参数文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解析</span></span><br><span class="line">I0610 04:53:36.096751 13919 solver.cpp:102] Creating training net from net file: /media/junhui/DATA/caffe_workspace/my_linearReggresion/mylr.prototxt</span><br><span class="line"><span class="comment"># 这两行前面解释过了，区分哪些层用于TRAIN，哪些层用于TEST</span></span><br><span class="line">I0610 04:53:36.097002 13919 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule <span class="keyword">in</span> layer mnist</span><br><span class="line">I0610 04:53:36.097012 13919 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule <span class="keyword">in</span> layer accuracy</span><br><span class="line">I0610 04:53:36.097085 13919 net.cpp:53] Initializing net from parameters:</span><br></pre></td></tr></table></figure>

<p>找到对应位置：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::InitTrainNet() &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// param_是一个SolverParameter对象，如果mylr.prototxt文件中定义了Net结构，则如下</span></span><br><span class="line">    <span class="keyword">if</span> (param_.has_net()) &#123;</span><br><span class="line">    LOG_IF(INFO, Caffe::root_solver()) <span class="comment">// 打印Log</span></span><br><span class="line">        &lt;&lt; <span class="string">"Creating training net from net file: "</span> &lt;&lt; param_.net();</span><br><span class="line">    <span class="comment">// 解析mylr.prototxt中内容，将其内容存入 NetParameter 对象 net_param 中。</span></span><br><span class="line">    <span class="comment">// 转换过程由ProtoBuffer工具完成的。</span></span><br><span class="line">    ReadNetParamsFromTextFileOrDie(param_.net(), &amp;net_param);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如此就将磁盘中的prototxt描述转换到内存。</p>
<h1 id="将内存中模型存储到磁盘"><a href="#将内存中模型存储到磁盘" class="headerlink" title="将内存中模型存储到磁盘"></a>将内存中模型存储到磁盘</h1><p>当需要保存当前模型和快照时：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I0610 04:53:38.909629 13919 solver.cpp:464] Snapshotting to binary proto file my_lr_iter_5000.caffemodel</span><br><span class="line">I0610 04:53:38.910568 13919 sgd_solver.cpp:284] Snapshotting solver state to binary proto file my_lr_iter_5000.solverstate</span><br></pre></td></tr></table></figure>

<p>solver.cpp:464 将模型存入磁盘：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="built_in">string</span> Solver&lt;Dtype&gt;::SnapshotToBinaryProto() &#123;</span><br><span class="line">    <span class="built_in">string</span> model_filename = SnapshotFilename(<span class="string">".caffemodel"</span>);</span><br><span class="line">    <span class="comment">// 文件名，只用提供后缀</span></span><br><span class="line">    LOG(INFO) &lt;&lt; <span class="string">"Snapshotting to binary proto file "</span> &lt;&lt; model_filename;</span><br><span class="line">    <span class="comment">// NetParameter 定义在 caffe.pb.h 中</span></span><br><span class="line">    NetParameter net_param;</span><br><span class="line">    <span class="comment">// 序列化到 net_param，一个 ProtoBuffer 对象</span></span><br><span class="line">    net_-&gt;ToProto(&amp;net_param, param_.snapshot_diff());</span><br><span class="line">    <span class="comment">// 将这个 ProtoBuffer 对象写入磁盘</span></span><br><span class="line">    WriteProtoToBinaryFile(net_param, model_filename);</span><br><span class="line">    <span class="keyword">return</span> model_filename;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述写入磁盘过程在<a href="https://ashburnlee.github.io/2020/06/04/caffe-Blob-1/" target="_blank" rel="noopener">caffe-Blob-(1)</a>解释过。</p>
<p>sgd_solver.cpp:284 将快照存入磁盘：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> SGDSolver&lt;Dtype&gt;::SnapshotSolverStateToBinaryProto(</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; model_filename) &#123;</span><br><span class="line">    <span class="comment">// SolverState 定义在 caffe.pb.h 中</span></span><br><span class="line">    <span class="comment">// 创建一个序列化对象</span></span><br><span class="line">    SolverState state;</span><br><span class="line">    <span class="comment">// 获得当前网络迭代参数</span></span><br><span class="line">    state.set_iter(<span class="keyword">this</span>-&gt;iter_);</span><br><span class="line">    state.set_learned_net(model_filename);</span><br><span class="line">    state.set_current_step(<span class="keyword">this</span>-&gt;current_step_);</span><br><span class="line">    state.clear_history();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; history_.size(); ++i) &#123;</span><br><span class="line">        <span class="comment">// Add history</span></span><br><span class="line">        BlobProto* history_blob = state.add_history();</span><br><span class="line">        <span class="comment">// 写入 ProtoBuffer 对象</span></span><br><span class="line">        history_[i]-&gt;ToProto(history_blob);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">string</span> snapshot_filename = Solver&lt;Dtype&gt;::SnapshotFilename(<span class="string">".solverstate"</span>);</span><br><span class="line">    LOG(INFO)</span><br><span class="line">        &lt;&lt; <span class="string">"Snapshotting solver state to binary proto file "</span> &lt;&lt; snapshot_filename;</span><br><span class="line">    <span class="comment">// 将 ProtoBuffer 对象 写入磁盘</span></span><br><span class="line">    WriteProtoToBinaryFile(state, snapshot_filename.c_str());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>类<code>NetParameter</code>和类<code>SolverState</code>都是在<code>caffe.pb.h</code>文件中定义的。这个文件是编译时由 ProtoBuffer 的编译器自动生成的。其中定义了很多类，包括<code>NetParameter</code>和<code>SolverState</code>。为什么会自动成成的文件中会有特定的类名？ 和可能是ProtoBuffer根据<code>caffe.proto</code>协议生成的。</p>
<p>对比发现，<code>caffe.pb.h</code> 中所有的类名在 <code>caffe.proto</code> 中都是一个message 对象： <code>message NAME {}</code>。</p>
<p>上述属于内存磁盘通信的内容。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/11/caffe-%E9%98%85%E8%AF%BBLog%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97/" data-id="ckb9u1y9200008nfz2sh5hnv7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-数据-模型-模型输出log" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/10/caffe-%E6%95%B0%E6%8D%AE-%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BAlog/" class="article-date">
  <time datetime="2020-06-10T09:31:00.000Z" itemprop="datePublished">2020-06-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/10/caffe-%E6%95%B0%E6%8D%AE-%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BAlog/">caffe-数据&amp;模型-模型输出log</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>当执行训练后:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~/caffe-master/build/tools/caffe train \</span><br><span class="line">--solver=/media/junhui/DATA/caffe_workspace/my_linearReggresion/lr_solver.prototxt</span><br></pre></td></tr></table></figure>

<p>训练过程会在终端打印，终端日志信息以<code>glog</code>的格式输出：这个格式包括当前时间，进程号，源码行号，代码行号，以及输出信息，这个信息用于观察网络当前执行到哪一步。来分析一下使用Linear Reggresion对mnist分类，这个例子虽小，但五脏俱全。在必要的地方用做了注释，如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br></pre></td><td class="code"><pre><span class="line">Reggresion/lr_solver.prototxt </span><br><span class="line">I0610 04:53:35.880447 13919 caffe.cpp:204] Using GPUs 0</span><br><span class="line">I0610 04:53:35.903647 13919 caffe.cpp:209] GPU 0: GeForce GTX 1050</span><br><span class="line"><span class="comment"># 解析训练超参数文件lr_solver.prototxt，并初始化solver</span></span><br><span class="line">I0610 04:53:36.096128 13919 solver.cpp:45] Initializing solver from parameters: </span><br><span class="line">test_iter: 100</span><br><span class="line">test_interval: 500 </span><br><span class="line">base_lr: 0.01</span><br><span class="line">max_iter: 10000</span><br><span class="line">lr_policy: <span class="string">"inv"</span></span><br><span class="line">gamma: 0.0001</span><br><span class="line">power: 0.75</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.0005</span><br><span class="line">snapshot: 5000</span><br><span class="line">snapshot_prefix: <span class="string">"my_lr"</span></span><br><span class="line">solver_mode: GPU</span><br><span class="line">device_id: 0</span><br><span class="line">net: <span class="string">"/media/junhui/DATA/caffe_workspace/my_linearReggresion/mylr.prototxt"</span></span><br><span class="line">train_state &#123;</span><br><span class="line">  level: 0</span><br><span class="line">  stage: <span class="string">""</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ～～～～～～～～～～～～～～～～～训练网络构建开始～～～～～～～～～～～～～～～～～</span></span><br><span class="line"><span class="comment"># 创建蓝图中的Net</span></span><br><span class="line">I0610 04:53:36.096751 13919 solver.cpp:102] Creating training net from net file: /media/junhui/DATA/caffe_workspace/my_linearReggresion/mylr.prototxt</span><br><span class="line"><span class="comment"># 这里指出，用于TEST的数据层和accuracy层，的phase值为“TEST”，将不在“TRAIN”阶段使用</span></span><br><span class="line">I0610 04:53:36.097002 13919 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule <span class="keyword">in</span> layer mnist</span><br><span class="line">I0610 04:53:36.097012 13919 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule <span class="keyword">in</span> layer accuracy</span><br><span class="line"><span class="comment"># 解析网络结构参数文件mylr.prototxt，并初始化用作TRAIN的Net</span></span><br><span class="line">I0610 04:53:36.097085 13919 net.cpp:53] Initializing net from parameters: </span><br><span class="line"><span class="comment"># 这里的3层layer堆叠起来才是用于训练的网络，从下面看，TEST网络就很清晰了</span></span><br><span class="line">name: <span class="string">"lrNet"</span></span><br><span class="line">state &#123;</span><br><span class="line">  phase: TRAIN</span><br><span class="line">  level: 0</span><br><span class="line">  stage: <span class="string">""</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 1. 数据层，生成两个top： LMDB-&gt;“data”&amp;“label”</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.0039063</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="built_in">source</span>: <span class="string">"/media/junhui/DATA/caffe_workspace/my_linearReggresion/mnist_train_lmdb"</span></span><br><span class="line">    batch_size: 64</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 2. 全连接层，"data"-&gt;"ip"</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"ip"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 10</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 3. softmax 层："ip"&amp;"label"-&gt;"loss"</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="comment"># 从LMDB文件中读取训练数据，</span></span><br><span class="line">I0610 04:53:36.097178 13919 layer_factory.hpp:77] Creating layer mnist</span><br><span class="line">I0610 04:53:36.097410 13919 db_lmdb.cpp:35] Opened lmdb /media/junhui/DATA/caffe_workspace/my_linearReggresion/mnist_train_lmdb</span><br><span class="line"><span class="comment"># 1. 创建数据层，产生两个数据对象，“data”&amp;“label”</span></span><br><span class="line">I0610 04:53:36.097434 13919 net.cpp:86] Creating Layer mnist</span><br><span class="line">I0610 04:53:36.097456 13919 net.cpp:382] mnist -&gt; data</span><br><span class="line">I0610 04:53:36.097476 13919 net.cpp:382] mnist -&gt; label</span><br><span class="line"><span class="comment"># 数据行输出的大小为[64,1,28,28]</span></span><br><span class="line">I0610 04:53:36.098232 13919 data_layer.cpp:45] output data size: 64,1,28,28</span><br><span class="line">I0610 04:53:36.099445 13919 net.cpp:124] Setting up mnist</span><br><span class="line">I0610 04:53:36.099474 13919 net.cpp:131] Top shape: 64 1 28 28 (50176)</span><br><span class="line">I0610 04:53:36.099484 13919 net.cpp:131] Top shape: 64 (64)</span><br><span class="line"><span class="comment"># 统计内存占用，这个值会在train过程中累积</span></span><br><span class="line">I0610 04:53:36.099489 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 200960</span><br><span class="line"><span class="comment"># 2. 创建ip，全连接层</span></span><br><span class="line">I0610 04:53:36.099498 13919 layer_factory.hpp:77] Creating layer ip</span><br><span class="line">I0610 04:53:36.099509 13919 net.cpp:86] Creating Layer ip</span><br><span class="line"><span class="comment"># 从“data”生层“ip”，就是这层的输出</span></span><br><span class="line">I0610 04:53:36.099529 13919 net.cpp:408] ip &lt;- data</span><br><span class="line">I0610 04:53:36.099541 13919 net.cpp:382] ip -&gt; ip</span><br><span class="line">I0610 04:53:36.100448 13919 net.cpp:124] Setting up ip</span><br><span class="line">I0610 04:53:36.100461 13919 net.cpp:131] Top shape: 64 10 (640)</span><br><span class="line">I0610 04:53:36.100478 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 203520</span><br><span class="line"><span class="comment"># 3. 创建最后一层得到loss</span></span><br><span class="line">I0610 04:53:36.100493 13919 layer_factory.hpp:77] Creating layer loss</span><br><span class="line">I0610 04:53:36.100505 13919 net.cpp:86] Creating Layer loss</span><br><span class="line"><span class="comment"># 该层输入为“ip”&amp;“label”输出为“loss”</span></span><br><span class="line">I0610 04:53:36.100510 13919 net.cpp:408] loss &lt;- ip</span><br><span class="line">I0610 04:53:36.100517 13919 net.cpp:408] loss &lt;- label</span><br><span class="line">I0610 04:53:36.100523 13919 net.cpp:382] loss -&gt; loss</span><br><span class="line">I0610 04:53:36.100535 13919 layer_factory.hpp:77] Creating layer loss</span><br><span class="line">I0610 04:53:36.643620 13919 net.cpp:124] Setting up loss</span><br><span class="line"><span class="comment"># 输出的loss大小为1，其权值为1</span></span><br><span class="line">I0610 04:53:36.643661 13919 net.cpp:131] Top shape: (1)</span><br><span class="line">I0610 04:53:36.643664 13919 net.cpp:134]     with loss weight 1</span><br><span class="line"><span class="comment"># 目前所占内存空间 200MB</span></span><br><span class="line">I0610 04:53:36.643699 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 203524</span><br><span class="line"><span class="comment"># 从后先前执行反向计算，哪里需要计算，就算哪里</span></span><br><span class="line">I0610 04:53:36.643705 13919 net.cpp:200] loss needs backward computation.</span><br><span class="line">I0610 04:53:36.643714 13919 net.cpp:200] ip needs backward computation.</span><br><span class="line">I0610 04:53:36.643719 13919 net.cpp:202] mnist does not need backward computation.</span><br><span class="line"><span class="comment"># TRAIN网络只输出loss</span></span><br><span class="line">I0610 04:53:36.643726 13919 net.cpp:244] This network produces output loss</span><br><span class="line">I0610 04:53:36.643734 13919 net.cpp:257] Network initialization <span class="keyword">done</span>.</span><br><span class="line"><span class="comment"># ～～～～～～～～～～～～～～～～～训练网络构建结束～～～～～～～～～～～～～～～～～</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ～～～～～～～～～～～～～～～～～测试网络构建开始～～～～～～～～～～～～～～～～～</span></span><br><span class="line">I0610 04:53:36.644055 13919 solver.cpp:190] Creating <span class="built_in">test</span> net (<span class="comment">#0) specified by net file: /media/junhui/DATA/caffe_workspace/my_linearReggresion/mylr.prototxt</span></span><br><span class="line"><span class="comment"># 这里指出，用于TRAIN的数据层的phase值为“TRAIN”，将不在“TEST”阶段使用</span></span><br><span class="line">I0610 04:53:36.644089 13919 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule <span class="keyword">in</span> layer mnist</span><br><span class="line">I0610 04:53:36.644132 13919 net.cpp:53] Initializing net from parameters: </span><br><span class="line"><span class="comment"># 同样的，给出完整的TEST网络的结构</span></span><br><span class="line">name: <span class="string">"lrNet"</span></span><br><span class="line">state &#123;</span><br><span class="line">  phase: TEST <span class="comment"># 用于TEST</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 数据层</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.0039063</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="built_in">source</span>: <span class="string">"/media/junhui/DATA/caffe_workspace/my_linearReggresion/mnist_test_lmdb"</span></span><br><span class="line">    batch_size: 100</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"ip"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 10</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 计算accuracy</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"accuracy"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Accuracy"</span></span><br><span class="line">  bottom: <span class="string">"ip"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"accuracy"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 计算loss</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br><span class="line">I0610 04:53:36.644286 13919 layer_factory.hpp:77] Creating layer mnist</span><br><span class="line">I0610 04:53:36.644841 13919 db_lmdb.cpp:35] Opened lmdb /media/junhui/DATA/caffe_workspace/my_linearReggresion/mnist_test_lmdb</span><br><span class="line">I0610 04:53:36.644881 13919 net.cpp:86] Creating Layer mnist</span><br><span class="line">I0610 04:53:36.644889 13919 net.cpp:382] mnist -&gt; data</span><br><span class="line">I0610 04:53:36.644898 13919 net.cpp:382] mnist -&gt; label</span><br><span class="line">I0610 04:53:36.645038 13919 data_layer.cpp:45] output data size: 100,1,28,28</span><br><span class="line">I0610 04:53:36.646373 13919 net.cpp:124] Setting up mnist</span><br><span class="line">I0610 04:53:36.646389 13919 net.cpp:131] Top shape: 100 1 28 28 (78400)</span><br><span class="line">I0610 04:53:36.646394 13919 net.cpp:131] Top shape: 100 (100)</span><br><span class="line">I0610 04:53:36.646397 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 314000</span><br><span class="line"><span class="comment"># 这一层由caffe解析后自动加上，label_mnist_1_split</span></span><br><span class="line">I0610 04:53:36.646402 13919 layer_factory.hpp:77] Creating layer label_mnist_1_split</span><br><span class="line">I0610 04:53:36.646409 13919 net.cpp:86] Creating Layer label_mnist_1_split</span><br><span class="line">I0610 04:53:36.646426 13919 net.cpp:408] label_mnist_1_split &lt;- label</span><br><span class="line"><span class="comment"># 复制两个label_mnist_1_split，一个用于计算最终accuracy，一个用于计算最终loss</span></span><br><span class="line">I0610 04:53:36.646445 13919 net.cpp:382] label_mnist_1_split -&gt; label_mnist_1_split_0</span><br><span class="line">I0610 04:53:36.646454 13919 net.cpp:382] label_mnist_1_split -&gt; label_mnist_1_split_1</span><br><span class="line">I0610 04:53:36.646559 13919 net.cpp:124] Setting up label_mnist_1_split</span><br><span class="line">I0610 04:53:36.646585 13919 net.cpp:131] Top shape: 100 (100)</span><br><span class="line">I0610 04:53:36.646590 13919 net.cpp:131] Top shape: 100 (100)</span><br><span class="line">I0610 04:53:36.646595 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 314800</span><br><span class="line">I0610 04:53:36.646598 13919 layer_factory.hpp:77] Creating layer ip</span><br><span class="line">I0610 04:53:36.646606 13919 net.cpp:86] Creating Layer ip</span><br><span class="line">I0610 04:53:36.646611 13919 net.cpp:408] ip &lt;- data</span><br><span class="line">I0610 04:53:36.646617 13919 net.cpp:382] ip -&gt; ip</span><br><span class="line">I0610 04:53:36.646811 13919 net.cpp:124] Setting up ip</span><br><span class="line">I0610 04:53:36.646819 13919 net.cpp:131] Top shape: 100 10 (1000)</span><br><span class="line">I0610 04:53:36.646824 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 318800</span><br><span class="line"><span class="comment"># 这一层由caffe解析后自动加上，ip_ip_0_split，</span></span><br><span class="line">I0610 04:53:36.646834 13919 layer_factory.hpp:77] Creating layer ip_ip_0_split</span><br><span class="line">I0610 04:53:36.646840 13919 net.cpp:86] Creating Layer ip_ip_0_split</span><br><span class="line">I0610 04:53:36.646845 13919 net.cpp:408] ip_ip_0_split &lt;- ip</span><br><span class="line"><span class="comment"># 复制两个ip_ip_0_split，一个用于计算最终accuracy，一个用于计算最终loss</span></span><br><span class="line">I0610 04:53:36.646852 13919 net.cpp:382] ip_ip_0_split -&gt; ip_ip_0_split_0</span><br><span class="line">I0610 04:53:36.646859 13919 net.cpp:382] ip_ip_0_split -&gt; ip_ip_0_split_1</span><br><span class="line">I0610 04:53:36.646891 13919 net.cpp:124] Setting up ip_ip_0_split</span><br><span class="line">I0610 04:53:36.646898 13919 net.cpp:131] Top shape: 100 10 (1000)</span><br><span class="line">I0610 04:53:36.646914 13919 net.cpp:131] Top shape: 100 10 (1000)</span><br><span class="line">I0610 04:53:36.646919 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 326800</span><br><span class="line"><span class="comment"># _0 计算accuracy</span></span><br><span class="line">I0610 04:53:36.646940 13919 layer_factory.hpp:77] Creating layer accuracy</span><br><span class="line">I0610 04:53:36.646947 13919 net.cpp:86] Creating Layer accuracy</span><br><span class="line">I0610 04:53:36.646952 13919 net.cpp:408] accuracy &lt;- ip_ip_0_split_0</span><br><span class="line">I0610 04:53:36.646957 13919 net.cpp:408] accuracy &lt;- label_mnist_1_split_0</span><br><span class="line">I0610 04:53:36.646963 13919 net.cpp:382] accuracy -&gt; accuracy</span><br><span class="line">I0610 04:53:36.646972 13919 net.cpp:124] Setting up accuracy</span><br><span class="line">I0610 04:53:36.646977 13919 net.cpp:131] Top shape: (1)</span><br><span class="line">I0610 04:53:36.646981 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 326804</span><br><span class="line"><span class="comment"># _0 计算loss</span></span><br><span class="line">I0610 04:53:36.646986 13919 layer_factory.hpp:77] Creating layer loss</span><br><span class="line">I0610 04:53:36.646992 13919 net.cpp:86] Creating Layer loss</span><br><span class="line">I0610 04:53:36.646997 13919 net.cpp:408] loss &lt;- ip_ip_0_split_1</span><br><span class="line">I0610 04:53:36.647002 13919 net.cpp:408] loss &lt;- label_mnist_1_split_1</span><br><span class="line">I0610 04:53:36.647024 13919 net.cpp:382] loss -&gt; loss</span><br><span class="line">I0610 04:53:36.647034 13919 layer_factory.hpp:77] Creating layer loss</span><br><span class="line">I0610 04:53:36.647753 13919 net.cpp:124] Setting up loss</span><br><span class="line">I0610 04:53:36.647764 13919 net.cpp:131] Top shape: (1)</span><br><span class="line">I0610 04:53:36.647770 13919 net.cpp:134]     with loss weight 1</span><br><span class="line">I0610 04:53:36.647779 13919 net.cpp:139] Memory required <span class="keyword">for</span> data: 326808</span><br><span class="line"><span class="comment"># 给出哪些需要后向传播，哪些不需要!!!!</span></span><br><span class="line">I0610 04:53:36.647785 13919 net.cpp:200] loss needs backward computation.</span><br><span class="line">I0610 04:53:36.647792 13919 net.cpp:202] accuracy does not need backward computation.</span><br><span class="line">I0610 04:53:36.647799 13919 net.cpp:200] ip_ip_0_split needs backward computation.</span><br><span class="line">I0610 04:53:36.647804 13919 net.cpp:200] ip needs backward computation.</span><br><span class="line">I0610 04:53:36.647809 13919 net.cpp:202] label_mnist_1_split does not need backward computation.</span><br><span class="line">I0610 04:53:36.647814 13919 net.cpp:202] mnist does not need backward computation.</span><br><span class="line"><span class="comment"># TEST网络输出accuracy和loss</span></span><br><span class="line">I0610 04:53:36.647819 13919 net.cpp:244] This network produces output accuracy</span><br><span class="line">I0610 04:53:36.647826 13919 net.cpp:244] This network produces output loss</span><br><span class="line">I0610 04:53:36.647835 13919 net.cpp:257] Network initialization <span class="keyword">done</span>.</span><br><span class="line">I0610 04:53:36.647861 13919 solver.cpp:57] Solver scaffolding <span class="keyword">done</span>.</span><br><span class="line"><span class="comment"># ～～～～～～～～～～～～～～～～～测试网络构建结束～～～～～～～～～～～～～～～～～</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ～～～～～～～～～～～～～～～～～训练测试开始执行～～～～～～～～～～～～～～～～～</span></span><br><span class="line">I0610 04:53:36.647935 13919 caffe.cpp:239] Starting Optimization</span><br><span class="line">I0610 04:53:36.647941 13919 solver.cpp:289] Solving lrNet</span><br><span class="line">I0610 04:53:36.647945 13919 solver.cpp:290] Learning Rate Policy: inv</span><br><span class="line"><span class="comment"># 第一次迭代，打印测试结果</span></span><br><span class="line">I0610 04:53:36.647997 13919 solver.cpp:347] Iteration 0, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:36.648779 13919 blocking_queue.cpp:49] Waiting <span class="keyword">for</span> data</span><br><span class="line">I0610 04:53:36.698768 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:36.699136 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.1184</span></span><br><span class="line">I0610 04:53:36.699177 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 2.31538 (* 1 = 2.31538 loss)</span></span><br><span class="line"><span class="comment"># 训练迭代500次后，打印测试结果</span></span><br><span class="line">I0610 04:53:36.872504 13919 solver.cpp:347] Iteration 500, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:36.922243 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:36.923786 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.8987</span></span><br><span class="line">I0610 04:53:36.923828 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.378121 (* 1 = 0.378121 loss)</span></span><br><span class="line">I0610 04:53:37.032925 13919 blocking_queue.cpp:49] Waiting <span class="keyword">for</span> data</span><br><span class="line">I0610 04:53:37.075943 13925 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line"><span class="comment"># 训练迭代又500次后，打印测试结果</span></span><br><span class="line">I0610 04:53:37.098423 13919 solver.cpp:347] Iteration 1000, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:37.149353 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:37.149734 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.9064</span></span><br><span class="line">I0610 04:53:37.149776 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.3422 (* 1 = 0.3422 loss)</span></span><br><span class="line">I0610 04:53:37.316992 13919 solver.cpp:347] Iteration 1500, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:37.366453 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:37.366799 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.912</span></span><br><span class="line">I0610 04:53:37.366842 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.321062 (* 1 = 0.321062 loss)</span></span><br><span class="line"><span class="comment"># 迭代过程的打印信息，略...</span></span><br><span class="line">I0610 04:53:38.582134 13919 blocking_queue.cpp:49] Waiting <span class="keyword">for</span> data</span><br><span class="line">I0610 04:53:38.680619 13919 solver.cpp:347] Iteration 4500, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:38.731168 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:38.731528 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.9195</span></span><br><span class="line">I0610 04:53:38.731568 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.292936 (* 1 = 0.292936 loss)</span></span><br><span class="line">I0610 04:53:38.796696 13925 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line"><span class="comment"># 保存这个时候的模型和快照</span></span><br><span class="line">I0610 04:53:38.909629 13919 solver.cpp:464] Snapshotting to binary proto file my_lr_iter_5000.caffemodel</span><br><span class="line">I0610 04:53:38.910568 13919 sgd_solver.cpp:284] Snapshotting solver state to binary proto file my_lr_iter_5000.solverstate</span><br><span class="line">I0610 04:53:38.910995 13919 solver.cpp:347] Iteration 5000, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:38.961858 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:38.962236 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.9202</span></span><br><span class="line">I0610 04:53:38.962280 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.289039 (* 1 = 0.289039 loss)</span></span><br><span class="line">I0610 04:53:38.978883 13919 blocking_queue.cpp:49] Waiting <span class="keyword">for</span> data</span><br><span class="line">I0610 04:53:39.144309 13919 solver.cpp:347] Iteration 5500, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:39.193158 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:39.193552 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.92</span></span><br><span class="line">I0610 04:53:39.193594 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.290407 (* 1 = 0.290407 loss)</span></span><br><span class="line">I0610 04:53:39.237748 13925 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line"><span class="comment"># 迭代过程的打印信息，略...</span></span><br><span class="line">I0610 04:53:40.892292 13919 blocking_queue.cpp:49] Waiting <span class="keyword">for</span> data</span><br><span class="line">I0610 04:53:40.902132 13925 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:40.947559 13919 solver.cpp:347] Iteration 9500, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:40.996812 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:40.997177 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.9214</span></span><br><span class="line">I0610 04:53:40.997220 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.282299 (* 1 = 0.282299 loss)</span></span><br><span class="line"><span class="comment"># 保存这时候的模型和快照</span></span><br><span class="line">I0610 04:53:41.165763 13919 solver.cpp:464] Snapshotting to binary proto file my_lr_iter_10000.caffemodel</span><br><span class="line">I0610 04:53:41.166873 13919 sgd_solver.cpp:284] Snapshotting solver state to binary proto file my_lr_iter_10000.solverstate</span><br><span class="line">I0610 04:53:41.167291 13919 solver.cpp:347] Iteration 10000, Testing net (<span class="comment">#0)</span></span><br><span class="line">I0610 04:53:41.218529 13926 data_layer.cpp:73] Restarting data prefetching from start.</span><br><span class="line">I0610 04:53:41.220217 13919 solver.cpp:414]     Test net output <span class="comment">#0: accuracy = 0.922</span></span><br><span class="line">I0610 04:53:41.220261 13919 solver.cpp:414]     Test net output <span class="comment">#1: loss = 0.281314 (* 1 = 0.281314 loss)</span></span><br><span class="line">I0610 04:53:41.220270 13919 solver.cpp:332] Optimization Done.</span><br><span class="line">I0610 04:53:41.220276 13919 caffe.cpp:250] Optimization Done.</span><br><span class="line"><span class="comment"># ～～～～～～～～～～～～～～～～～训练测试执行结束～～～～～～～～～～～～～～～～～</span></span><br><span class="line"><span class="comment"># 完</span></span><br></pre></td></tr></table></figure>

<p>总结：</p>
<ul>
<li>过程是先从网络结构超参数文件和训练超参数文件中解析出TRAIN网络和TEST网络，后构建两个网络，最后开始训练。</li>
<li>根据每条信息在源码中的执行位置，可以追踪源码执行细节。比如<code>Memory required for data</code>如何统计内存使用量的，看<code>net.cpp</code>139行：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// memory_used_ 保存当前net占用内存空间</span></span><br><span class="line"><span class="comment">// 遍历当前net的每一个Layer</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; param.layer_size(); ++layer_id) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 遍历当前Layer的top</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size(); ++top_id) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 内存用量累加</span></span><br><span class="line">        memory_used_ += top_vecs_[layer_id][top_id]-&gt;count();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 打印这一层内存用量</span></span><br><span class="line">    LOG_IF(INFO, Caffe::root_solver()) <span class="comment">// 打印log</span></span><br><span class="line">        &lt;&lt; <span class="string">"Memory required for data: "</span> &lt;&lt; <span class="function">memory_used_ * <span class="title">sizeof</span><span class="params">(Dtype)</span></span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><font color="gree" size="5">问题</font>：</p>
<ol>
<li>TEST网络为什么也需要后传计算？</li>
<li>上述结果是使用GPU计算的，但是在log中并没有显示使用了哪个<code>.cu</code>文件。肯定执行了CUDA文件，应该是没有记录。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/10/caffe-%E6%95%B0%E6%8D%AE-%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BAlog/" data-id="ckb95zgxq000gc6fz40y265y6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-数据-模型-模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/09/caffe-%E6%95%B0%E6%8D%AE-%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-06-09T09:30:40.000Z" itemprop="datePublished">2020-06-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/09/caffe-%E6%95%B0%E6%8D%AE-%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B/">caffe-数据&amp;模型-模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="模型三部分参数"><a href="#模型三部分参数" class="headerlink" title="模型三部分参数"></a>模型三部分参数</h1><p>模型是caffe学习系统包含数据与模型两个核心组件之一。一个caffe模型含有三部分参数：</p>
<ul>
<li>要学习的参数：</li>
</ul>
<p>即权值，通过 初始化和反向传播得到更新。随着训练的结束，要学习的参数就此确定，也意味着确定了一个具体的模型。这个模型或者说是最终的学习参数会保存在<code>.caffemodel</code>文件中。我们可以直接使用这个模型，或者是在这个模型的基础上继续学习。</p>
<ul>
<li>网络结构超参数：</li>
</ul>
<p>就是Net的蓝图，即如何构建一个具体的Net，是一种构建策略。比如当前卷积层的kernel数量，kernel大小，步长等参数，显然，结构参数是在训练网络前就确定了的。注意：同一个Net的训练结构和测试结构可能不同。结构参数由Net的蓝图文件<code>.prototxt</code>提供，读取这个文件，得到Net的结构细节，从而指导caffe构建制定Net。</p>
<ul>
<li>训练超参数：</li>
</ul>
<p>控用于制训练过程的参数，如learning rate，迭代次数，CPU或GPU训练，等。其描述存在于一个<code>.prototxt</code>文件，一般使用<code>solver.prototxt</code>这个文件名。</p>
<h1 id="例"><a href="#例" class="headerlink" title="例"></a>例</h1><p>先准备数据。在将原始数据（raw data）转换为LMDB格式之后，就可以由数据层（DataLayer）不断从LMDB（磁盘上）读取数据进入网络。</p>
<p><font color="red">为了对caffe的pipline有个清晰的理解</font>，在工作目录创建文件夹<code>my_linearReggresion</code> 存放关于这个网络的所有内容，将之前创建好的LMDB数据文件放入 <code>my_linearReggresion</code>。之后创建网络结构文件 <code>mylr.prototxt</code> 和训练超参数文件 <code>lr_solver.prototxt</code>。</p>
<p>此时的<code>my_linearReggresion</code>目录tree如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── lr_solver.prototxt</span><br><span class="line">├── mnist_test_lmdb</span><br><span class="line">│   ├── data.mdb</span><br><span class="line">│   └── lock.mdb</span><br><span class="line">├── mnist_train_lmdb</span><br><span class="line">│   ├── data.mdb</span><br><span class="line">│   └── lock.mdb</span><br><span class="line">└── mylr.prototxt</span><br></pre></td></tr></table></figure>

<p>上述是以一个简单的logistic reggresion为例。其网络结构如下图（<font color="red">包含训练和测试两个结构，两者共同的层只需定义一遍，不同阶段的要分别定义</font>）：<br>[插图结构图]<br>参照结构图，定义网络结构超参数文件和训练超参数文件。</p>
<h1 id="编辑mylr-prototxt"><a href="#编辑mylr-prototxt" class="headerlink" title="编辑mylr.prototxt"></a>编辑<code>mylr.prototxt</code></h1><p>先编辑网络结构超参数文件，就是编辑<code>mylr.prototxt</code>文件。如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"lrNet"</span></span><br><span class="line"><span class="comment"># 数据层，用于train</span></span><br><span class="line">layer&#123;</span><br><span class="line">    name: <span class="string">"mnist"</span></span><br><span class="line">    <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    include &#123;</span><br><span class="line">        phase: TRAIN</span><br><span class="line">    &#125;</span><br><span class="line">    transform_param&#123;</span><br><span class="line">        scale: 0.0039063</span><br><span class="line">    &#125;</span><br><span class="line">    data_param&#123;</span><br><span class="line">        <span class="built_in">source</span>: <span class="string">"MNIST_LMDB_PATH/mnist_train_lmdb"</span></span><br><span class="line">        batch_size: 64</span><br><span class="line">        backend: lmdb</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 数据层，用于test</span></span><br><span class="line">layer&#123;</span><br><span class="line">    name: <span class="string">"mnist"</span></span><br><span class="line">    <span class="built_in">type</span>: <span class="string">"Data"</span>  <span class="comment"># 从哪里定义的“Data”</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    include &#123;</span><br><span class="line">        phase: TEST</span><br><span class="line">    &#125;</span><br><span class="line">    transform_param&#123;  <span class="comment"># 从哪里定义</span></span><br><span class="line">        scale: 0.0039063</span><br><span class="line">    &#125;</span><br><span class="line">    data_param&#123;   <span class="comment"># 从哪里定义</span></span><br><span class="line">        <span class="built_in">source</span>: <span class="string">"MNIST_LMDB_PATH/mnist_test_lmdb"</span></span><br><span class="line">        batch_size: 100</span><br><span class="line">        backend: lmdb</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 内积层</span></span><br><span class="line">layer&#123;</span><br><span class="line">    name: <span class="string">"ip"</span></span><br><span class="line">    <span class="built_in">type</span>: <span class="string">"InnerProduct"</span> <span class="comment"># 从哪里定义</span></span><br><span class="line">    bottom: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"ip"</span></span><br><span class="line">    param&#123;   <span class="comment"># 从哪里定义</span></span><br><span class="line">        lr_mult: 1</span><br><span class="line">    &#125;</span><br><span class="line">    param&#123;</span><br><span class="line">        lr_mult: 2</span><br><span class="line">    &#125;</span><br><span class="line">    inner_product_param&#123;  <span class="comment"># 从哪里定义</span></span><br><span class="line">        num_output: 10</span><br><span class="line">        weight_filler&#123;</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">        &#125;</span><br><span class="line">        bias_filler&#123;</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 在Test阶段使用</span></span><br><span class="line">layer&#123;</span><br><span class="line">    name: <span class="string">"accuracy"</span></span><br><span class="line">    <span class="built_in">type</span>: <span class="string">"Accuracy"</span></span><br><span class="line">    bottom: <span class="string">"ip"</span></span><br><span class="line">    bottom: <span class="string">"label"</span></span><br><span class="line">    top: <span class="string">"accuracy"</span></span><br><span class="line">    include &#123;</span><br><span class="line">        phase: TEST</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">layer&#123;</span><br><span class="line">    name: <span class="string">"loss"</span></span><br><span class="line">    <span class="built_in">type</span>: <span class="string">"SoftMaxWithLoss"</span></span><br><span class="line">    bottom: <span class="string">"ip"</span></span><br><span class="line">    bottom: <span class="string">"label"</span></span><br><span class="line">    top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中<code>source</code>提供LMDB数据的路径，提供绝对路径不用担心出错。</p>
<h1 id="编辑lr-solver-prototxt"><a href="#编辑lr-solver-prototxt" class="headerlink" title="编辑lr_solver.prototxt"></a>编辑<code>lr_solver.prototxt</code></h1><p>按照上述蓝图构建用于训练的网络和用于测试的网络。之后编辑训练超参数文件<code>lr_solver.prototxt</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结构文件的路径，告诉caffe，构建蓝图在哪</span></span><br><span class="line">net: <span class="string">"PATH/lr.prototxt"</span></span><br><span class="line">test_iter: 100</span><br><span class="line"><span class="comment"># 每500次做一次测试</span></span><br><span class="line">test_interval: 500</span><br><span class="line">base_lr: 0.01</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_delay: 0.0005</span><br><span class="line">lr_policy: <span class="string">"inv"</span></span><br><span class="line">gamma: 0.0001</span><br><span class="line">power: 0.75</span><br><span class="line">max_iter: 10000</span><br><span class="line"><span class="comment"># 每迭代5000次拍个快照</span></span><br><span class="line">snapshot: 5000</span><br><span class="line">snapshot_prefix: <span class="string">"PATH/mnist/lrNet"</span></span><br><span class="line">solver_mode: GPU</span><br></pre></td></tr></table></figure>
<p>note: 想上添加<code>type</code>关键字，用来指定所使用的优化方法，如<code>type: &quot;Nesterov&quot;</code>，<code>type: &quot;AdaGrad&quot;</code>，<code>type: &quot;AdaDelta&quot;</code>等。默认使用<code>SGD</code>。<br>有了数据有了模型就可以训练了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CAFFE train \</span><br><span class="line">--solver=PATH/lr_solver.prototxt</span><br></pre></td></tr></table></figure>

<p>其中<code>CAFFE</code>，表示caffe_master提前编译好的caffe工具，就像在caffe_master中的命令<code>./build/tools/caffe</code>。</p>
<p>总结，要想正确运行，必须保证所配置的路径是有效的：</p>
<ul>
<li>用于执行指令的<code>build/tools/caffe</code>命令的路径，</li>
<li>训练超参数文件的路径<code>lr_solver.prototxt</code>，</li>
<li>LMDB数据路径，</li>
<li>模型结构超参数文件路径<code>mylr.prototxt</code>，</li>
</ul>
<h1 id="对于Layer的类型"><a href="#对于Layer的类型" class="headerlink" title="对于Layer的类型"></a>对于Layer的类型</h1><p>caffe现有的<code>Layer type</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">known types: AbsVal, Accuracy, ArgMax, BNLL, BatchNorm, </span><br><span class="line">BatchReindex, Bias, Clip, Concat, ContrastiveLoss, Convolution, </span><br><span class="line">Crop, Data, Deconvolution, Dropout, DummyData, ELU, Eltwise, </span><br><span class="line">Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, </span><br><span class="line">HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, </span><br><span class="line">InnerProduct, Input, LRN, LSTM, LSTMUnit, Log, MVN, MemoryData, </span><br><span class="line">MultinomialLogisticLoss, PReLU, Parameter, Pooling, Power, </span><br><span class="line">Python, RNN, ReLU, Reduction, Reshape, SPP, Scale, Sigmoid, </span><br><span class="line">SigmoidCrossEntropyLoss, Silence, Slice, Softmax, </span><br><span class="line">SoftmaxWithLoss, Split, Swish, TanH, Threshold, Tile, WindowData</span><br></pre></td></tr></table></figure>

<p>当训练并测试完毕，会有模型文件和快照文件保存于当前目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── lr_solver.prototxt</span><br><span class="line">├── mnist_test_lmdb</span><br><span class="line">│   ├── data.mdb</span><br><span class="line">│   └── lock.mdb</span><br><span class="line">├── mnist_train_lmdb</span><br><span class="line">│   ├── data.mdb</span><br><span class="line">│   └── lock.mdb</span><br><span class="line">├── my_lr_iter_10000.caffemodel</span><br><span class="line">├── my_lr_iter_10000.solverstate</span><br><span class="line">├── my_lr_iter_5000.caffemodel</span><br><span class="line">├── my_lr_iter_5000.solverstate</span><br><span class="line">└── mylr.prototxt</span><br></pre></td></tr></table></figure>

<p><font color="gree" size="5">问题</font>：<br>这些关键字是在哪了与Layer的实现对应起来的呢？<font color="red">当将自己实现的Layer或其他组件键入到caffe源码中时，就需要在某处添加对应关键字。</font></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/09/caffe-%E6%95%B0%E6%8D%AE-%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B/" data-id="ckb95zgvz0000c6fz37cz1r86" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-Net-hpp" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/08/caffe-Net-hpp/" class="article-date">
  <time datetime="2020-06-08T13:15:10.000Z" itemprop="datePublished">2020-06-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/08/caffe-Net-hpp/">caffe-Net.hpp</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>从prototxt文件解析Net的结构，以 Lenet 为例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;caffe/blob.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;caffe/net.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;caffe/util/io.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> caffe;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// prototxt文件路径</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span> <span class="title">proto</span><span class="params">(<span class="string">"../Lenet.prototxt"</span>)</span></span>;</span><br><span class="line">    <span class="comment">// 创造一个Net对象。phase设为TEST，测试网络</span></span><br><span class="line">    Net&lt;<span class="keyword">float</span>&gt; nn(proto, caffe::TEST);</span><br><span class="line">    <span class="comment">// 获取这个Net的layer names:</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; ln = nn.layer_names();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:ln)&#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;i&lt;&lt;<span class="string">" "</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// 取这个Net的`blob_names`</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; bn = nn.blob_names();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:bn)&#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;i&lt;&lt;<span class="string">" "</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>返回这个 TEST Net 中的blob name：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># layer names</span></span><br><span class="line">mnist conv1 pool1 conv2 pool2 ip1 relu1 ip2 loss </span><br><span class="line"><span class="comment"># blob names</span></span><br><span class="line">data label conv1 pool1 conv2 pool2 ip1 ip2 loss</span><br></pre></td></tr></table></figure>

<p>BLob对象存放每个Layer的输出和输入结果。每个Layer对输入Blob进行某种计算。layer name 和 blob names 不存在任何关系。所有的Layer和Blob用名字区分，</p>
<p>先看一个Net对象都有什么成员属性：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span>：</span><br><span class="line">  <span class="comment">// 这个 Net 的名称</span></span><br><span class="line">  <span class="built_in">string</span> name_;</span><br><span class="line">  <span class="comment">// 这个 Net 用于 TRAIN or TEST</span></span><br><span class="line">  Phase phase_;</span><br><span class="line">  <span class="comment">// 记录这个 Net 中的每一层</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt;&gt;&gt; layers_;</span><br><span class="line">  <span class="comment">// 记录每一层的名称</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; layer_names_;</span><br><span class="line">  <span class="comment">// 记录每一层与其位置Index的对应关系</span></span><br><span class="line">  <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; layer_names_index_;</span><br><span class="line">  <span class="comment">// 记录每层是否需要反向传播</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; layer_need_backward_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 存储层之间的中间结果</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;&gt; blobs_;</span><br><span class="line">  <span class="comment">// 记录每个blob的名称</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; blob_names_;</span><br><span class="line">  <span class="comment">// 记录每个Blob与其位置Index的对应关系</span></span><br><span class="line">  <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; blob_names_index_;</span><br><span class="line">  <span class="comment">// 记录每个blob是否需要反向计算</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; blob_need_backward_;</span><br><span class="line">  <span class="comment">/// bottom_vecs stores the vectors containing the input for each layer.</span></span><br><span class="line">  <span class="comment">/// They don't actually host the blobs (blobs_ does), so we simply store</span></span><br><span class="line">  <span class="comment">/// pointers.</span></span><br><span class="line">  <span class="comment">// 记录每个Layer的输入Blob，但并不是Blob的内容，而是Blob的地址。</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&gt; bottom_vecs_;</span><br><span class="line">  <span class="comment">// 与上者相关，</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; bottom_id_vecs_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt; bottom_need_backward_;</span><br><span class="line">  <span class="comment">/// top_vecs stores the vectors containing the output for each layer</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&gt; top_vecs_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; top_id_vecs_;</span><br><span class="line">  <span class="comment">/// Vector of weight in the loss (or objective) function of each net blob,</span></span><br><span class="line">  <span class="comment">/// indexed by blob_id.</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;Dtype&gt; blob_loss_weights_;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; param_id_vecs_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; param_owners_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; param_display_names_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt; param_layer_indices_;</span><br><span class="line">  <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; param_names_index_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/// blob indices for the input and the output of the net</span></span><br><span class="line">  <span class="comment">// 这个Net 的输入输出Blob 在 blobs_ 中的索引</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; net_input_blob_indices_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; net_output_blob_indices_;</span><br><span class="line">  <span class="comment">// 输入输出Blob</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; net_input_blobs_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; net_output_blobs_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个 Net 中的参数，权值</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;&gt; params_;</span><br><span class="line">  <span class="comment">// 这个 Net 中可训练的权值</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The mapping from params_ -&gt; learnable_params_: we have</span></span><br><span class="line"><span class="comment">   * learnable_param_ids_.size() == params_.size(),</span></span><br><span class="line"><span class="comment">   * and learnable_params_[learnable_param_ids_[i]] == params_[i].get()</span></span><br><span class="line"><span class="comment">   * if and only if params_[i] is an "owner"; otherwise, params_[i] is a sharer</span></span><br><span class="line"><span class="comment">   * and learnable_params_[learnable_param_ids_[i]] gives its owner.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; learnable_param_ids_;</span><br><span class="line">  <span class="comment">/// the learning rate multipliers for learnable_params_</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; params_lr_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; has_params_lr_;</span><br><span class="line">  <span class="comment">/// the weight decay multipliers for learnable_params_</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; params_weight_decay_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; has_params_decay_;</span><br><span class="line">  <span class="comment">/// The bytes of memory used by this net</span></span><br><span class="line">  <span class="keyword">size_t</span> memory_used_;</span><br><span class="line">  <span class="comment">/// Whether to compute and display debug info for the net.</span></span><br><span class="line">  <span class="keyword">bool</span> debug_info_;</span><br><span class="line">  <span class="comment">// Callbacks</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;Callback*&gt; before_forward_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;Callback*&gt; after_forward_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;Callback*&gt; before_backward_;</span><br><span class="line">  <span class="built_in">vector</span>&lt;Callback*&gt; after_backward_;</span><br></pre></td></tr></table></figure>

<p>说明一下：</p>
<ul>
<li>其实并不能单单从属性的名字中知道它是干啥的。其与原理相关，需要在试验中分析每个属性的作用。</li>
<li>上述属性中大部分是vector容器，所以猜想，每个容器存放这个Net的所有Layer的对应属性。</li>
<li>上述大部分的类属性都有一个getter()函数，返回这个Net中每个对象。</li>
</ul>
<p>Net中由两类Blob，以param开头的是权值Blob；以blob开头的是数据Blob。前者决定了模型是什么样的，后者是每一个Layer的输入和输出（样本数据），是这个Net中被处理的数据。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/08/caffe-Net-hpp/" data-id="ckb81e7my0000k4fz19mucpra" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-cpp-void型指针" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/" class="article-date">
  <time datetime="2020-06-08T12:19:39.000Z" itemprop="datePublished">2020-06-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/">cpp-void型指针</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面这个函数什么意思：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">foo</span><span class="params">(<span class="keyword">void</span>* a)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>他表示foo接受任何类型的指针，并输出任何类型的指针。使用方式总结如下：</p>
<h2 id="1-void指针可以指向任何类型指针-但反过来就不对了"><a href="#1-void指针可以指向任何类型指针-但反过来就不对了" class="headerlink" title="1. void指针可以指向任何类型指针,但反过来就不对了"></a>1. void指针可以指向任何类型指针,但反过来就不对了</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> f = <span class="number">5.5</span>;</span><br><span class="line"><span class="keyword">float</span>* pf = &amp;f;</span><br><span class="line"><span class="keyword">void</span>* pv = pf;  <span class="comment">// void指针可以指向float型指针</span></span><br><span class="line"><span class="keyword">float</span>* pf2 = pv; <span class="comment">// 错，float指针不能指向void指针</span></span><br></pre></td></tr></table></figure>

<h2 id="2-void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型"><a href="#2-void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型" class="headerlink" title="2.void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型"></a>2.void指针只有强制转换类型后才可以取值，而且要转换成所保存地址中，内容的类型</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> x = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">void</span>* yv = foo(&amp;x);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;yv&lt;&lt;<span class="string">" "</span>&lt;&lt;*(<span class="keyword">float</span>*)yv&lt;&lt;<span class="built_in">endl</span>; <span class="comment">// 返回0x7ffc78a54e1c 4</span></span><br><span class="line"><span class="comment">//cout&lt;&lt;*yv&lt;&lt;endl;  // 编译错误</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*(<span class="keyword">double</span>*)yv&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 返回1.44068e+273。返回值错误</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;(<span class="keyword">double</span>)(*(<span class="keyword">float</span>*)yv)&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 返回4。正确</span></span><br></pre></td></tr></table></figure>

<h2 id="3-void指针可以使用nullptr初始化"><a href="#3-void指针可以使用nullptr初始化" class="headerlink" title="3. void指针可以使用nullptr初始化"></a>3. void指针可以使用nullptr初始化</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span>* vPtr = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*vPtr&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 编译错误，vPtr不指向任何对象，所以取不到任何内容</span></span><br></pre></td></tr></table></figure>
<h2 id="4-接受任何类型的指针，并输出任何类型的指针"><a href="#4-接受任何类型的指针，并输出任何类型的指针" class="headerlink" title="4. 接受任何类型的指针，并输出任何类型的指针"></a>4. 接受任何类型的指针，并输出任何类型的指针</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span>* vPtr = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">void</span>* vFunPtr = foo(vPtr);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;vFunPtr&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 空指针，所以返回0</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/08/cpp-void%E5%9E%8B%E6%8C%87%E9%92%88/" data-id="ckb6h8i2f00006sfze2n59ljw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-Layer中有什么" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/" class="article-date">
  <time datetime="2020-06-07T12:27:32.000Z" itemprop="datePublished">2020-06-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/">caffe-Layer中有什么</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一个Layer对象以一个Blob为输入（bottom），另一个Blob为输出（top）。主要机选包括前向计算和后向计算：前向计算对输入blob进行处理，得到输出blob。后向计算对输出blob的diff部分做处理得到输入blob的diff。</p>
<p>注意了，caffe中的<code>top</code> 和<code>bottom</code>都是<code>vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt;&gt;&gt;</code>，<font color="red">其元素为多个指向blob的指针。而并非值一个blob对象！</font></p>
<p>既然<code>blobs_</code>是训练参数，那么向该层输入的数据在哪？？？</p>
<p>下面内容位于<code>Layer.hpp</code>文件。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span> &#123;</span></span><br><span class="line"><span class="comment">// 先看类成员属性，以下划线结尾的变量，对类内可见，对该类之外不可见。</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="comment">// 保存该层参数的 protobuf</span></span><br><span class="line">    <span class="comment">// LayerParameter类声明在这里：</span></span><br><span class="line">    <span class="comment">// .build_release/src/caffe/proto/caffe.pb.h 这是编译后自动生成的文件</span></span><br><span class="line">    LayerParameter layer_param_;</span><br><span class="line">    <span class="comment">// 这层所处是哪个阶段，train OR test</span></span><br><span class="line">    Phase phase_;</span><br><span class="line">    <span class="comment">// 多个Blob指针，指向这层内部的学习参数</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt;&gt;&gt; blobs_;</span><br><span class="line">    <span class="comment">// 是否计算对应参数的误差梯度</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; param_propagate_down_;</span><br><span class="line">    <span class="comment">// 目标函数中是否每个Top blob都有非零权值</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;Dtype&gt; loss_;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 上述三个vector的长度一样！</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造，从LayerParameter对象中加载参数</span></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Layer</span><span class="params">(<span class="keyword">const</span> LayerParameter&amp; param)</span></span></span><br><span class="line">    : layer_param_(param) &#123;</span><br><span class="line">        <span class="comment">// 设置阶段</span></span><br><span class="line">        phase_ = param.phase();</span><br><span class="line">        <span class="comment">// 如果有数据，则设置blob，具体是从磁盘读取到这个Layer的blob</span></span><br><span class="line">        <span class="keyword">if</span> (layer_param_.blobs_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// WHY blob的个数resize到blob的大小 ？？？</span></span><br><span class="line">            blobs_.resize(layer_param_.blobs_size()); </span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; layer_param_.blobs_size(); ++i) &#123;</span><br><span class="line">                <span class="comment">// 这个blob[i]指针接管一个新的blob指针</span></span><br><span class="line">                blobs_[i].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;());</span><br><span class="line">                <span class="comment">// 从磁盘读取数据到当前blob[i]</span></span><br><span class="line">                blobs_[i]-&gt;FromProto(layer_param_.blobs(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">virtual</span> ~Layer() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 不能覆盖这个方法，提供4个功能</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 检查bottom 和topblob是否满足这层的要求</span></span><br><span class="line">        CheckBlobCounts(bottom, top);</span><br><span class="line">        <span class="comment">//2. 调用自己实现的层配置函数</span></span><br><span class="line">        LayerSetUp(bottom, top);</span><br><span class="line">        <span class="comment">//3. 对输出blob 变形</span></span><br><span class="line">        Reshape(bottom, top);</span><br><span class="line">        <span class="comment">//4. </span></span><br><span class="line">        SetLossWeights(top);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 层的相关配置，由自己实现（子类实现）</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">LayerSetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自己实现（子类实现），调整top blob和中间buffer的形状</span></span><br><span class="line">    <span class="comment">// 以适应bottom blob的形状。纯虚函数</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 根据bottom blob 计算top blob和loss。返回这一层的总loss。</span></span><br><span class="line">    <span class="comment">// 该函数调用Forward_cpu()和Forward_gpu()执行真正的计算；</span></span><br><span class="line">    <span class="comment">// 如果该层有非零权值，则计算并返回loss。</span></span><br><span class="line">    <span class="comment">// 在子类实现Forward_cpu()和Forward_gpu()。毕竟不同层的计算方式不同。</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">Forward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 反向传播计算，给定top 的梯度，计算bottom的梯度。</span></span><br><span class="line">    <span class="comment">// 参数中top 中含有上层来的梯度误差diff</span></span><br><span class="line">    <span class="comment">// propagate_down 其长度与bottom长度相同，</span></span><br><span class="line">    <span class="comment">// 其中每一个值表示是否将对应的误差传到对应的bottom。</span></span><br><span class="line">    <span class="comment">// bottom 输入blobs，经过Backward()计算后， 其diff 保存误差梯度，</span></span><br><span class="line">    <span class="comment">// 实际上的后向传播的执行由Backward_cpu() 和 Backward_gpu()实现。</span></span><br><span class="line">    <span class="comment">// 子类需要实现Backward_cpu() 和 Backward_gpu()</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">Backward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回这层中可训练参数 blob向量</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blobs() &#123;</span><br><span class="line">        <span class="keyword">return</span> blobs_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该层的 曾参数（由protobuff提供）</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> LayerParameter&amp; <span class="title">layer_param</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; </span><br><span class="line">        <span class="keyword">return</span> layer_param_; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将该层层参数写入 protobuff</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">ToProto</span><span class="params">(LayerParameter* param, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回top指定index的blob的loss值</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> Dtype <span class="title">loss</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> top_index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (loss_.size() &gt; top_index) ? loss_[top_index] : Dtype(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为top指定index的blob的loss 设值</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">set_loss</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> top_index, <span class="keyword">const</span> Dtype value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (loss_.size() &lt;= top_index) &#123;</span><br><span class="line">            loss_.resize(top_index + <span class="number">1</span>, Dtype(<span class="number">0</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        loss_[top_index] = value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该层的类型</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">""</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该层需要输入或输出的blobs数，由子类实现。</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">ExactNumBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MaxBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">ExactNumTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MaxTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 该层的top blobs个数和bottom blobs个数是否相同。子类实现</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">EqualNumBottomTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 是否需要自动创造匿名top blobs，</span></span><br><span class="line">    <span class="comment">// 如果返回true，Net::Init()会创建足够多的匿名top blobs来满足</span></span><br><span class="line">    <span class="comment">// ExactNumTopBlobs() 或MinTopBlobs().</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">AutoTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">AllowForceBackward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> bottom_index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">param_propagate_down</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (param_propagate_down_.size() &gt; param_id) ?</span><br><span class="line">        param_propagate_down_[param_id] : <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">set_param_propagate_down</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id, <span class="keyword">const</span> <span class="keyword">bool</span> value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (param_propagate_down_.size() &lt;= param_id) &#123;</span><br><span class="line">            param_propagate_down_.resize(param_id + <span class="number">1</span>, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        param_propagate_down_[param_id] = value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cpu和gpu 前行计算，其具体实现在具体的层中，将一直看到</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// LOG(WARNING) &lt;&lt; "Using CPU code as backup.";</span></span><br><span class="line">        <span class="keyword">return</span> Forward_cpu(bottom, top);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// LOG(WARNING) &lt;&lt; "Using CPU code as backup.";</span></span><br><span class="line">        Backward_cpu(top, propagate_down, bottom);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called by the parent Layer's SetUp to check that the number of bottom</span></span><br><span class="line"><span class="comment">     * and top Blobs provided as input match the expected numbers specified by</span></span><br><span class="line"><span class="comment">     * the &#123;ExactNum,Min,Max&#125;&#123;Bottom,Top&#125;Blobs() functions.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 最后两个函数由父类Layer 的SetUp()函数调用</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">CheckBlobCounts</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">                                <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (ExactNumBottomBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_EQ(ExactNumBottomBlobs(), bottom.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer takes "</span> &lt;&lt; ExactNumBottomBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" bottom blob(s) as input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MinBottomBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_LE(MinBottomBlobs(), bottom.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer takes at least "</span> &lt;&lt; MinBottomBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" bottom blob(s) as input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MaxBottomBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_GE(MaxBottomBlobs(), bottom.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer takes at most "</span> &lt;&lt; MaxBottomBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" bottom blob(s) as input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ExactNumTopBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_EQ(ExactNumTopBlobs(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces "</span> &lt;&lt; ExactNumTopBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" top blob(s) as output."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MinTopBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_LE(MinTopBlobs(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces at least "</span> &lt;&lt; MinTopBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" top blob(s) as output."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (MaxTopBlobs() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            CHECK_GE(MaxTopBlobs(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces at most "</span> &lt;&lt; MaxTopBlobs()</span><br><span class="line">                &lt;&lt; <span class="string">" top blob(s) as output."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (EqualNumBottomTopBlobs()) &#123;</span><br><span class="line">            CHECK_EQ(bottom.size(), top.size())</span><br><span class="line">                &lt;&lt; type() &lt;&lt; <span class="string">" Layer produces one top blob as output for each "</span></span><br><span class="line">                &lt;&lt; <span class="string">"bottom blob input."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called by SetUp to initialize the weights associated with any top blobs in</span></span><br><span class="line"><span class="comment">     * the loss function. Store non-zero loss weights in the diff blob.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">SetLossWeights</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_loss_weights = layer_param_.loss_weight_size();</span><br><span class="line">        <span class="keyword">if</span> (num_loss_weights) &#123;</span><br><span class="line">            CHECK_EQ(top.size(), num_loss_weights) &lt;&lt; <span class="string">"loss_weight must be "</span></span><br><span class="line">                <span class="string">"unspecified or specified once per top blob."</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</span><br><span class="line">                <span class="keyword">const</span> Dtype loss_weight = layer_param_.loss_weight(top_id);</span><br><span class="line">                <span class="keyword">if</span> (loss_weight == Dtype(<span class="number">0</span>)) &#123; <span class="keyword">continue</span>; &#125;</span><br><span class="line">                <span class="keyword">this</span>-&gt;set_loss(top_id, loss_weight);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> count = top[top_id]-&gt;count();</span><br><span class="line">                Dtype* loss_multiplier = top[top_id]-&gt;mutable_cpu_diff();</span><br><span class="line">                caffe_set(count, loss_weight, loss_multiplier);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    DISABLE_COPY_AND_ASSIGN(Layer);</span><br><span class="line">&#125;;  <span class="comment">// class Layer</span></span><br></pre></td></tr></table></figure>

<p><code>Layer.cpp</code>文件内容：可见Layer的真正实现都在其子类中：<code>src/caffe/layers/*.cpp</code>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/layer.hpp"</span></span></span><br><span class="line"><span class="keyword">namespace</span> caffe &#123;</span><br><span class="line">INSTANTIATE_CLASS(Layer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Instantiate a class with float and double specifications.</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INSTANTIATE_CLASS(classname) \</span></span><br><span class="line">    <span class="keyword">char</span> gInstantiationGuard#<span class="meta">#classname; \</span></span><br><span class="line">    <span class="keyword">template</span> <span class="class"><span class="keyword">class</span> <span class="title">classname</span>&lt;float&gt;;</span> \</span><br><span class="line">    <span class="keyword">template</span> <span class="class"><span class="keyword">class</span> <span class="title">classname</span>&lt;double&gt;</span></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/07/caffe-Layer%E4%B8%AD%E6%9C%89%E4%BB%80%E4%B9%88/" data-id="ckb581vn200007ofzbv2f7tac" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-blob-cpp文件" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/" class="article-date">
  <time datetime="2020-06-06T15:51:19.000Z" itemprop="datePublished">2020-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/">caffe-Blob.cpp文件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>定义Blob类中每个成员函数，<code>void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp;)</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape) &#123;</span><br><span class="line">    <span class="comment">// 确保Shape中元素少于Blob允许的最大维度数</span></span><br><span class="line">    CHECK_LE(shape.size(), kMaxBlobAxes);</span><br><span class="line">    count_ = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 调用vector.resize(), </span></span><br><span class="line">    shape_.resize(shape.size());</span><br><span class="line">    <span class="comment">// shape_data_为空指针，或这个指针多指向的内存小于变形后的大小，则：</span></span><br><span class="line">    <span class="keyword">if</span> (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)) &#123;</span><br><span class="line">        <span class="comment">// 让shape_data_接管一个新的指针，它指向一块新的内存</span></span><br><span class="line">        shape_data_.reset(<span class="keyword">new</span> SyncedMemory(shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开辟临时空间向其传入当前CPU数据，返回一个指针shape_data</span></span><br><span class="line">    <span class="keyword">int</span>* shape_data = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>*&gt;(shape_data_-&gt;mutable_cpu_data());</span><br><span class="line">    <span class="comment">// 遍历Shape中每一元素（每一维度）</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.size(); ++i) &#123;</span><br><span class="line">        CHECK_GE(shape[i], <span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 只要这个Blob中的count_(元素个数)不是0，即这个Blob存在元素，则？？？</span></span><br><span class="line">        <span class="comment">// 若Blob中不存在元素，则？？？。</span></span><br><span class="line">        <span class="keyword">if</span> (count_ != <span class="number">0</span>) &#123; </span><br><span class="line">            CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; <span class="string">"blob size exceeds INT_MAX"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 更新count_, 记录变形后的这个Blob元素个数</span></span><br><span class="line">        count_ *= shape[i];</span><br><span class="line">        <span class="comment">// 更新shape_，用新的这个维度值替换旧的</span></span><br><span class="line">        shape_[i] = shape[i];</span><br><span class="line">        <span class="comment">// ????????</span></span><br><span class="line">        shape_data[i] = shape[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当数据个数超过Blob容量，怎更新容量大小</span></span><br><span class="line">    <span class="keyword">if</span> (count_ &gt; capacity_) &#123;</span><br><span class="line">        capacity_ = count_;</span><br><span class="line">        <span class="comment">// 让data_和diff_分别接管一块新的内存</span></span><br><span class="line">        data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));</span><br><span class="line">        diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个函数的功能，当新的reshape后所有维度元素个数N之和小于原来元素个数M，只取M的前N个元素；<br>当等于时，元素不变。当大于时，所有元素变为零。不过正常的使用是元素个数相同间的reshape。</p>
<p>有了上述的方法，下面的方法调用上面的方法：<br><code>void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp;)</code><br><code>void Blob&lt;Dtype&gt;::Reshape(int,int,int,int)</code>：</p>
<p>读取Blob中已有的cpu_data</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data() <span class="keyword">const</span> &#123;</span><br><span class="line">  CHECK(data_);</span><br><span class="line">  <span class="comment">// 成员data_为共享指针，其指向的存储空间含有cpu_data</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为Blob设置cpu_data</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) &#123;</span><br><span class="line">    CHECK(data);</span><br><span class="line">    <span class="comment">// Make sure CPU and GPU sizes remain equal</span></span><br><span class="line">    <span class="keyword">size_t</span> size = count_ * <span class="keyword">sizeof</span>(Dtype);</span><br><span class="line">    <span class="keyword">if</span> (data_-&gt;size() != size) &#123;</span><br><span class="line">        data_.reset(<span class="keyword">new</span> SyncedMemory(size));</span><br><span class="line">        diff_.reset(<span class="keyword">new</span> SyncedMemory(size));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 用传入参数'data'设置data_成员变量</span></span><br><span class="line">    data_-&gt;set_cpu_data(data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可写访问CPU_data</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() &#123;</span><br><span class="line">    CHECK(data_);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其他关于访问，设置cpu，gpu的 data和diff都类似，略：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// const只读访问</span></span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// 只读访问GPU数据形状</span></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="title">gpu_shape</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">gpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">gpu_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// mutable读写访问</span></span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_diff</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_gpu_diff</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 设置cpu和gpu数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(Dtype* data)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(Dtype* data)</span></span>;</span><br></pre></td></tr></table></figure>

<p>共享BLob数据data：共享diff与下面代码一样。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareData(<span class="keyword">const</span> Blob&amp; other) &#123;</span><br><span class="line">  CHECK_EQ(count_, other.count());</span><br><span class="line">  <span class="comment">// 将这个BLob的data_设为与other一样的值，共享</span></span><br><span class="line">  data_ = other.data();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行Updata():<br>其中：<code>caffe_axpy</code>在<code>src/caffe/util/math_functions.cpp</code>中，<br><code>caffe_gpu_axpy</code>在<code>src/caffe/util/math_functions.cu</code>中。<br>这两个操作实际是：<code>data_[i] = data_[i] - diff_[i], 其中i=0,1,2,3,4...</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Update() &#123;</span><br><span class="line">    <span class="comment">// 执行计算取决于数据在哪</span></span><br><span class="line">    <span class="keyword">switch</span> (data_-&gt;head()) &#123;    <span class="comment">// 获得当前SyncedMemory对象状态</span></span><br><span class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:  <span class="comment">// 如果在cpu则</span></span><br><span class="line">        <span class="comment">// 执行在CPU上的计算</span></span><br><span class="line">        caffe_axpy&lt;Dtype&gt;(count_, </span><br><span class="line">            Dtype(<span class="number">-1</span>),</span><br><span class="line">            <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;cpu_data()),</span><br><span class="line">            <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</span><br><span class="line">    <span class="keyword">case</span> SyncedMemory::SYNCED:</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">    <span class="comment">// 若使用GPU，则执行在GPU上update()</span></span><br><span class="line">    caffe_gpu_axpy&lt;Dtype&gt;(count_, </span><br><span class="line">        Dtype(<span class="number">-1</span>),</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;gpu_data()),</span><br><span class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    NO_GPU;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        LOG(FATAL) &lt;&lt; <span class="string">"Syncedmem not initialized."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其他类似的函数结构相同，只是核心操作不同，略</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算l1范数 元素和</span></span><br><span class="line"><span class="function">Dtype <span class="title">asum_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype <span class="title">asum_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// 计算l2范数 元素平方和</span></span><br><span class="line"><span class="function">Dtype <span class="title">sumsq_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype <span class="title">sumsq_diff</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="comment">// 元素可以一个常数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_data</span><span class="params">(Dtype scale_factor)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_diff</span><span class="params">(Dtype scale_factor)</span></span>;</span><br></pre></td></tr></table></figure>

<p>共享数据很直接，略</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 共享other这个Blob的data_和diff_</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShareData</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShareDiff</span><span class="params">(<span class="keyword">const</span> Blob&amp; other)</span></span>;</span><br></pre></td></tr></table></figure>

<p>从其他blob拷贝数据到当前Blob：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, </span><br><span class="line">    <span class="keyword">bool</span> reshape) &#123;</span><br><span class="line">    <span class="comment">// 必要时reshape</span></span><br><span class="line">    <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</span><br><span class="line">        <span class="keyword">if</span> (reshape) &#123;</span><br><span class="line">            ReshapeLike(source);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG(FATAL) &lt;&lt; <span class="string">"Trying to copy blobs of different sizes."</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">switch</span> (Caffe::mode()) &#123;</span><br><span class="line">        <span class="comment">// 如果是GPU模式就拷贝GPU数据</span></span><br><span class="line">        <span class="keyword">case</span> Caffe::GPU:</span><br><span class="line">            <span class="keyword">if</span> (copy_diff) &#123;</span><br><span class="line">                caffe_copy(count_, source.gpu_diff(),</span><br><span class="line">                    <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                caffe_copy(count_, source.gpu_data(),</span><br><span class="line">                    <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        <span class="comment">// 如果是CPU模式就拷贝CPU数据</span></span><br><span class="line">        <span class="keyword">case</span> Caffe::CPU:</span><br><span class="line">            <span class="keyword">if</span> (copy_diff) &#123;</span><br><span class="line">            caffe_copy(count_, source.cpu_diff(),</span><br><span class="line">                <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            caffe_copy(count_, source.cpu_data(),</span><br><span class="line">                <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">        LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<p>反序列化数据，将磁盘数据读入protobuff：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::FromProto(<span class="keyword">const</span> BlobProto&amp; proto, <span class="keyword">bool</span> reshape) &#123;</span><br><span class="line">    <span class="comment">// 如果需要reshape，先reshape</span></span><br><span class="line">    <span class="keyword">if</span> (reshape) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape;</span><br><span class="line">        <span class="keyword">if</span> (proto.has_num() || proto.has_channels() ||</span><br><span class="line">            proto.has_height() || proto.has_width()) &#123;</span><br><span class="line">            <span class="comment">// Using deprecated 4D Blob dimensions --</span></span><br><span class="line">            <span class="comment">// shape is (num, channels, height, width).</span></span><br><span class="line">            shape.resize(<span class="number">4</span>);</span><br><span class="line">            shape[<span class="number">0</span>] = proto.num();</span><br><span class="line">            shape[<span class="number">1</span>] = proto.channels();</span><br><span class="line">            shape[<span class="number">2</span>] = proto.height();</span><br><span class="line">            shape[<span class="number">3</span>] = proto.width();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            shape.resize(proto.shape().dim_size());</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; proto.shape().dim_size(); ++i) &#123;</span><br><span class="line">                shape[i] = proto.shape().dim(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        Reshape(shape); <span class="comment">// 按照维度信息变换</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        CHECK(ShapeEquals(proto)) &lt;&lt; <span class="string">"shape mismatch (reshape not set)"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 从protobuff拷贝数据到当前Blob：</span></span><br><span class="line">    <span class="comment">// 获取当前Blob的mutable_cpu_data的地址data_vec,</span></span><br><span class="line">    <span class="comment">// 将protobuff中double或float数据 data写入到地址data_vec  </span></span><br><span class="line">    <span class="comment">// diff 与data一样：</span></span><br><span class="line">    Dtype* data_vec = mutable_cpu_data();</span><br><span class="line">    <span class="keyword">if</span> (proto.double_data_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        CHECK_EQ(count_, proto.double_data_size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            data_vec[i] = proto.double_data(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    CHECK_EQ(count_, proto.data_size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            data_vec[i] = proto.data(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (proto.double_diff_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        CHECK_EQ(count_, proto.double_diff_size());</span><br><span class="line">        Dtype* diff_vec = mutable_cpu_diff();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            diff_vec[i] = proto.double_diff(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (proto.diff_size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        CHECK_EQ(count_, proto.diff_size());</span><br><span class="line">        Dtype* diff_vec = mutable_cpu_diff();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            diff_vec[i] = proto.diff(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将数据序列化（写入磁盘）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">void</span> Blob&lt;<span class="keyword">double</span>&gt;::ToProto(BlobProto* proto, <span class="keyword">bool</span> write_diff) <span class="keyword">const</span> &#123;</span><br><span class="line">    <span class="comment">// 重置protobuff维度，清理原有的数据 并写入新的cpu_data数据</span></span><br><span class="line">    proto-&gt;clear_shape();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape_.size(); ++i) &#123;</span><br><span class="line">        proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    proto-&gt;clear_double_data();</span><br><span class="line">    proto-&gt;clear_double_diff();</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span>* data_vec = cpu_data();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">        proto-&gt;add_double_data(data_vec[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果需要写入diff，也要写入cpu_diff数据</span></span><br><span class="line">    <span class="keyword">if</span> (write_diff) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">double</span>* diff_vec = cpu_diff();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count_; ++i) &#123;</span><br><span class="line">            proto-&gt;add_double_diff(diff_vec[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/06/caffe-blob-cpp%E6%96%87%E4%BB%B6/" data-id="ckb3x32no0000bcfze29lg327" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-SyncedMemory" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/06/caffe-SyncedMemory/" class="article-date">
  <time datetime="2020-06-06T15:46:51.000Z" itemprop="datePublished">2020-06-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/06/caffe-SyncedMemory/">caffe-SyncedMemory</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>结构：caffe命名空间中包含两个inline函数和类SyncedMemory的声明。 </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CAFFE_SYNCEDMEM_HPP_</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CAFFE_SYNCEDMEM_HPP_</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MKL</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"mkl.h"</span>  <span class="comment">// 使用intel数学运算库MKL</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/common.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> caffe &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用cudaMallocHost()方法从Host内存开辟空间。从这里开辟空间对于单个GPU性能提升不明显，但是对于大的模型在多GPU上的训练，有较大的性能提升。</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeMallocHost</span><span class="params">(<span class="keyword">void</span>** ptr, <span class="keyword">size_t</span> size, <span class="keyword">bool</span>* use_cuda)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY <span class="comment">// 如果没有指明CPU_ONLY，则：</span></span></span><br><span class="line">    <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123;</span><br><span class="line">        CUDA_CHECK(cudaMallocHost(ptr, size));</span><br><span class="line">        *use_cuda = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MKL <span class="comment">// 如果使用MKL，则：</span></span></span><br><span class="line">    *ptr = mkl_malloc(size ? size:<span class="number">1</span>, <span class="number">64</span>);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span>  <span class="comment">// 否则：</span></span></span><br><span class="line">    *ptr = <span class="built_in">malloc</span>(size);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    *use_cuda = <span class="literal">false</span>;</span><br><span class="line">    CHECK(*ptr) &lt;&lt; <span class="string">"host allocation of size "</span> &lt;&lt; size &lt;&lt; <span class="string">" failed"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与开辟空间对应，用于释放内存。</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeFreeHost</span><span class="params">(<span class="keyword">void</span>* ptr, <span class="keyword">bool</span> use_cuda)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">    <span class="keyword">if</span> (use_cuda) &#123;</span><br><span class="line">        CUDA_CHECK(cudaFreeHost(ptr));</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MKL</span></span><br><span class="line">    mkl_free(ptr);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">free</span>(ptr);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 负责Host和Device内存的分配，和两者间内存同步</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SyncedMemory</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构造和析构函数</span></span><br><span class="line">    SyncedMemory();</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">SyncedMemory</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line">    ~SyncedMemory();</span><br><span class="line">    <span class="comment">// 只读方式只读数据</span></span><br><span class="line">    <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 设置数据</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</span><br><span class="line">    <span class="comment">// 读写方式读取数据</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">// 状态变量：未初始化，CPU数据有效，GPU数据有效，已同步</span></span><br><span class="line">    <span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, </span><br><span class="line">                        HEAD_AT_CPU, </span><br><span class="line">                        HEAD_AT_GPU, </span><br><span class="line">                        SYNCED &#125;;</span><br><span class="line">    <span class="comment">// 获取当前状态变量值</span></span><br><span class="line">    <span class="function">SyncedHead <span class="title">head</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> head_; &#125;</span><br><span class="line">    <span class="comment">// 返回当前存储空间的大小</span></span><br><span class="line">    <span class="keyword">size_t</span> size() <span class="keyword">const</span> &#123; <span class="keyword">return</span> size_; &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">async_gpu_push</span><span class="params">(<span class="keyword">const</span> cudaStream_t&amp; stream)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">check_device</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">to_cpu</span><span class="params">()</span></span>;  <span class="comment">// 同步数据到host</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">to_gpu</span><span class="params">()</span></span>;  <span class="comment">// 同步数据到Device</span></span><br><span class="line">    <span class="keyword">void</span>* cpu_ptr_;  <span class="comment">// 位于Host的数据指针</span></span><br><span class="line">    <span class="keyword">void</span>* gpu_ptr_;  <span class="comment">// 位于Device的数据指针</span></span><br><span class="line">    <span class="keyword">size_t</span> size_;  <span class="comment">// 存储空间的大小</span></span><br><span class="line">    SyncedHead head_;  <span class="comment">// 当前状态变量</span></span><br><span class="line">    <span class="keyword">bool</span> own_cpu_data_;  <span class="comment">// 是否有cpu数据的所有权</span></span><br><span class="line">    <span class="keyword">bool</span> cpu_malloc_use_cuda_;  <span class="comment">// 是否</span></span><br><span class="line">    <span class="keyword">bool</span> own_gpu_data_;  <span class="comment">// 是否有gpu数据的所有权</span></span><br><span class="line">    <span class="keyword">int</span> device_;  <span class="comment">// 设备号</span></span><br><span class="line"></span><br><span class="line">    DISABLE_COPY_AND_ASSIGN(SyncedMemory);</span><br><span class="line">&#125;;  <span class="comment">// class SyncedMemory</span></span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace caffe</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// CAFFE_SYNCEDMEM_HPP_</span></span></span><br></pre></td></tr></table></figure>

<h1 id="补充，条件编译预编译指令"><a href="#补充，条件编译预编译指令" class="headerlink" title="补充，条件编译预编译指令"></a>补充，条件编译预编译指令</h1><p><code>#define</code><br><code>#undef</code><br><code>#if</code><br><code>#ifdef</code><br><code>#ifndef</code><br><code>#elif</code><br><code>#else</code><br><code>#endif</code><br><code>defined</code>：与#if, #elif配合使用，判断某个宏是否被定义</p>
<p>宏<code>USE_MKL</code>的定义在文件<code>include/caffe/util/mkl_alternate.hpp</code>中。<br>宏<code>CPU_ONLY</code>的定义在文件``？？？？？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/06/caffe-SyncedMemory/" data-id="ckb3tbsv90002v6fzfnpmc64r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">53</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">33</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a><span class="tag-list-count">18</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 17.5px;">CUDA</a> <a href="/tags/Caffe/" style="font-size: 15px;">Caffe</a> <a href="/tags/Test-Analysis/" style="font-size: 12.5px;">Test Analysis</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 10px;">二分查找</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 10px;">递归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">31</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/30/%E6%AF%94%E7%89%B9%E5%B8%81-%E4%BB%A5%E5%A4%AA%E5%9D%8A-%E5%8C%BA%E5%9D%97%E9%93%BE/">比特币-以太坊-区块链</a>
          </li>
        
          <li>
            <a href="/2020/06/30/LeetCode-%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%E7%9B%B8%E5%85%B3/">LeetCode-优先队列相关</a>
          </li>
        
          <li>
            <a href="/2020/06/28/caffe-%E5%B7%A5%E5%85%B7%E7%AE%B1/">caffe-工具箱</a>
          </li>
        
          <li>
            <a href="/2020/06/25/LeetCode-merge%E5%BA%94%E7%94%A8-%E6%B1%82%E9%80%86%E5%BA%8F%E5%AF%B9/">LeetCode-merge应用-求逆序对</a>
          </li>
        
          <li>
            <a href="/2020/06/23/LeetCode-%E9%93%BE%E8%A1%A8%E7%9B%B8%E5%85%B3/">LeetCode-链表相关</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>