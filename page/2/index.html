<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Junhui&#39;s Journal">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junhui">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-caffe-sigmoid-cross-entropy-loss-layer类" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/02/caffe-sigmoid-cross-entropy-loss-layer%E7%B1%BB/" class="article-date">
  <time datetime="2020-06-01T22:46:30.000Z" itemprop="datePublished">2020-06-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/02/caffe-sigmoid-cross-entropy-loss-layer%E7%B1%BB/">caffe-sigmoid_cross_entropy_loss_layer类</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="sigmoid-cross-entropy-loss-layer类"><a href="#sigmoid-cross-entropy-loss-layer类" class="headerlink" title="sigmoid_cross_entropy_loss_layer类"></a>sigmoid_cross_entropy_loss_layer类</h1><p>头文件： <code>./include/caffe/layers/sigmoid_cross_entropy_loss_layer.hpp</code><br>CPU实现： <code>./src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp</code><br>GPU实现： <code>./src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu</code></p>
<h2 id="所需要基本操作的CPU和GPU实现："><a href="#所需要基本操作的CPU和GPU实现：" class="headerlink" title="所需要基本操作的CPU和GPU实现："></a>所需要基本操作的CPU和GPU实现：</h2><p><code>bottom_diff = sigmoid_output_data - target</code> 的实现如下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CPU</span></span><br><span class="line">caffe_sub(count, sigmoid_output_data, target, bottom_diff):</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对应的GPU</span></span><br><span class="line">caffe_copy(count, sigmoid_output_data, bottom_diff);</span><br><span class="line">caffe_gpu_axpy(count, Dtype(<span class="number">-1</span>), target, bottom_diff);</span><br></pre></td></tr></table></figure>

<p><code>bottom_diff</code> 中每个元素乘以<code>loss_weight</code>， 共操作<code>count</code>个元素。其实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CPU</span></span><br><span class="line">caffe_scal(count, loss_weight, bottom_diff);</span><br><span class="line"><span class="comment">// GPU</span></span><br><span class="line">caffe_gpu_scal(count, loss_weight, bottom_diff);</span><br></pre></td></tr></table></figure>

<p>上述函数分别使用了<code>cBLAS</code> 和<code>cuBlas</code>两个库函数。<code>sigmoid_output_data</code>是前向传播的结果。上述两步其实是反向传播的过程，最终将结果写入<code>bottom_diff</code>中，它是<code>Blob</code>的一部分，会随着数据的走向继续传播下去。</p>
<h2 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h2><ol>
<li><p>CPU</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Backward_cpu(</span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, </span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line"><span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</span><br><span class="line">	LOG(FATAL) &lt;&lt; <span class="keyword">this</span>-&gt;type()</span><br><span class="line">			&lt;&lt; <span class="string">" Layer cannot backpropagate to label inputs."</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</span><br><span class="line">	<span class="comment">// First, compute the diff</span></span><br><span class="line">	<span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">	<span class="keyword">const</span> Dtype* sigmoid_output_data = sigmoid_output_-&gt;cpu_data();</span><br><span class="line">	<span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;cpu_data();</span><br><span class="line">	Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</span><br><span class="line">	caffe_sub(count, sigmoid_output_data, target, bottom_diff);</span><br><span class="line">	<span class="comment">// Zero out gradient of ignored targets.</span></span><br><span class="line">	<span class="keyword">if</span> (has_ignore_label_) &#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">			<span class="keyword">const</span> <span class="keyword">int</span> target_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(target[i]);</span><br><span class="line">			<span class="keyword">if</span> (target_value == ignore_label_) &#123;</span><br><span class="line">				bottom_diff[i] = <span class="number">0</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// Scale down gradient</span></span><br><span class="line">	Dtype loss_weight = top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>] / normalizer_;</span><br><span class="line">	caffe_scal(count, loss_weight, bottom_diff);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 因为是在CPU端，与GPU无关，所以上述code中没有<code>gpu_data</code>或<code>gpu_diff</code>。<br> 主要操作，取数据，执行操作：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 取Blob数据</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line"><span class="keyword">const</span> Dtype* sigmoid_output_data = sigmoid_output_-&gt;cpu_data();</span><br><span class="line"><span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;cpu_data();</span><br><span class="line">Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</span><br><span class="line"><span class="comment">// 如上述操作</span></span><br><span class="line">caffe_sub(count, sigmoid_output_data, target, bottom_diff);</span><br></pre></td></tr></table></figure>
</li>
<li><p>GPU</p>
<p> 与cpu相似：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Backward_gpu(</span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, </span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">	<span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</span><br><span class="line">	LOG(FATAL) &lt;&lt; <span class="keyword">this</span>-&gt;type()</span><br><span class="line">				&lt;&lt; <span class="string">" Layer cannot backpropagate to label inputs."</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</span><br><span class="line">		<span class="comment">// First, compute the diff</span></span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">		<span class="keyword">const</span> Dtype* sigmoid_output_data = sigmoid_output_-&gt;gpu_data();</span><br><span class="line">		<span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;gpu_data();</span><br><span class="line">		Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_gpu_diff();</span><br><span class="line">		caffe_copy(count, sigmoid_output_data, bottom_diff);</span><br><span class="line">		caffe_gpu_axpy(count, Dtype(<span class="number">-1</span>), target, bottom_diff);</span><br><span class="line">		<span class="comment">// Zero out gradient of ignored targets.</span></span><br><span class="line">		<span class="keyword">if</span> (has_ignore_label_) &#123;</span><br><span class="line">			<span class="comment">// NOLINT_NEXT_LINE(whitespace/operators)</span></span><br><span class="line">			SigmoidCrossEntropyLossIgnoreDiffGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count),</span><br><span class="line">			CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(count, ignore_label_, target, bottom_diff);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// Scale down gradient</span></span><br><span class="line">		Dtype loss_weight = top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>] / normalizer_;</span><br><span class="line">		caffe_gpu_scal(count, loss_weight, bottom_diff);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 加上kernel函数，其作用是将不需要计算梯度的位置设为零，与CPU含义相同：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SigmoidCrossEntropyLossIgnoreDiffGPU</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">						<span class="keyword">const</span> <span class="keyword">int</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">						<span class="keyword">const</span> <span class="keyword">int</span> ignore_label, </span></span></span><br><span class="line"><span class="function"><span class="params">						<span class="keyword">const</span> Dtype* target, </span></span></span><br><span class="line"><span class="function"><span class="params">						Dtype* diff)</span> </span>&#123;</span><br><span class="line">		CUDA_KERNEL_LOOP(i, count) &#123;</span><br><span class="line">			<span class="keyword">const</span> <span class="keyword">int</span> target_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(target[i]);</span><br><span class="line">			<span class="keyword">if</span> (target_value == ignore_label) &#123;</span><br><span class="line">				diff[i] = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>头文件中的成员属性：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// 一个SigmoidLayer类对象指针，预测值到概率值的映射</span></span><br><span class="line"><span class="built_in">shared_ptr</span>&lt;SigmoidLayer&lt;Dtype&gt; &gt; sigmoid_layer_;</span><br><span class="line"><span class="comment">/// 接收SigmoidLayer的输出.</span></span><br><span class="line"><span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; sigmoid_output_;</span><br><span class="line"><span class="comment">/// bottom vector holder to call the underlying SigmoidLayer::Forward</span></span><br><span class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; sigmoid_bottom_vec_;</span><br><span class="line"><span class="comment">/// top vector holder to call the underlying SigmoidLayer::Forward</span></span><br><span class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; sigmoid_top_vec_;</span><br><span class="line"><span class="comment">/// Whether to ignore instances with a certain label.</span></span><br><span class="line"><span class="keyword">bool</span> has_ignore_label_;</span><br><span class="line"><span class="comment">/// The label indicating that an instance should be ignored.</span></span><br><span class="line"><span class="keyword">int</span> ignore_label_;</span><br><span class="line"><span class="comment">/// How to normalize the loss.</span></span><br><span class="line">LossParameter_NormalizationMode normalization_;</span><br><span class="line">Dtype normalizer_;</span><br><span class="line"><span class="keyword">int</span> outer_num_, inner_num_;</span><br></pre></td></tr></table></figure>

<p>先执行forward操作：<code>sigmoid_layer_-&gt;Forward(_, _)</code> 。其参数<code>sigmoid_bottom_vec_</code>和<code>sigmoid_top_vec_</code>是两个该类的成员变量，其值随操作的执行而改变，这里要改变的是前者，这个实现在源码中的成员函数<code>LayerSetUp()</code>。</p>
<p><code>sigmoid_layer_</code>也是成员变量，其定义：<code>shared_ptr&lt;SigmoidLayer&lt;Dtype&gt; &gt; sigmoid_layer_;</code>。CPU和GPU实现见下：</p>
<ol>
<li><p>CPU</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Forward_cpu(</span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, </span><br><span class="line">						<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">	<span class="comment">// The forward pass computes the sigmoid outputs.</span></span><br><span class="line">	<span class="comment">// 1. Forward计算sigmoid 的输出，并且取数据</span></span><br><span class="line">	sigmoid_bottom_vec_[<span class="number">0</span>] = bottom[<span class="number">0</span>];</span><br><span class="line">	sigmoid_layer_-&gt;Forward(sigmoid_bottom_vec_, sigmoid_top_vec_);</span><br><span class="line">	<span class="comment">// Compute the loss (negative log likelihood)</span></span><br><span class="line">	<span class="comment">// Stable version of loss computation from input data</span></span><br><span class="line">	<span class="keyword">const</span> Dtype* input_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">	<span class="keyword">const</span> Dtype* target = bottom[<span class="number">1</span>]-&gt;cpu_data();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2. 计算 对数似然</span></span><br><span class="line">	<span class="keyword">int</span> valid_count = <span class="number">0</span>;</span><br><span class="line">	Dtype loss = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bottom[<span class="number">0</span>]-&gt;count(); ++i) &#123;</span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> target_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(target[i]);</span><br><span class="line">		<span class="keyword">if</span> (has_ignore_label_ &amp;&amp; target_value == ignore_label_) &#123;</span><br><span class="line">			<span class="keyword">continue</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		loss -= input_data[i] * (target[i] - (input_data[i] &gt;= <span class="number">0</span>)) -</span><br><span class="line">			<span class="built_in">log</span>(<span class="number">1</span> + <span class="built_in">exp</span>(input_data[i] - <span class="number">2</span> * input_data[i] * (input_data[i] &gt;= <span class="number">0</span>)));</span><br><span class="line">		++valid_count;</span><br><span class="line">	&#125;</span><br><span class="line">	normalizer_ = get_normalizer(normalization_, valid_count);</span><br><span class="line">	top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss / normalizer_;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>再看<code>sigmoid_layer_-&gt;Forward(_, _);</code>，<code>SigmoidLayer</code>类并没有<code>Formard()</code>方法，所以此方法一定是从其父类继承而来。看源码找到继承顺序：<code>SigmoidLayer::NeuronLayer::Layer</code>，所以这里的<code>Forward()</code>是<code>Layer</code>类的方法，祥看<code>Layer.hpp</code>。</p>
<ol start="2">
<li><p>GPU</p>
<p> 与CPU类似，将CPU中的for循环由kernel函数代替：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SigmoidCrossEntropyLossForwardGPU</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> nthreads,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">const</span> Dtype* input_data, <span class="keyword">const</span> Dtype* target, Dtype* loss,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">const</span> <span class="keyword">bool</span> has_ignore_label_, <span class="keyword">const</span> <span class="keyword">int</span> ignore_label_,</span></span></span><br><span class="line"><span class="function"><span class="params">		Dtype* counts)</span> </span>&#123;</span><br><span class="line">	CUDA_KERNEL_LOOP(i, nthreads) &#123;</span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> target_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(target[i]);</span><br><span class="line">		<span class="keyword">if</span> (has_ignore_label_ &amp;&amp; target_value == ignore_label_) &#123;</span><br><span class="line">			loss[i] = <span class="number">0</span>;</span><br><span class="line">			counts[i] = <span class="number">0</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			loss[i] = input_data[i] * (target[i] - (input_data[i] &gt;= <span class="number">0</span>)) -</span><br><span class="line">				<span class="built_in">log</span>(<span class="number">1</span> + <span class="built_in">exp</span>(input_data[i] - <span class="number">2</span> * input_data[i] *</span><br><span class="line">				(input_data[i] &gt;= <span class="number">0</span>)));</span><br><span class="line">			counts[i] = <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> GPU中的前传播：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> SigmoidCrossEntropyLossLayer&lt;Dtype&gt;::Forward_gpu()&#123;...&#125;</span><br></pre></td></tr></table></figure>
<p> 函数体省略，不过在源码中有一点提出：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Since this memory is not used for anything, we use it here to avoid having</span></span><br><span class="line"><span class="comment">// to allocate new GPU memory to accumulate intermediate results.</span></span><br><span class="line">Dtype* loss_data = bottom[<span class="number">0</span>]-&gt;mutable_gpu_diff();</span><br><span class="line">Dtype* count_data = bottom[<span class="number">1</span>]-&gt;mutable_gpu_diff();</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Clear scratch memory to prevent interfering with backward (see #6202).</span></span><br><span class="line">caffe_gpu_set(bottom[<span class="number">0</span>]-&gt;count(), Dtype(<span class="number">0</span>), bottom[<span class="number">0</span>]-&gt;mutable_gpu_diff());</span><br><span class="line">caffe_gpu_set(bottom[<span class="number">1</span>]-&gt;count(), Dtype(<span class="number">0</span>), bottom[<span class="number">1</span>]-&gt;mutable_gpu_diff());</span><br></pre></td></tr></table></figure>

<p> 这是CPU版本中没有的，因为kernel函数中需要传入对象数组，但是这部分的地址没有被开辟，所以为了避免在GPU上为中间结果开辟空间，所以使用Blob的暂时没有使用到的部分，作为临时存储空间，只不过，函数结束后要清理这部分空间。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p> 这个类除了上述的方法，还有其他方法详见源文件。</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/02/caffe-sigmoid-cross-entropy-loss-layer%E7%B1%BB/" data-id="ckax32ye80002lcfz8wrwhkd6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-sigmoidLayer类" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/02/caffe-sigmoidLayer%E7%B1%BB/" class="article-date">
  <time datetime="2020-06-01T22:44:29.000Z" itemprop="datePublished">2020-06-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/02/caffe-sigmoidLayer%E7%B1%BB/">caffe-sigmoidLayer类</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>源码初体验，看一下sigmoid_layer类。</p>
<h1 id="sigmoid-layers类"><a href="#sigmoid-layers类" class="headerlink" title="sigmoid_layers类"></a>sigmoid_layers类</h1><p>这个类的所有内容</p>
<p>头文件： <code>./include/caffe/layers/sigmoid_layer.hpp</code><br>CPU实现： <code>./src/caffe/layers/sigmoid_layer.cpp</code><br>GPU实现：<code>./src/caffe/layers/sigmoid_layer.cu</code></p>
<p>对于这个类的官方文档<a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html" target="_blank" rel="noopener">见此</a></p>
<h2 id="头文件sigmoid-layer-hpp中包含"><a href="#头文件sigmoid-layer-hpp中包含" class="headerlink" title="头文件sigmoid_layer.hpp中包含"></a>头文件sigmoid_layer.hpp中包含</h2><ol>
<li>继承自NeuronLayer::Layer类的构造函数：SigmoidLayer()</li>
<li>返回这个列的名字：type()</li>
<li>前先计算的CPU声明：Forward_cpu()和GPU声明：Forward_gpu()</li>
<li>后传计算的CPU声明：Backward_cpu()和GPU声明：Backward_gpu()</li>
</ol>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><ol>
<li><p>CPU</p>
<p> 前向计算是将bottom数据经过sigmoid函数得到top数据。所以其基本操作是sigmoid()。CPU实现：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Dtype <span class="title">sigmoid</span><span class="params">(Dtype x)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0.5</span> * <span class="built_in">tanh</span>(<span class="number">0.5</span> * x) + <span class="number">0.5</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 有了sigmoid()，前向传播计算如下：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Forward_cpu(<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">	Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">	Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</span><br><span class="line">	<span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">		top_data[i] = sigmoid(bottom_data[i]);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> Blob是caffe中最小的数据载体，Blob的定义见Blob的笔记博客。</p>
</li>
<li><p>GPU<br> sigmoid()对应的GPU实现：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SigmoidForward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out)</span> </span>&#123;</span><br><span class="line">	CUDA_KERNEL_LOOP(index, n) &#123;</span><br><span class="line">		out[index] = <span class="number">0.5</span> * <span class="built_in">tanh</span>(<span class="number">0.5</span> * in[index]) + <span class="number">0.5</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 其中<code>CUDA_KERNEL_LOOP(index, n)</code>给定线程id，并且将线程映射到数据上，实现数据并行。其宏定义在这里<code>include/caffe/util/device_alternate.hpp</code>：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CUDA: grid stride looping</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_KERNEL_LOOP(i, n) \</span></span><br><span class="line"> 	<span class="keyword">for</span> (<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; \</span><br><span class="line">      i &lt; (n); \</span><br><span class="line">      i += blockDim.x * gridDim.x)</span><br></pre></td></tr></table></figure>
<p> 这是个通用的循环，具体细节见关于CUDA的笔记博客。</p>
<p> 同样的，GPU的前行传播：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Forward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">	Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;gpu_data();</span><br><span class="line">	Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_gpu_data();</span><br><span class="line">	<span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line"></span><br><span class="line">	SigmoidForward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</span><br><span class="line">	count, bottom_data, top_data);</span><br><span class="line">	CUDA_POST_KERNEL_CHECK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 其中指定了当下机器每block可用threads数目，并可计算出使用到的block数。</p>
<p> 具体地：<code>CAFFE_CUDA_NUM_THREADS</code>=512，每个block启用512个threads，而    <code>CAFFE_GET_BLOCKS(count)</code>：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">CAFFE_GET_BLOCKS</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> (N + <span class="number">512</span> - <span class="number">1</span>) / <span class="number">512</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 对于像sigmoid简单的算子，直观上看，GPU实现其实就是将CPU实现的最内层的循环去掉，用并行执行的kernel函数替代。</p>
</li>
</ol>
<h2 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h2><ol>
<li><p>CPU</p>
<p> 根据sigmoid 反向传播公式可以很容易写出如下：{<font color="red" size="4">将code中去掉的const都加上</font>}</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Backward_cpu(</span><br><span class="line">							<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span><br><span class="line">							<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span><br><span class="line">							<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">	<span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</span><br><span class="line">		Dtype* top_data = top[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">		Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</span><br><span class="line">		Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</span><br><span class="line">		<span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">			<span class="keyword">const</span> Dtype sigmoid_x = top_data[i];</span><br><span class="line">			bottom_diff[i] = top_diff[i] * sigmoid_x * (<span class="number">1.</span> - sigmoid_x);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 其中<code>top_diff[i]</code>是与前行传播的输出有关的数值。</p>
</li>
<li><p>GPU</p>
<p> 对于GPU实现，只需将上述code 中最内层循环用kernel函数代替，所以要实现kernel函数：{<font color="red" size="4">将code中去掉的const都加上</font>}</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">SigmoidBackward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, </span></span></span><br><span class="line"><span class="function"><span class="params">							Dtype* in_diff,</span></span></span><br><span class="line"><span class="function"><span class="params">							Dtype* out_data, </span></span></span><br><span class="line"><span class="function"><span class="params">							Dtype* out_diff)</span> </span>&#123;</span><br><span class="line">	CUDA_KERNEL_LOOP(index, n) &#123;</span><br><span class="line">	Dtype sigmoid_x = out_data[index];</span><br><span class="line">		out_diff[index] = in_diff[index] * sigmoid_x * (<span class="number">1</span> - sigmoid_x);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 替换循环：{<font color="red" size="4">将code中去掉的const都加上</font>}</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> SigmoidLayer&lt;Dtype&gt;::Backward_gpu(<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span><br><span class="line">							<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span><br><span class="line">							<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">	<span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</span><br><span class="line">		Dtype* top_data = top[<span class="number">0</span>]-&gt;gpu_data();</span><br><span class="line">		Dtype* top_diff = top[<span class="number">0</span>]-&gt;gpu_diff();</span><br><span class="line">		Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_gpu_diff();</span><br><span class="line">		<span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line"></span><br><span class="line">		SigmoidBackward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</span><br><span class="line">			count, top_diff, top_data, bottom_diff);</span><br><span class="line">		CUDA_POST_KERNEL_CHECK;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 上述很直接。</p>
</li>
</ol>
<p><font color="gree" size="5">敲黑板</font><br><font color="orange" size="4">技巧</font>：在linux中使用<code>grep</code>命令可以在一个项目中查找关键字：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -n -H -r <span class="string">"CUDA_KERNEL_LOOP"</span></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/06/02/caffe-sigmoidLayer%E7%B1%BB/" data-id="ckax32ycl0000lcfzhejk7s23" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-anaconda-虚拟环境" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/26/anaconda-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/" class="article-date">
  <time datetime="2020-03-25T23:54:49.000Z" itemprop="datePublished">2020-03-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Utility/">Utility</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/26/anaconda-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/">anaconda 虚拟环境</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="conda-虚拟环境"><a href="#conda-虚拟环境" class="headerlink" title="conda 虚拟环境"></a>conda 虚拟环境</h2><p>conda使得在不同项目中使用不同版本的包包，不同环境中的包互不冲突。<br>而且可以指定包的版本，非常方便。</p>
<p>常用命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">conda env list        <span class="comment"># 列出已存在的虚拟环境</span></span><br><span class="line">conda create --name yolo python=3.5  <span class="comment">#新建yolo环境并且安装python3.5</span></span><br><span class="line"></span><br><span class="line">conda activate yolo   <span class="comment">#进入或者切换到yolo</span></span><br><span class="line">conda deactivate</span><br><span class="line">conda info --envs</span><br><span class="line"></span><br><span class="line">conda search keras   <span class="comment">#搜索keras的所有可下载版本</span></span><br><span class="line">conda list -n yolo   <span class="comment">#列出yolo环境中已有 包</span></span><br><span class="line">conda install -n yolo keras==2.1.5  <span class="comment">#向指定环境中安装指定的包</span></span><br><span class="line"></span><br><span class="line">conda remove -n yolo keras</span><br><span class="line">conda upgrade -n yolo keras</span><br><span class="line"></span><br><span class="line">conda remove -n yolo --all    <span class="comment">#删除整个yolo环境</span></span><br><span class="line">conda create -n yolo --<span class="built_in">clone</span> yolov3   <span class="comment">#复制yolo环境</span></span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes   <span class="comment">#设置搜索时显示通道地址</span></span><br><span class="line">conda config --show      <span class="comment">#产看镜像源</span></span><br></pre></td></tr></table></figure>

<h2 id="trouble-shooting"><a href="#trouble-shooting" class="headerlink" title="trouble shooting"></a>trouble shooting</h2><p>错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install: Segmentation fault</span><br></pre></td></tr></table></figure>

<p>原因：由于网络或者其他原因，包下载不完整。</p>
<p>解决：清除所有不完整的缓存，后重新安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda clean -a</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/26/anaconda-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/" data-id="ckatxvx5c0002lzfzd7ej0jyx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-LeetCode-nowcoder-深入理解链表" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/12/LeetCode-nowcoder-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E9%93%BE%E8%A1%A8/" class="article-date">
  <time datetime="2020-03-12T09:40:51.000Z" itemprop="datePublished">2020-03-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LeetCode/">LeetCode</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/12/LeetCode-nowcoder-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E9%93%BE%E8%A1%A8/">LeetCode-符串通配符</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>自己处理输入输出</p>
<ul>
<li><p>描述：</p>
<p>  实现如下2个通配符：</p>
<ol>
<li><p><code>*</code>：匹配0个或以上的字符（字符由英文字母和数字0-9组成，不区分大小写。下同）</p>
</li>
<li><p><code>？</code>：匹配1个字符</p>
<p>input:</p>
<p>先输入一个带有通配符的字符串，再输入一个需要匹配的字符串。如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te?t*.*</span><br><span class="line">txt12.xls</span><br></pre></td></tr></table></figure>

<p>output:</p>
<p>返回匹配的结果，正确输出true，错误输出false。如上例返回false。</p>
</li>
</ol>
</li>
<li><p>思路：</p>
<ol>
<li>终止条件先后有序</li>
<li>对于<code>if(*str1 == &#39;*&#39;)</code>中，三个递归match，好好体会三种情况<ol>
<li><code>a*c</code>, <code>ac</code>。<code>*</code>与0个匹配</li>
<li><code>a*c</code>, <code>abc</code>。<code>*</code>与1个匹配</li>
<li><code>a*c</code>, <code>abbbc</code>。<code>*</code>与多个匹配</li>
</ol>
</li>
</ol>
</li>
<li><p>实现：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">match</span><span class="params">(<span class="keyword">char</span> *str1, <span class="keyword">char</span> *str2)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 终止条件 同时到字符串尾，放回true</span></span><br><span class="line">    <span class="keyword">if</span>(*str1 == <span class="string">'\0'</span>  &amp;&amp; *str2 == <span class="string">'\0'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="comment">// 只有一个到尾，返回false</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(*str1 == <span class="string">'\0'</span> || *str2 == <span class="string">'\0'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// 对于‘？’，一定匹配，所以查看下一对字符</span></span><br><span class="line">    <span class="keyword">if</span>(*str1 == <span class="string">'?'</span>)</span><br><span class="line">        <span class="keyword">return</span> match(str1+<span class="number">1</span>, str2+<span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 当两个字符相等，一定匹配，查看下一对字符</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(*str1 == *str2)</span><br><span class="line">        <span class="keyword">return</span> match(str1+<span class="number">1</span>, str2+<span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 对于‘*’, 匹配零个，一个或多个</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(*str1 == <span class="string">'*'</span>)</span><br><span class="line">        <span class="keyword">return</span> match(str1+<span class="number">1</span>, str2) ||    <span class="comment">//零个</span></span><br><span class="line">               match(str1+<span class="number">1</span>, str2+<span class="number">1</span>) ||  <span class="comment">// 一个</span></span><br><span class="line">               match(str1, str2+<span class="number">1</span>);      <span class="comment">// 多个</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> str1[<span class="number">100</span>], str2[<span class="number">100</span>];</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;str1&gt;&gt;str2)&#123;</span><br><span class="line">        <span class="keyword">if</span>(match(str1, str2))</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"true"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"false"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/12/LeetCode-nowcoder-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E9%93%BE%E8%A1%A8/" data-id="ckatsrgs1002txqfz3xpwejc8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Cpp-pro-tip-3-尽可能不使用类型转换" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/12/Cpp-pro-tip-3-%E5%B0%BD%E5%8F%AF%E8%83%BD%E4%B8%8D%E4%BD%BF%E7%94%A8%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/" class="article-date">
  <time datetime="2020-03-12T09:35:30.000Z" itemprop="datePublished">2020-03-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/12/Cpp-pro-tip-3-%E5%B0%BD%E5%8F%AF%E8%83%BD%E4%B8%8D%E4%BD%BF%E7%94%A8%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/">Cpp pro tip 3 杂记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="explicit-修饰constructor"><a href="#explicit-修饰constructor" class="headerlink" title="explicit 修饰constructor"></a>explicit 修饰constructor</h1><p>比较区别：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 无explicit</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">foo</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    foo(<span class="keyword">int</span> n)&#123; num_ = n; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> num_;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 有explicit</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">bar</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">bar</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123; num_ = n; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> num_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">foo <span class="title">f1</span><span class="params">(<span class="number">2</span>)</span></span>;   <span class="comment">// correct</span></span><br><span class="line">    foo* f2 = <span class="keyword">new</span> foo(<span class="number">2</span>);  <span class="comment">// correct</span></span><br><span class="line">    foo f3 = <span class="number">2</span>;  <span class="comment">// correct </span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function">bar <span class="title">b1</span><span class="params">(<span class="number">3</span>)</span></span>;   <span class="comment">// correct</span></span><br><span class="line">    bar* b2 = <span class="keyword">new</span> bar(<span class="number">3</span>); <span class="comment">// correct</span></span><br><span class="line">    bar b3 = <span class="number">3</span>; <span class="comment">// incorrect</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>bar b3 = 3</code>含义是，3的类型int正好是bar单参数构造函数的参数类型int，这时候编译器就自动调用这个构造函数，创建一个bar的对象。这在<code>explicit</code>修饰下是不可行的，只能显式地以3为参数传给构造函数。</p>
<h1 id="shared-ptr-lt-gt"><a href="#shared-ptr-lt-gt" class="headerlink" title="shared_ptr&lt;&gt;"></a>shared_ptr&lt;&gt;</h1><p>理解shared_ptr可以这样理解：<br>对于<code>std::shared_ptr&lt;string&gt; ptr(new string)</code>，这里有两个对象，“我”和“你”（均为指针）。“我”是<code>ptr</code>，而“你”对应了<code>new string</code>。这时“我”和“你”就有了关系，<font color="red">“我”管理“你”</font>。当<code>new string</code>即“你”不存在时，我不管理任何人，即我管理一个空指针。</p>
<h2 id="定义并初始化"><a href="#定义并初始化" class="headerlink" title="定义并初始化"></a>定义并初始化</h2><p>定义一个<font color="red">空共享指针</font>，指向类型为int的对象, owns no pointer（“我”没有管理任何指针）, 由于ptr1是空指针，所以为ptr1所指向的对象赋值是违法的：<code>segmentation fault</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="built_in">string</span>&gt; ptr1; </span><br><span class="line"><span class="keyword">if</span> (!ptr1)</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"ptr1==NULL"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// 违法，因为ptr1没有指向任何对象，所以给对象赋值是逻辑错误</span></span><br><span class="line">*ptr1 = <span class="string">"string ptr1"</span>;</span><br></pre></td></tr></table></figure>

<p>初始化一个shared_ptr ptr2，<font color="red">非空指针</font>，指向“”。此后，ptr2 就可以像string* 类型的指针一样使用。ptr2 可以管理使用new 关键字分配出来的指针。（<font color="red">new 出来的一定是个指针</font>）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ptr2 管理一个指向string的指针</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="built_in">string</span>&gt; ptr2(<span class="keyword">new</span> <span class="built_in">string</span>);</span><br><span class="line">*ptr2 = <span class="string">"string ptr2"</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*ptr2&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>

<p>常用的初始化方法:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一般的初始化一个shared_ptr方法</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="built_in">string</span>&gt; ptr3(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"string ptr3"</span>));</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*ptr3&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 推荐的初始化一个shared_ptr</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="built_in">string</span>&gt; ptr4 = make_shared&lt;<span class="built_in">string</span>&gt;(<span class="string">"string ptr4"</span>);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*ptr4&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>

<h2 id="成员方法"><a href="#成员方法" class="headerlink" title="成员方法"></a>成员方法</h2><p><code>ptr.get()</code>方法，它返回一个指针，这个指针指向ptr所指向对象：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">string</span>* p = ptr4.get();</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*p&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p><code>ptr.reset()</code>方法，改变ptr所指向内容。这里有个好例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; sp;  <span class="comment">// empty </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 管理一个指针</span></span><br><span class="line">sp.reset (<span class="keyword">new</span> <span class="keyword">int</span>);       </span><br><span class="line">*sp=<span class="number">10</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; *sp &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除所管理的指针, 干礼一个新指针</span></span><br><span class="line">sp.reset (<span class="keyword">new</span> <span class="keyword">int</span>);       </span><br><span class="line">*sp=<span class="number">20</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; *sp &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除所管理的指针</span></span><br><span class="line">sp.reset();</span><br></pre></td></tr></table></figure>

<p>以下情况相同：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ptr4.reset(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"string reset ptr4"</span>));</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*ptr4&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span>* pp = <span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"fuck you"</span>);</span><br><span class="line">ptr4.reset(pp);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;*ptr4&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/12/Cpp-pro-tip-3-%E5%B0%BD%E5%8F%AF%E8%83%BD%E4%B8%8D%E4%BD%BF%E7%94%A8%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/" data-id="ckatsrgrw002hxqfz00ox1w7j" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Cpp-pro-tip-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/09/Cpp-pro-tip-2/" class="article-date">
  <time datetime="2020-03-09T06:29:07.000Z" itemprop="datePublished">2020-03-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/09/Cpp-pro-tip-2/">Cpp pro tip 尽可能使用const</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>如果某个值需要保持不变，那么你就应该说出来，这样编译器能确保这写约束不被违反。const可以放于很多位置。</p>
<h1 id="1-作用于指针"><a href="#1-作用于指针" class="headerlink" title="1. 作用于指针"></a>1. 作用于指针</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> greeting[] = <span class="string">"hello"</span>;</span><br><span class="line"><span class="keyword">char</span>* p = greeting;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* p = greeting;   <span class="comment">// 1</span></span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span> p = greeting;   <span class="comment">// 2</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* <span class="keyword">const</span> p = greeting;  <span class="comment">// 3</span></span><br></pre></td></tr></table></figure>

<p>const的出现位置[左数右针]：</p>
<ol>
<li><code>const *</code>，<font color="red">数据是常量</font>，表示被指向的对象是常量，而指针是可以变的。</li>
<li><code>* const</code>，表示<font color="red">指针是常量</font>。<font color="red">这个指针不能指向不同的东西，但它指向东西的数据可以改变</font>。</li>
<li><code>const * const</code>，表示指针和被指向的对象都是常量</li>
</ol>
<p>对于在*左侧，下面两种表达意义相同，都在左侧：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* greeting;</span><br><span class="line"><span class="keyword">char</span> <span class="keyword">const</span>* greeting;</span><br></pre></td></tr></table></figure>

<h1 id="2-作用于特殊指针，iterator"><a href="#2-作用于特殊指针，iterator" class="headerlink" title="2. 作用于特殊指针，iterator"></a>2. 作用于特殊指针，iterator</h1><p>对于STL中的迭代器，加入我希望迭代器所指向的东西不可被修改（即对数据只读），那么就需要一个<code>const_iterator</code>：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line"><span class="comment">// 数据const</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::const_iterator cItr = vec.begain();</span><br><span class="line">*cItr = <span class="number">10</span>;   <span class="comment">// 错，所指向的东西不能改</span></span><br><span class="line">cItr++；      <span class="comment">// 对，指针可改</span></span><br></pre></td></tr></table></figure>

<p>而另一种情况，指针不能改变其指向，但是可改变其所指向数据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指针const</span></span><br><span class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::iterator itr = vec.begain();</span><br><span class="line">*itr = <span class="number">10</span>;   <span class="comment">// 对，所指向的东西可改</span></span><br><span class="line">itr++；      <span class="comment">// 错，指针指向不可改</span></span><br></pre></td></tr></table></figure>

<h1 id="3-作用与函数"><a href="#3-作用与函数" class="headerlink" title="3. 作用与函数"></a>3. 作用与函数</h1><ol>
<li><p>函数返回值为const</p>
<p> 将函数返回值声明为const，这个返回值不能被修改。降低了因为用户的错误而造成的意外。</p>
</li>
<li><p>函数参数为const</p>
<p> 除非你要改动局部变量（函数参数），否则将参数声明为const。</p>
</li>
</ol>
<h1 id="4-作用与类成员函数"><a href="#4-作用与类成员函数" class="headerlink" title="4. 作用与类成员函数"></a>4. 作用与类成员函数</h1><p>即const位于成员函数()和{}之间，如：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">foo</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">area</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">        length_ = x;</span><br><span class="line">        width_ = y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// length和width，不能被这个函数修改。</span></span><br><span class="line">    <span class="comment">// 限制对象成员函数 改变对象数据成员，保护了程序的健壮性和稳定性</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="built_in">cout</span>&lt;&lt;<span class="string">"area="</span>&lt;&lt;length_ * width_&lt;&lt;<span class="built_in">endl</span>;&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> length_;</span><br><span class="line">    <span class="keyword">int</span> width_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>print()成员函数，为const函数，表示这个成员函数不能修改成员变量。所以不能将area()成员函数设为const，因为这个函数的 目的就是为成员变量赋值。</p>
<p><font color="red">const成员函数，和非const成员函数使得我们知道一个类中哪些函数可以修改成员属性，哪些不可以</font>。</p>
<p>tip:<br>提升c++程序效率的一个方法是以<code>pass by referrence-to-const</code>的方式或<code>pass by pointer-to-const</code>传递对象：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 此为以成员函数</span></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">int</span> <span class="title">opt</span><span class="params">( <span class="keyword">const</span> foo&amp; x)</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> x.length_ * x.width_&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/09/Cpp-pro-tip-2/" data-id="ckatsrgry002mxqfz2hz125w4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-如何使用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/07/caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/" class="article-date">
  <time datetime="2020-03-07T15:06:20.000Z" itemprop="datePublished">2020-03-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/07/caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/">caffe-使用已有的lenet模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>关于如何使用，在网上找了半天，没有一篇博客说清楚。之后才发现，下载好的caffe的examples目录中就是我想找的。通过examples，caffe的工作流程基本明了。一般由4步：</p>
<ol>
<li>数据准备： 将原始数据转化为caffe-format</li>
<li>定义网络结构</li>
<li>配置solver</li>
<li>训练</li>
</ol>
<p>给个例子mnist</p>
<h1 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1.数据准备"></a>1.数据准备</h1><p>回到caffe的根目录后执行下面脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$.&#x2F;data&#x2F;mnist&#x2F;get_mnist.sh</span><br><span class="line">$.&#x2F;examples&#x2F;mnist&#x2F;create_mnist.sh</span><br></pre></td></tr></table></figure>

<p>先下载解压mnist数据集，后将源数据通过<code>build/examples/mnist/convert_mnist_data.bin</code>写入将原始mnist数据写成<code>LMDB</code>格式，所以会有两个文件生成：<code>mnist_train_lmdb</code>和<code>mnist_test_lmdb</code>。</p>
<h1 id="2-定义模型结构"><a href="#2-定义模型结构" class="headerlink" title="2.定义模型结构"></a>2.定义模型结构</h1><p>我使用caffe为我们定义好的<code>LeNet</code>网络，它的定义在文件<code>examples/mnist/lenet_train_test.prototxt</code>中。定义的格式是Google Protobuff，解析<code>.prorotxt</code>文件的　同意规则由文件<code>/src/caffe/proto/caffe.proto.</code>提供。</p>
<p>打开文件<code>lenet_train_test.prototxt</code>，是lenet的网络结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">name: &quot;LeNet&quot;</span><br><span class="line">layer &#123;</span><br><span class="line">&#x2F;&#x2F; ...layer definition...</span><br><span class="line">include: &#123; phase: TRAIN &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>若干层（Layer）堆叠在一起，构成了一个网络（Net）。lenet网络每层定义的细节间附录。</li>
<li>这个文件被称作<code>network definition protobuf file</code>。其中<code>top</code>表示这层输出，<code>bottom</code>表示这层出入。caffe中模型的逻辑图结构与其<code>.prototxt</code>文件的对应。</li>
<li>网络结构可以通过可视化工具绘制。</li>
</ul>
<h1 id="3-指明模型参数：solver-prototxt"><a href="#3-指明模型参数：solver-prototxt" class="headerlink" title="3.指明模型参数：solver.prototxt"></a>3.指明模型参数：solver.prototxt</h1><p>从这里查看网络参数<code>examples/mnist/lenet_solver.prototxt</code>。在caffe中，设定模型参数（非训练参数，即超参数）被称作<code>solver</code>，这个文件被称作<code>solver protobuf file</code>，也是Google protobuff 文件格式。如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络结构定义</span></span><br><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span></span><br><span class="line"><span class="comment"># test_iter specifies how many forward passes the test should carry out.</span></span><br><span class="line"><span class="comment"># In the case of MNIST, we have test batch size 100 and 100 test iterations,</span></span><br><span class="line"><span class="comment"># covering the full 10,000 testing images.</span></span><br><span class="line">test_iter: 100</span><br><span class="line"><span class="comment"># Carry out testing every 500 training iterations.</span></span><br><span class="line">test_interval: 500</span><br><span class="line"><span class="comment"># The base learning rate, momentum and the weight decay of the network.</span></span><br><span class="line">base_lr: 0.01</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.0005</span><br><span class="line"><span class="comment"># The learning rate policy</span></span><br><span class="line">lr_policy: <span class="string">"inv"</span></span><br><span class="line">gamma: 0.0001</span><br><span class="line">power: 0.75</span><br><span class="line"><span class="comment"># Display every 100 iterations</span></span><br><span class="line">display: 100</span><br><span class="line"><span class="comment"># The maximum number of iterations</span></span><br><span class="line">max_iter: 10000</span><br><span class="line"><span class="comment"># snapshot intermediate results</span></span><br><span class="line">snapshot: 5000</span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet"</span></span><br><span class="line"><span class="comment"># solver mode: CPU or GPU</span></span><br><span class="line">solver_mode: GPU</span><br></pre></td></tr></table></figure>

<p><font color="red">这个文件在程序中按照<code>caffe.proto</code>的协议解析到一个<code>Caffe::SolverParameter</code>对象中，进而将解析后的参数使用到网络中。用于解析的文件是<code>caffe.pb.h</code>和<code>caffe.pb.cc</code>，其位于<code>build/src/caffe/proto/</code></font>。</p>
<p>默认使用GPU。可以使用同目录的其他的参数配置文件（不同的模型）：</p>
<p><code>lenet_multistep_solver.prototxt</code><br><code>lenet_solver_adam.prototxt</code><br><code>lenet_solver_rmsprop.prototxt</code></p>
<h1 id="4-训练与测试"><a href="#4-训练与测试" class="headerlink" title="4.训练与测试"></a>4.训练与测试</h1><p>有了caffe-format的数据；有了网络结构；有了确定的超参数，就可以执行下面脚本开始训练：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;examples&#x2F;mnist&#x2F;train_lenet.sh</span><br></pre></td></tr></table></figure>

<p><code>train_lenet.sh</code>文件包含一行代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env sh</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"> </span><br><span class="line">./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt <span class="variable">$@</span></span><br></pre></td></tr></table></figure>
<p>使用<code>caffe</code>命令（这个<code>caffe</code>是tools中的命令，这是caffe提供的命令行接口：<code>cmdcaffe</code>）， 指明是<code>trian</code>，指明参数配置，即使用哪个<code>.prototxt</code>文件。</p>
<p>执行包括两部分：</p>
<ul>
<li><p>输出模型结构</p>
<p>  包括每一层输入的大小，方便debug。</p>
</li>
<li><p>训练过程</p>
</li>
</ul>
<p>训练结束后，在<code>examples/mnist/</code>路径下得到两个文件<code>lenet_iter_10000.caffemodel</code>和<code>lenet_iter_10000.solverstate</code>。</p>
<ol>
<li><code>.caffemodel</code>文件是最终模型，二进制文件。</li>
<li><code>.solverstate</code>是保存的训练状态（snapshot），可以从此继续开始训练。</li>
</ol>
<h1 id="训练结束"><a href="#训练结束" class="headerlink" title="训练结束"></a>训练结束</h1><p><code>lenet_iter_10000</code>是binary protobuf 文件，就是训练10000次的最终模型，用于对真实样本的推理。</p>
<p>MNIST数据集是小数据集，所以使用GPU的效果并不好，原因是GPU的计算核心与存储的通讯开销。所以对于复杂模型和数据集GPU的速度提升是显著的。</p>
<h1 id="使用预训练的model"><a href="#使用预训练的model" class="headerlink" title="使用预训练的model"></a>使用预训练的model</h1><p>训练结束后生成的<code>.caffemodel</code>是可以直接拿来使用的模型（其实就是所有训练好的参数），用作推理。如何使用？</p>
<p>如果要使用预训练的模型，从这里下载预训练的模型<a href="http://dl.caffe.berkeleyvision.org/" target="_blank" rel="noopener">caffe model index</a>，如<code>bvlc_reference_caffenet.caffemodel</code>， 并且将其放到<code>caffe-root/models/bvlc_reference_caffenet/</code>中。</p>
<p>使用预训练模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指明 -weights 关键字，提供预训练模型</span></span><br><span class="line">./build/tools/caffe train -solver examples/finetuning_on_flickr_style/solver.prototxt -weights models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>生成制定格式数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$.&#x2F;data&#x2F;mnist&#x2F;get_mnist.sh</span><br><span class="line">$.&#x2F;examples&#x2F;mnist&#x2F;create_mnist.sh</span><br></pre></td></tr></table></figure>

<p>在GPU上训练：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt -gpu 0</span><br></pre></td></tr></table></figure>

<p>从5000次接着训练：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/tools/caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate</span><br></pre></td></tr></table></figure>

<h1 id="附录-lenet模型定义"><a href="#附录-lenet模型定义" class="headerlink" title="附录 lenet模型定义"></a>附录 lenet模型定义</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"LeNet"</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.00390625</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="built_in">source</span>: <span class="string">"examples/mnist/mnist_train_lmdb"</span></span><br><span class="line">    batch_size: 64</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: 0.00390625</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="built_in">source</span>: <span class="string">"examples/mnist/mnist_test_lmdb"</span></span><br><span class="line">    batch_size: 100</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"conv1"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Convolution"</span></span><br><span class="line">  bottom: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"conv1"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 20</span><br><span class="line">    kernel_size: 5</span><br><span class="line">    stride: 1</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"pool1"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Pooling"</span></span><br><span class="line">  bottom: <span class="string">"conv1"</span></span><br><span class="line">  top: <span class="string">"pool1"</span></span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX</span><br><span class="line">    kernel_size: 2</span><br><span class="line">    stride: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"conv2"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Convolution"</span></span><br><span class="line">  bottom: <span class="string">"pool1"</span></span><br><span class="line">  top: <span class="string">"conv2"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 50</span><br><span class="line">    kernel_size: 5</span><br><span class="line">    stride: 1</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"pool2"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Pooling"</span></span><br><span class="line">  bottom: <span class="string">"conv2"</span></span><br><span class="line">  top: <span class="string">"pool2"</span></span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX</span><br><span class="line">    kernel_size: 2</span><br><span class="line">    stride: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip1"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"pool2"</span></span><br><span class="line">  top: <span class="string">"ip1"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 500</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"relu1"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"ReLU"</span></span><br><span class="line">  bottom: <span class="string">"ip1"</span></span><br><span class="line">  top: <span class="string">"ip1"</span></span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip2"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"ip1"</span></span><br><span class="line">  top: <span class="string">"ip2"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 2</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 10</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"accuracy"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Accuracy"</span></span><br><span class="line">  bottom: <span class="string">"ip2"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"accuracy"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip2"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/07/caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/" data-id="ckatsrgsk0046xqfzdag77562" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Caffe-如何使用-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/" class="article-date">
  <time datetime="2020-03-07T14:30:38.000Z" itemprop="datePublished">2020-03-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/">Caffe-Blob Layer Net</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Layer和Net"><a href="#Layer和Net" class="headerlink" title="Layer和Net"></a>Layer和Net</h2><p>caffe中的每个层对象，都是一个模型的基本计算单元。一个层对象包括 “…filters, pool, take inner products, apply nonlinearities like rectified-linear and sigmoid and other elementwise transformations, normalize, load data, and compute losses like softmax and hinge”。见<a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html" target="_blank" rel="noopener">官方文档</a></p>
<p>几乎所有的层类都会继承自<code>Layer</code>类：<code>class caffe::Layer&lt; Dtype &gt;</code>这些层类必须实现一个<code>Forward()</code>, 它接受一个<code>input Blob（bottom）</code>计算输出的<code>Blob（top）</code>。还要实现一个<code>Backward()</code>，它使用给定其输出<code>Blob</code>的误差梯度，计算相对于其输入<code>Blob</code>的误差梯度。（实现反向传播）</p>
<p>每个层对象一定包含三个基本方法：setup()，forward()，backward()。</p>
<ol>
<li>setup()：一些计算前的操作，用来初始化层。</li>
<li>forward()：前向计算。多个Layer构成一个Net，一个Net中连续的前向传播由<code>Net::Forward()</code>实现。</li>
<li>backward()：官方文档解释的很好：“given the gradient w.r.t. the top output, compute the gradient w.r.t. to the input and send to the bottom. A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.”。其中<code>w.r.t.</code>表示“with respect to”，中文表示“对…（求梯度等操作）”。这就是反向对参数求梯度的过程。当多个层链接到一起，就会形成反向传播的链条（对应了链式法则）。这个链式法则在<code>Net::Backward()</code>中实现</li>
</ol>
<p>前向计算和后向计算都有CPU和GPU版本的实现。实现自己的层也是不难的，只需要定义好上述三个关键方法。</p>
<p><a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="noopener">这里</a>是管方文档中所有的层。<br><br><a href="http://caffe.berkeleyvision.org/tutorial/forward_backward.html" target="_blank" rel="noopener">这里</a>是caffe中的前向传播和后向传播的官方描述。</p>
<p>一个Net由多个Layer构成，如下面一个计算图（有向无环图）：</p>
<div align="center"><img src="/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/logreg.jpg" width="350"></div>

<p>其中蓝色矩形表示layer，里边的小写表示层的name，大写表示层的type；黄色多边形表示在图中游走的数据<code>Blob</code>，数据移动方向从下向上，下为<code>bottom</code>，上为<code>top</code>。这个图对应的<code>.prototxt</code>如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"LogReg"</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span>   <span class="comment">#输出</span></span><br><span class="line">  top: <span class="string">"label"</span>  <span class="comment">#输出</span></span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="built_in">source</span>: <span class="string">"input_leveldb"</span></span><br><span class="line">    batch_size: 64</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"data"</span>  <span class="comment">#输入</span></span><br><span class="line">  top: <span class="string">"ip"</span>       <span class="comment">#输出</span></span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip"</span>     <span class="comment">#输入</span></span><br><span class="line">  bottom: <span class="string">"label"</span>   <span class="comment">#输入</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个Net的初始化通过执行<code>Net::Init()</code>，其作用是生成Blob和Layers，并调用Layer的setup函数。终端的输出信息表示caffe还有记录的工作被执行。<br><br>用<code>Caffe::mode()</code>和<code>Caffe::set_mode()</code>指定使用CPU或GPU执行.CPU和GPU的切换是无缝的，与模型的定义是无关的。<br><br>模型的定义在机器中是以<code>protocol buffer schema (prototxt)</code>形式存在的。对应训练好的模型是以<code>binary protocol buffer (binaryproto)</code>存在与磁盘的，就是相应路径中的<code>.caffemodel</code>文件。<br><br>模型在caffe中的格式<code>。prototxt</code>由<code>caffe.proto</code>解析。细节见<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto" target="_blank" rel="noopener">这里</a>本地路径：<code>caffe-ROOT/src/caffe/proto/caffe.proto</code>。所使用的技术是<a href="https://code.google.com/p/protobuf/" target="_blank" rel="noopener">Google Protocol Buffer</a>通过科学上网查看。</p>
<p>为什么要使用<code>protocal buffer</code>来格式化模型？</p>
<h1 id="Blob"><a href="#Blob" class="headerlink" title="Blob"></a>Blob</h1><p>Blob为模型提供数据和数据载体，在模型的正向反向传播中移动。它还提供了CPU和GPU间的同步机制。Blob中的数据可以是一批图像，模型参数，中间计算结果。在Blob中不同类型的数据，其大小是不同的。</p>
<ol>
<li><p>图像数据Blob，其中的数据是4维的：<code>（N, K, H, W）</code>分别表示batch size，channel数量，长和宽。当一个Blob对象中的数据变化时，变化优先级从右向左。所以索引为<code>（n, k, h, w）</code>的数值在物理存储中的索引是<code>((n×K + k)×H + h)×W + w</code>。</p>
</li>
<li><p>对于非图像的数据，使用2D Blob，（N, D），此时通常与<code>InnerProductLayer</code>一同使用。</p>
</li>
<li><p>对于参数Blob，其维度要具体问题具体分析了。比如对于conv层，由64个conv kernel，一个kernel的大小是11×11，输入通道数为3，那么此情况的参数Blob大小为（96,3,11,11）。而对于全连接层或上述的<code>InnerProductLayer</code>，如果输出1000维，输入1024维，那么此情况的参数Blob大小为（1000,1024）。</p>
</li>
</ol>
<p>Blob数据一旦定以好后，caffe的模块化就可以做剩下的工作了。</p>
<h2 id="Blob细节"><a href="#Blob细节" class="headerlink" title="Blob细节"></a>Blob细节</h2><p>通常我们关心Blob中的数值（一般的数据，比如一批输入图像）和梯度值。所以一个Blob由两块儿存储：<code>data</code>和<code>diff</code>。</p>
<p>更进一步，Blob中的数值有两种访问的方式，cont和mutable，前者数值不变，后者数值可改变。比如</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">const</span> Dtype* <span class="title">cpu_data</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function">Dtype* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<p>同样的，对于<code>cpu_diff</code>和<code>gpu_data</code>, <code>gpu_diff</code> 也有两种访问方式。</p>
<h2 id="Blob为什么要如此设计？"><a href="#Blob为什么要如此设计？" class="headerlink" title="Blob为什么要如此设计？"></a>Blob为什么要如此设计？</h2><p>官方文档给出如下解释：</p>
<ol>
<li><p>Blob使用<code>SyncedMem</code>类实现CPU与GPU间的数据拷贝，目的是隐藏延时。</p>
</li>
<li><p>如果不想更改数值，那么始终使用const方式调用，而且永远不要将指针存储在自己的对象中。每次处理Blob时，都要调用函数来获取指针，因为<code>SyncedMem</code>类需要这个函数来确定何时复制数据。</p>
</li>
<li><p>在实际中，调用设备内核来执行GPU计算的同时，CPU将数据从磁盘加载到Blob，并将Blob转移到下一层，这是GPU和CPU延时隐藏的基本技巧。因为大部分层都有GPU实现，所以所有的中间数据和梯度都将保留在GPU的内存中。</p>
</li>
</ol>
<p>其他原因如，Blob与其他深度学习框架的数据结构相对应，tensorflow中的tensor，cuda-convnet中的NVMatrix，等，这使得框架之间的转换更容易。</p>
<h2 id="Blob中数据在CPU和GPU之间拷贝行为"><a href="#Blob中数据在CPU和GPU之间拷贝行为" class="headerlink" title="Blob中数据在CPU和GPU之间拷贝行为"></a>Blob中数据在CPU和GPU之间拷贝行为</h2><p>假设数据在CPU（Host）上被初始化，存在于一个Blob中。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个const访问指针，一个mutable访问指针</span></span><br><span class="line"><span class="keyword">const</span> Dtype* foo;   </span><br><span class="line">Dtype* bar;</span><br><span class="line"><span class="comment">// GPU上没有数据数据</span></span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line"><span class="comment">// 上下两句间没有任何操作，所以下一句并不会发生数据拷贝</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// no data copied since both have up-to-date contents.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// 一样，没有数据拷贝.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 对数据进行操作...</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// 当前位于GPU上，没有数据拷贝到gpu.   ？？？？？？</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// 因为数据被操作，所以数据拷贝从GPU到CPU发生。</span></span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// 因为GPU和CPU都是最新数据，所以没有数据拷贝发生。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ？？？？？？</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// still no data copied.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// data copied gpu-&gt;cpu.</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/07/Caffe-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-2/" data-id="ckatxvx570000lzfzc08zem2b" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-caffe-安装及trouble-shooting" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/06/caffe-%E5%AE%89%E8%A3%85%E5%8F%8Atrouble-shooting/" class="article-date">
  <time datetime="2020-03-06T15:15:29.000Z" itemprop="datePublished">2020-03-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Caffe/">Caffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/06/caffe-%E5%AE%89%E8%A3%85%E5%8F%8Atrouble-shooting/">caffe 安装及trouble shooting</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-安装依赖库"><a href="#1-安装依赖库" class="headerlink" title="1. 安装依赖库"></a>1. 安装依赖库</h1><p>执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler</span><br><span class="line"><span class="variable">$sudo</span> apt-get install --no-install-recommends libboost-all-dev</span><br><span class="line"><span class="variable">$sudo</span> apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</span><br><span class="line"><span class="variable">$sudo</span> apt-get install libatlas-base-dev</span><br></pre></td></tr></table></figure>

<p>安装第一个就出问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libprotobuf-dev</span><br></pre></td></tr></table></figure>

<p>返回错误：</p>
<p>由于解决问题后，将terminal的输出被clear掉了，所以用以下相似的问题来说明解决过程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The following packages have unmet dependencies:</span><br><span class="line"> libxml2-dev : Depends: libxml2 (&#x3D; 2.7.8.dfsg-5.1ubuntu4) but 2.7.8.dfsg-5.1ubuntu4.6 is to be installed</span><br></pre></td></tr></table></figure>

<p>解决方法：</p>
<ol>
<li>安装<code>aptitude</code>:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$sudo apt-get install aptitude</span><br><span class="line">$aptitude why-not libxml2</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>找到已经安装的对应的<code>libxml2</code>包：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$dpkg -l | grep libxml2</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>删除<code>libxml2</code>，并删除其他所有依赖包，<code>--force-all</code>参数不能少：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$sudo dpkg --purge --force-all libxml2</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>更正错误：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$sudo apt-get -f install</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>安装欠缺的包</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$sudo apt-get install libxml2-dev</span><br></pre></td></tr></table></figure>

<p>遇到相同的问题，将<code>libxml2-dev</code>和<code>libxml2</code>，替换成自己问题中的对应库。</p>
<h1 id="2-安装NVIDIA驱动和CUDA，openCV"><a href="#2-安装NVIDIA驱动和CUDA，openCV" class="headerlink" title="2. 安装NVIDIA驱动和CUDA，openCV"></a>2. 安装NVIDIA驱动和CUDA，openCV</h1><p>早已安装完成</p>
<h1 id="3-下载caffe源码"><a href="#3-下载caffe源码" class="headerlink" title="3. 下载caffe源码"></a>3. 下载caffe源码</h1><p>从<a href="https://github.com/BVLC/caffe" target="_blank" rel="noopener">这里</a>下载到<code>home/XXX</code>。</p>
<p>修改<code>Makefile.config</code>文件。先复制一份：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp Makefile.config.example Makefile.config</span><br></pre></td></tr></table></figure>

<p>后修改配置文件<code>Makefile.config</code>，下面是我的修改：</p>
<ol>
<li><p>hdf5</p>
<p> 将hdf5的路径添加到<code>INCLUDE_DIRS</code>和<code>LIBRARY_DIRS</code>之后：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INCLUDE_DIRS :&#x3D; $(PYTHON_INCLUDE) &#x2F;usr&#x2F;local&#x2F;include &#x2F;usr&#x2F;include&#x2F;hdf5&#x2F;serial</span><br><span class="line">LIBRARY_DIRS :&#x3D; $(PYTHON_LIB) &#x2F;usr&#x2F;local&#x2F;lib &#x2F;usr&#x2F;lib &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;hdf5&#x2F;serial</span><br></pre></td></tr></table></figure></li>
<li><p>sm_version</p>
<p> 由于我的机器的CUDA版本是9.0，所以要将下面的<code>sm_20</code>, <code>sm_21</code>注释掉，低版本的一些指令对于高版本的不适用。如下：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CUDA_ARCH :&#x3D; -gencode arch&#x3D;compute_30,code&#x3D;sm_30 \</span><br><span class="line">     -gencode arch&#x3D;compute_35,code&#x3D;sm_35 \</span><br><span class="line">     -gencode arch&#x3D;compute_50,code&#x3D;sm_50 \</span><br><span class="line">     -gencode arch&#x3D;compute_52,code&#x3D;sm_52 \</span><br><span class="line">     -gencode arch&#x3D;compute_60,code&#x3D;sm_60 \</span><br><span class="line">     -gencode arch&#x3D;compute_61,code&#x3D;sm_61 \</span><br><span class="line">     -gencode arch&#x3D;compute_61,code&#x3D;compute_61 \</span><br><span class="line"> #   -gencode arch&#x3D;compute_20,code&#x3D;sm_20 \</span><br><span class="line"> #   -gencode arch&#x3D;compute_20,code&#x3D;sm_21 \</span><br></pre></td></tr></table></figure>
</li>
<li><p>OpenCV版本</p>
<p> 去掉下面的注释，表示使用openCV 版本3：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OPENCV_VERSION :&#x3D; 3</span><br></pre></td></tr></table></figure>
</li>
<li><p>CUDA路径</p>
<p> 检查机器CUDA路径是否正确：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_DIR :&#x3D; &#x2F;usr&#x2F;local&#x2F;cuda</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="4-编译测试"><a href="#4-编译测试" class="headerlink" title="4. 编译测试"></a>4. 编译测试</h1><p>配置完成之后编译，<code>-j4</code>表示用4个核心执行任务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$sudo make all -j4 </span><br><span class="line">$sudo make test -j4</span><br><span class="line">$sudo make runtest -j4</span><br></pre></td></tr></table></figure>

<p>期待不出错。</p>
<p>如果有错，比如编译期间又修改了<code>Makefile.config</code>文件，返回类似错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undefined reference to &#96;cv::imread(cv::String const&amp;, int)</span><br></pre></td></tr></table></figure>

<p>可是OpenCV已经修改过了，此时的处理情况：</p>
<p>将编译了一半的<code>build</code>文件夹删除，后重新编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$sudo rm -rf .&#x2F;build&#x2F;*</span><br><span class="line">$sudo make all -j4</span><br></pre></td></tr></table></figure>

<p>其中<code>build/</code>文件夹是编译后得到的，</p>
<p>最终期望的是<code>sudo make runtest -j4</code>运行无误，如果terminal最后返回：</p>
<div align="center"><img src="/2020/03/06/caffe-%E5%AE%89%E8%A3%85%E5%8F%8Atrouble-shooting/result.png"></div>

<p>表示caffe 安装配置成功完成（如果要使用python接口，还要其他操作，我使用CXX）。</p>
<h1 id="5-编译python接口"><a href="#5-编译python接口" class="headerlink" title="5. 编译python接口"></a>5. 编译python接口</h1><p>caffe使用python2.</p>
<ol>
<li><p>第O步，使用linux自带python2，安装依赖库：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python-pip</span><br><span class="line">sudo apt-get install python-numpy</span><br></pre></td></tr></table></figure>

<p> 在caffe根目录的python文件夹下，有一个<code>requirements.txt</code>的清单文件，上面列出了需要的依赖库，需要按照这个清单安装：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gfortran</span><br><span class="line"><span class="built_in">cd</span> ./python</span><br><span class="line">sudo cat python/requirements.txt|xargs -L 1 sudo pip install -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<p> 检查</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install -r python/requirements.txt</span><br></pre></td></tr></table></figure>
<p> 编译成功的会显示<code>Requirement already satisfied</code>，没有编译成功的会继续安装。</p>
</li>
<li><p>第一步，执行命令：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ make pycaffe -j4</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步，在<code>~/.bashrc</code>或<code>/etc/profile</code>中添加caffe中的python 路径：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> PYTHONPATH=/home/junhui/caffe-master/python:$PYTHONPATH</span><br></pre></td></tr></table></figure></li>
<li><p>第三步，source使之生效：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<p> 现在在python脚本中就可以<code>import caffe</code>了。</p>
</li>
</ol>
<h1 id="可能的出错："><a href="#可能的出错：" class="headerlink" title="可能的出错："></a>可能的出错：</h1><ol>
<li><p>numpy/arrayobject.h: missing</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">junhui@gnome:~/caffe$ sudo make pytest –j4</span><br><span class="line">CXX/LD -o python/caffe/_caffe.so python/caffe/_caffe.cpp</span><br><span class="line">python/caffe/_caffe.cpp:10:31: fatal error: numpy/arrayobject.h: No such file or directory</span><br><span class="line">    <span class="comment">#include &lt;numpy/arrayobject.h&gt;</span></span><br><span class="line">                                ^</span><br><span class="line">compilation terminated.</span><br><span class="line">Makefile:517: recipe <span class="keyword">for</span> target <span class="string">'python/caffe/_caffe.so'</span> failed</span><br><span class="line">make: *** [python/caffe/_caffe.so] Error 1</span><br></pre></td></tr></table></figure>
<p> 解决方法：</p>
<p> 在Makefile.config文件中的这个地方：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We need to be able to find Python.h and numpy/arrayobject.h.</span></span><br><span class="line">PYTHON_INCLUDE := /usr/include/python2.7 \</span><br><span class="line">        /usr/lib/python2.7/dist-packages/numpy/core/include</span><br></pre></td></tr></table></figure>
<p> 添加一行：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We need to be able to find Python.h and numpy/arrayobject.h.</span></span><br><span class="line">PYTHON_INCLUDE := /usr/include/python2.7 \</span><br><span class="line">        /usr/lib/python2.7/dist-packages/numpy/core/include \</span><br><span class="line">        /usr/<span class="built_in">local</span>/lib/python2.7/dist-packages/numpy/core/include</span><br></pre></td></tr></table></figure>

<p> 再次编译成功</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ make pycaffe -j4</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python.h: No suchfile or directory</p>
<p> caffe 使用python2，需要在<code>./bashrc</code>中添加python2的路径： </p>
<p> 如果不是使用anaconda2中的python2，则添加如下第一句；如果使用用的anaconda2中的python2，则添加如下第二句</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CPLUS_INCLUDE_PATH=/usr/include/python2.7:<span class="variable">$CPLUS_INCLUDE_PATH</span></span><br><span class="line"><span class="built_in">export</span> CPLUS_INCLUDE_PATH=/home/junhui/anaconda2/include/python2.7/:<span class="variable">$CPLUS_INCLUDE_PATHort</span></span><br></pre></td></tr></table></figure></li>
<li><p>protobuf出错</p>
<p> 使用anaconda时出的错：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CXX .build_release/src/caffe/proto/caffe.pb.cc</span><br><span class="line">In file included from .build_release/src/caffe/proto/caffe.pb.cc:5:0:</span><br><span class="line">.build_release/src/caffe/proto/caffe.pb.h:12:2: error: <span class="comment">#error This file was generated by a newer version of protoc which is</span></span><br><span class="line"><span class="comment">#error This file was generated by a newer version of protoc which is</span></span><br><span class="line">^</span><br><span class="line">.build_release/src/caffe/proto/caffe.pb.h:13:2: error: <span class="comment">#error incompatible with your Protocol Buffer headers. Please update</span></span><br><span class="line"><span class="comment">#error incompatible with your Protocol Buffer headers.  Please update</span></span><br><span class="line">^</span><br><span class="line">.build_release/src/caffe/proto/caffe.pb.h:14:2: error: <span class="comment">#error your headers.</span></span><br><span class="line"><span class="comment">#error your headers.</span></span><br><span class="line">^</span><br><span class="line">In file included from .build_release/src/caffe/proto/caffe.pb.cc:5:0:</span><br><span class="line">.build_release/src/caffe/proto/caffe.pb.h:26:55: fatal error: google/protobuf/generated_enum_reflection.h: No such file or directory</span><br><span class="line"><span class="comment">#include &lt;google/protobuf/generated_enum_reflection.h&gt;</span></span><br><span class="line"></span><br><span class="line">compilation terminated.</span><br><span class="line">make: *** [.build_release/src/caffe/proto/caffe.pb.o] Error 1</span><br></pre></td></tr></table></figure>
<p> 解决方法，将机器上的anaconda3删除，使用linux自带python2，并安装以来库。（其实可以使用anaconda2）.</p>
</li>
<li><p>make clean</p>
<p> 如果<code>make</code>中间出错，改错后，先<code>make clean</code>，后重新<code>make</code></p>
</li>
</ol>
<h1 id="import-caffe-成功后测试pycaffe"><a href="#import-caffe-成功后测试pycaffe" class="headerlink" title="import caffe 成功后测试pycaffe"></a>import caffe 成功后测试pycaffe</h1><p>在mnist数据集上运行lenet</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo ./data/mnist/get_mnist.sh</span><br><span class="line">sudo ./examples/mnist/create_mnist.sh</span><br><span class="line">sudo ./examples/mnist/train_lenet.sh</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/06/caffe-%E5%AE%89%E8%A3%85%E5%8F%8Atrouble-shooting/" data-id="ckatsrgsl0048xqfzc9asf5y7" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Cpp-pro-tip-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/05/Cpp-pro-tip-1/" class="article-date">
  <time datetime="2020-03-05T10:05:50.000Z" itemprop="datePublished">2020-03-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/05/Cpp-pro-tip-1/">Cpp pro tip 1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="从源文件到可执行文件"><a href="#从源文件到可执行文件" class="headerlink" title="从源文件到可执行文件"></a>从源文件到可执行文件</h1><ul>
<li>预处理（pre-processing）E<br>从<code>.c</code>到<code>.i</code>。编译器将C源代码中的包含的头文件如stdio.h编译进来，替换宏。</li>
<li>编译（Compiling）S<br>从<code>.i</code>到<code>.s</code>。gcc首先要检查代码的规范性、是否有语法错误等，以确定代码的实际要做的工作，在检查无误后，gcc把代码翻译成汇编语言。</li>
<li>汇编（Assembling） c<br>把编译阶段生成的<code>.s</code>文件转成二进制目标代码<code>.o</code>文件。</li>
<li>链接 （Linking）<br>链接到库中，生成可执行文件。</li>
</ul>
<p>绘制这里的图(<a href="https://blog.csdn.net/Meteor_s/article/details/85208589" target="_blank" rel="noopener">https://blog.csdn.net/Meteor_s/article/details/85208589</a>)</p>
<p><a href="https://www.cnblogs.com/ericling/articles/11736681.html" target="_blank" rel="noopener">这里</a>更详细</p>
<div align="center"><img src="/2020/03/05/Cpp-pro-tip-1/.png"></div>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gcc -E hello.c -o hello.i</span><br><span class="line">gcc –S hello.i –o hello.s</span><br><span class="line">gcc –c hello.s –o hello.o</span><br><span class="line">gcc hello.o –o hello</span><br></pre></td></tr></table></figure>

<p>生成<code>.o</code>文件，可以重定向，方便其他应用使用。</p>
<p>一条命令从源文件到可执行文件：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc hello.c –o hello</span><br></pre></td></tr></table></figure>

<h1 id="1-虚析构函数"><a href="#1-虚析构函数" class="headerlink" title="1 虚析构函数"></a>1 虚析构函数</h1><p>补充内容（tip 7）：</p>
<p>任何时候都应该为待多态性质的基类（父类）声明<code>virtual</code>析构函数。如果一个class含有任何<code>virual</code>函数，就一定要有一个<code>virtual</code>析构函数。</p>
<p>而如果这个class的设计不是作为基类使用，或者不准备让这个class具备多态性，就不该声明<code>virtual</code>析构函数。</p>
<p>含有多态性质的<code>base class</code>的设计目的是为了通过<code>base class</code>的接口来使用<code>derived class</code>。所以通常使用<code>base class</code>类的指针指向<code>derived class</code>的对象。</p>
<p>并不是所有的<code>base class</code>都是为了多态用途，比如STL容器的设计不是用作<code>base class</code>的。</p>
<p><code>virtual</code>同名函数的目的是允许<code>derived class</code>的实现可以<font color="red">客制化</font>。</p>
<h1 id="2-C-帮你实现了的函数"><a href="#2-C-帮你实现了的函数" class="headerlink" title="2 C++帮你实现了的函数"></a>2 C++帮你实现了的函数</h1><p>（tip 5）</p>
<p>当定义了一个class，又没有实现任何成员函数时，编译器会为这个class声明4个函数：</p>
<ul>
<li>一个default构造函数</li>
<li>一个copy构造函数</li>
<li>一个析构函数</li>
<li>一个copy操作符</li>
</ul>
<p>这些函数都是<code>public</code>和<code>inline</code>的。如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Myclass</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Myclass()&#123;...&#125;</span><br><span class="line">    Myclass(<span class="keyword">const</span> Myclass&amp; rhs)&#123;...&#125;</span><br><span class="line">    ~Myclass()&#123;...&#125;</span><br><span class="line">    Myclass&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Myclass&amp; rhs)&#123;...&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实例化这个class，并使用</span></span><br><span class="line">Myclass e1;       <span class="comment">//default 构造函数</span></span><br><span class="line"><span class="function">Myclass <span class="title">e2</span><span class="params">(e1)</span></span>;   <span class="comment">//copy构造函数</span></span><br><span class="line">e2=e1;            <span class="comment">//copy符号操作符</span></span><br><span class="line">                  <span class="comment">//析构函数</span></span><br></pre></td></tr></table></figure>

<p>其中</p>
<ol>
<li>编译器给出的析构函数是个<code>non-virtual</code>函数，除非这个class的<code>base class</code>含有<code>virtual</code>析构函数。此时这个class的虚拟性来自其<code>base class</code>。</li>
<li>对于构造函数，如果我声明了一个构造函数，那么编译器不再创建<code>default</code>构造函数</li>
<li>copy构造函数，和copy操作符只是单纯的将来源对象的每一个<code>non-static</code>成员变量拷贝到目标对象。所以: <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Myclass <span class="title">e2</span><span class="params">(e1)</span></span></span><br></pre></td></tr></table></figure>
 是使用<code>e1</code>的成员变量来初始化<code>e2</code>的成员变量。</li>
</ol>
<h1 id="3-为classes实现赋值操作符"><a href="#3-为classes实现赋值操作符" class="headerlink" title="3 为classes实现赋值操作符"></a>3 为classes实现赋值操作符</h1><p>使用赋值操作时，通常可以写成如下形式：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> x, y, z;</span><br><span class="line">x=y=z=<span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<p>上述实际上的赋值行为是</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x=(y=(z=<span class="number">2</span>));</span><br></pre></td></tr></table></figure>

<p>其行为是将2赋值给<code>z</code>，再将<code>z</code>赋值给<code>y</code>，最后将<code>y</code>赋值给<code>x</code>。</p>
<p>为了实现上述的连锁赋值，赋值操作符必须返回一个引用指向操作符的左侧：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Myclass</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Myclass&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Myclass&amp; rhs)&#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span>* <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>同样的上述的形式也适用于所有相关的操作符，如<code>+=</code>，<code>-=</code>，<code>×=</code>。</p>
<p>这是一条<font color="red">协议</font>，被所有内置类型和标准库函数共同遵守，所以，遵守它吧。</p>
<h1 id="4-将成员变量声明为private"><a href="#4-将成员变量声明为private" class="headerlink" title="4 将成员变量声明为private"></a>4 将成员变量声明为private</h1><p>为什么要把classes的成员变量声明为<code>private</code>：因为这体现了<font color="red">封装</font>。</p>
<p>类成员变量应该只被这个类的成员方法可见，<code>protected</code>成员变量同<code>public</code>成员变量一样<font color="red">缺乏封装性</font>。</p>
<p>从封装的角度讲，访问权限只有提供封装（private）和不提供封装两种。</p>
<h1 id="5-使用non-member-non-friend函数而非再定义一个member函数"><a href="#5-使用non-member-non-friend函数而非再定义一个member函数" class="headerlink" title="5 使用non-member non-friend函数而非再定义一个member函数"></a>5 使用non-member non-friend函数而非再定义一个member函数</h1><p>假设由一个类含有若干个清理函数，只是清理的对象不同：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WebBrowser</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clearHistory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clearCache</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clearCookies</span><span class="params">()</span></span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>其实很多时候，用户想要一次性执行这些动作，如何一次性完成，两种方案：</p>
<ol>
<li><p>给这个class中添加一个member函数<code>clearAll()</code>，它调用上述三个清理函数：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WebBrowser</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clearAll</span><span class="params">()</span></span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;；</span><br></pre></td></tr></table></figure></li>
<li><p>使用一个non-member，non-friend函数：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">clearBrowser</span><span class="params">(WebBrowser&amp; wb)</span></span>&#123;</span><br><span class="line">    wb.clearHistory();</span><br><span class="line">    wb.clearCache();</span><br><span class="line">    wb.clearCookies();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>pro tip告诉你使用后者。后者更体系那封装性。后者保护了类的包裹性，进而有较低的编译依赖，增强了类的<font color="orange">可延展性</font>。</p>
</li>
</ol>
<p>对于封装性，可以访问private成员变量的只有<font color="orange">member函数</font>和<font color="orange">friend函数</font>。上述两种方式提供了相同的功能，而后者提供了较强的封装性，因为<font color="red">它不能增加访问private变量的能力</font>。</p>
<p>P.S. friend函数和member函数对于类private成员的访问能力相同。</p>
<p>在C++ 中自然的做法是将类<code>WebBrowser</code>和函数<code>clearBrowser()</code>放置于同一个<code>namespace</code>中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> Web&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">WebBrowser</span>&#123;</span>...&#125;;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">clearBrowser</span><span class="params">(WebBrowser&amp; wb)</span></span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>namespace 可以跨越多个源码文件，而class不能。就是说相同的namespace中可以有多个头文件，这正是C++标准库的组织方式：<code>std::vector</code>, <code>std::sort</code>, <code>std::map</code>, … C++ 将不同的部分，不同<code>container</code>放在不同的头文件中，每个声明了<code>std</code>的某部分功能，这样当使用<code>vector</code>，时只用<code>include &lt;vector&gt;</code>，而不必将所有的<code>std</code>内容<code>include</code>进来。就是说，用户只用对所用的部分进行编译。而class必须整体定义，不能分割。</p>
<p>想要扩充这个namespace的能力，只需要在这个namespace中添加更多的non-member，non-friend函数。即增强了功能，又没有破坏封装性，由没有增加编译依赖。</p>
<p><font color="orange" size="4">敲黑板</font>两个角度：</p>
<ul>
<li>增强封装性</li>
<li>减小编译依赖</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/05/Cpp-pro-tip-1/" data-id="ckatsrgrz002oxqfz5ixp6ux4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">43</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 15px;">CUDA</a> <a href="/tags/Test-Analysis/" style="font-size: 10px;">Test Analysis</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/06/caffe-SyncedMemory/">caffe-SyncedMemory</a>
          </li>
        
          <li>
            <a href="/2020/06/06/caffe-Blob-2/">caffe-Blob-(2)</a>
          </li>
        
          <li>
            <a href="/2020/06/04/caffe-Blob-1/">caffe-Blob-(1)</a>
          </li>
        
          <li>
            <a href="/2020/06/04/caffe-%E6%89%80%E4%BD%BF%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/">caffe-所使用的数据格式</a>
          </li>
        
          <li>
            <a href="/2020/06/03/caffe-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8Epython%E6%8E%A5%E5%8F%A3/">caffe 命令行与python接口</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>