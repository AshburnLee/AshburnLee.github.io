<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Junhui&#39;s Journal">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junhui">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-CUDA-Grid-stride-Loop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/26/CUDA-Grid-stride-Loop/" class="article-date">
  <time datetime="2019-11-26T15:29:29.000Z" itemprop="datePublished">2019-11-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/26/CUDA-Grid-stride-Loop/">CUDA-Grid stride Loop</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Grid-stride-loop"><a href="#Grid-stride-loop" class="headerlink" title="Grid-stride loop"></a>Grid-stride loop</h2><p>Grid-stride loop 长这个样子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">float</span> *x, <span class="keyword">float</span> *y)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = index; i &lt; n; i += stride)</span><br><span class="line">    y[i] = x[i] + y[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当有足够的线程可以覆盖所有需要处理的数据时，一个线程处理一个数据。线程ID不需要更新，一次并行执行结束，如下:<br>Common CUDA guidance is to launch one thread per data element, which means to parallelize the above SAXPY loop we write a kernel that assumes we have enough threads to more than cover the array size:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">saxpy</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">float</span> a, <span class="keyword">float</span> *x, <span class="keyword">float</span> *y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; n) </span><br><span class="line">        y[i] = a * x[i] + y[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>一个grid可以覆盖所有数据，这种方式的kernel被称作<em>monolithic kernel</em>. 如下kernel可以一次处理1M的数据量:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Perform SAXPY on 1M elements</span></span><br><span class="line">saxpy&lt;&lt;&lt;<span class="number">4096</span>,<span class="number">256</span>&gt;&gt;&gt;(<span class="number">1</span>&lt;&lt;<span class="number">20</span>, <span class="number">2.0</span>, x, y);</span><br></pre></td></tr></table></figure>
<p>但是，当数据量很大时，超过可用的线程数，那么所有线程由不能只干一次活了，所有线程做完一批后更新ID接着做下一批。这种方式被称作<em>grid-stride loop</em>，如下边的kernel:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">saxpy</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">float</span> a, <span class="keyword">float</span> *x, <span class="keyword">float</span> *y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">         i &lt; n; </span><br><span class="line">         i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">          y[i] = a * x[i] + y[i];</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自然地，更新ID的方式就是，让ID加上grid的大小，即所有线程个数。一个grid 的所有线程个数就是<code>blockDim.x * gridDim.x</code>。<br>这个值可以称作为ID更新步长。加入我有1280个线程，那么线程0 将会处理元素0,1280,2560，…。这样做的好处是，保证了相邻的线程处理相邻的数据，这是效率最高的执行方式。如本文所讲“we ensure that all addressing within warps is unit-stride, so we get maximum memory coalescing, just as in the monolithic version.”</p>
<p>总结下<em>grid-stride loop</em>的优势:</p>
<p>1) <strong>Scalability and thread reuse</strong>. </p>
<p>保证可以处理任何量的数据，一批一批地串行就可以啦，没办法，可用线程数有限，这可以保证所有数据正确被处理。另一方面，可以限制block的数量，做微调，尝试提升性能。<br>“By using a loop, you can support <span style="color:red">any problem size</span>. even if it exceeds the largest grid size your CUDA device supports. Moreover, you can <span style="color:red">limit the number of blocks</span> you use to tune performance. For example, it’s often useful to launch a number of blocks <span style="color:red">that is a multiple of the number of multiprocessors on the device</span>, to balance utilization. As an example, we might launch the loop version of the kernel like this:”</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> numSMs;</span><br><span class="line">cudaDeviceGetAttribute(&amp;numSMs, cudaDevAttrMultiProcessorCount, devId);</span><br><span class="line"><span class="comment">// Perform SAXPY on 1M elements</span></span><br><span class="line">saxpy&lt;&lt;&lt;<span class="number">32</span>*numSMs, <span class="number">256</span>&gt;&gt;&gt;(<span class="number">1</span> &lt;&lt; <span class="number">20</span>, <span class="number">2.0</span>, x, y);</span><br></pre></td></tr></table></figure>
<p>When you limit the number of blocks in your grid, threads are reused for multiple computations. Thread reuse amortizes thread creation and destruction cost along with any other processing the kernel might do before or after the loop (such as thread-private or shared data initialization).</p>
<p>2) <strong>Debugging</strong></p>
<p>方便Debug，只是用一个线程，使整个过程变为串行处理，通过使用打印语句，找到错误，便于修改。<br>By using a loop instead of a <em>monolithic kernel</em>, you can easily switch to serial processing by launching one block with one thread. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saxpy&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(<span class="number">1</span>&lt;&lt;<span class="number">20</span>, <span class="number">2.0</span>, x, y);</span><br></pre></td></tr></table></figure>
<p>This makes it easier to emulate a serial host implementation to validate results, and it can make printf debugging easier by <span style="color:red">serializing the print order</span>. Serializing the computation also allows you to eliminate numerical variations caused by changes in the order of operations from run to run, helping you to verify that your numerics are correct before tuning the parallel version.</p>
<p>3) <strong>Portability and readability</strong></p>
<p>The grid-stride loop code is more like the original sequential loop code than the monolithic kernel code, making it clearer for other users. In fact we can pretty easily write a version of the kernel that compiles and runs either as a parallel CUDA kernel on the GPU or as a sequential loop on the CPU. The Hemi library provides a <code>grid_stride_range()</code> helper that makes this trivial using C++11 range-based for loops. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HEMI_LAUNCHABLE</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">saxpy</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">float</span> a, <span class="keyword">float</span> *x, <span class="keyword">float</span> *y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> i : hemi::grid_stride_range(<span class="number">0</span>, n)) &#123;</span><br><span class="line">        y[i] = a * x[i] + y[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>We can launch the kernel using this code, which generates a kernel launch when compiled for CUDA, or a function call when compiled for the CPU. <code>hemi::cudaLaunch(saxpy, 1&lt;&lt;20, 2.0, x, y);</code><br>Grid-stride loops are a great way to make your CUDA kernels flexible, scalable, debuggable, and even portable. </p>
<p>原文内容来自 Mark Harris<br>原文<a href="https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/" target="_blank" rel="noopener">链接</a></p>
<hr>
<p><span style="font-family:Papyrus; font-size:2em;">CUDA</span></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/26/CUDA-Grid-stride-Loop/" data-id="ck71en4is0004hefzgj373qq4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-求三角形顶点坐标-Cross-Production" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/10/CUDA-%E6%B1%82%E4%B8%89%E8%A7%92%E5%BD%A2%E9%A1%B6%E7%82%B9%E5%9D%90%E6%A0%87-Cross-Production/" class="article-date">
  <time datetime="2019-10-10T14:05:01.000Z" itemprop="datePublished">2019-10-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/10/CUDA-%E6%B1%82%E4%B8%89%E8%A7%92%E5%BD%A2%E9%A1%B6%E7%82%B9%E5%9D%90%E6%A0%87-Cross-Production/">CUDA-求三角形顶点坐标-Cross Production</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="三角型，已知三个角度和两个定点坐标，求第三个定点坐标"><a href="#三角型，已知三个角度和两个定点坐标，求第三个定点坐标" class="headerlink" title="三角型，已知三个角度和两个定点坐标，求第三个定点坐标"></a>三角型，已知三个角度和两个定点坐标，求第三个定点坐标</h1><p>一个三角形，已知两个顶点坐标<code>p1</code>和<code>p2</code>，和三个顶角<code>angle1</code>, <code>angle2</code>, <code>angle3</code>，求第三个顶点p3的坐标。方法如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = (p1X * cot(angle2) + p2X * cot(angle1) + p2Y - p1Y) / cot(angle1) + cot(angle2));</span><br><span class="line"></span><br><span class="line">y = (p1Y * cot(angle2) + p2Y * cot(angle1) + p1X - p2X) / (cot(angle1) + cot(angle2));</span><br></pre></td></tr></table></figure>

<p>其中 <code>cot(alpha) = 1/tan(alpha);</code>， 注意，<code>p1</code>，<code>p2</code>，<code>p3</code>逆时针位置关系。</p>
<h1 id="叉乘-Cross-Production"><a href="#叉乘-Cross-Production" class="headerlink" title="叉乘-Cross Production"></a>叉乘-Cross Production</h1><p>叉乘是用于判断点与向量的位置关系。如已知向量起点为 p1，终点为 p2，待判断的点为 pt。那么点与向量的位置关系由以下公式决定：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> tmp = (pt1-&gt;y - pt2-&gt;y)*pt-&gt;x + (pt2-&gt;x - pt1-&gt;x)*pt-&gt;y + pt1-&gt;x*pt2-&gt;y - pt2-&gt;x*pt1-&gt;y;</span><br></pre></td></tr></table></figure>

<p>其中各个点由struct 定义：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Points</span>&#123;</span></span><br><span class="line">	<span class="keyword">int</span> x;</span><br><span class="line">	<span class="keyword">int</span> y;</span><br><span class="line">	<span class="keyword">float</span> value;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>简单的数学完整实现：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Points</span>&#123;</span></span><br><span class="line">	<span class="keyword">int</span> x;</span><br><span class="line">	<span class="keyword">int</span> y;</span><br><span class="line">	<span class="keyword">float</span> value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pointLinePosition</span><span class="params">(struct Points*, struct Points*, struct Points*)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span>&#123;</span><br><span class="line">	<span class="comment">// 已知的向量</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">Points</span> <span class="title">p0</span>, <span class="title">p1</span>;</span></span><br><span class="line">	p0.x = <span class="number">0</span>, p0.y = <span class="number">0</span>;</span><br><span class="line">	p1.x = <span class="number">2</span>, p1.y = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 待判断的点</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">Points</span> <span class="title">px</span>;</span></span><br><span class="line">	px.x = <span class="number">5</span>, px.y = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> res = pointLinePosition(&amp;p0, &amp;p1, &amp;px);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (res == <span class="number">1</span>)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"left on  \n"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">if</span> (res == <span class="number">0</span>)</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"right \n"</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Determine the position bewteen a VECTOR(pt1-&gt;pt2) and a POINT(pt)</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pointLinePosition</span><span class="params">(struct Points* pt1, struct Points* pt2, struct Points* pt)</span></span>&#123;</span><br><span class="line">	<span class="keyword">float</span> tmp = (pt1-&gt;y - pt2-&gt;y)*pt-&gt;x + (pt2-&gt;x - pt1-&gt;x)*pt-&gt;y + pt1-&gt;x*pt2-&gt;y - pt2-&gt;x*pt1-&gt;y;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (tmp &gt; <span class="number">0</span> )&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"tmp: %.5f \n"</span>, tmp);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">if</span>(tmp &lt; <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"tmp: %.5f \n"</span>, tmp);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"tmp: %.5f \n"</span>, tmp);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/10/CUDA-%E6%B1%82%E4%B8%89%E8%A7%92%E5%BD%A2%E9%A1%B6%E7%82%B9%E5%9D%90%E6%A0%87-Cross-Production/" data-id="ck71en4jn001uhefzfcy4g5ir" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-在Device上初始化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/06/CUDA-%E5%9C%A8Device%E4%B8%8A%E5%88%9D%E5%A7%8B%E5%8C%96/" class="article-date">
  <time datetime="2019-10-06T11:45:39.000Z" itemprop="datePublished">2019-10-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/06/CUDA-%E5%9C%A8Device%E4%B8%8A%E5%88%9D%E5%A7%8B%E5%8C%96/">CUDA-在Device上初始化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="在GPU上初始化"><a href="#在GPU上初始化" class="headerlink" title="在GPU上初始化"></a>在GPU上初始化</h1><p>绝大多数CUDA教材，都会提出一般CUDA程序的标准步骤，其中一步是将Host的数据拷贝到Device。也就是说Host中数据要先被初始化，后才能拷贝到Device。其实，可以直接在Device中初始化数据，如此既可以避免Host到Device的数据传输，又可以加快数据初始化的速度。如下是一个小例：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cuda_runtime.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"device_launch_parameters.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10</span></span><br><span class="line"><span class="comment">//#define N 16384 // N*N = 268435456   // 1GB 4Bytes </span></span><br><span class="line"><span class="comment">//#define N 16384*2 // N*N = 268435456*4   // 4GB 4Bytes </span></span><br><span class="line"><span class="comment">//#define N 16384*4 // N*N = 268435456*16   // 16GB 4Bytes </span></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">initOnGPU</span><span class="params">(<span class="keyword">int</span>* dev_a)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> ix = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="keyword">int</span> iy = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">	<span class="keyword">int</span> tid = ix * N + iy;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (ix &lt; N &amp;&amp; iy &lt; N)&#123;</span><br><span class="line">        <span class="comment">// initialize on Device</span></span><br><span class="line">		dev_a[tid] = <span class="number">321</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1) </span></span><br><span class="line">	<span class="keyword">int</span>* h_a;</span><br><span class="line">	h_a = (<span class="keyword">int</span>*)<span class="built_in">malloc</span>(N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span>* dev_a;</span><br><span class="line">	cudaMalloc((<span class="keyword">int</span>**)&amp;dev_a, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2) init on gpu</span></span><br><span class="line">	<span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">2</span>, <span class="number">2</span>)</span></span>;</span><br><span class="line">	<span class="function">dim3 <span class="title">grid</span><span class="params">((N + block.x - <span class="number">1</span>), (N + block.y - <span class="number">1</span>))</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">clock_t</span> start = clock();</span><br><span class="line">	initOnGPU &lt;&lt;&lt; grid, block &gt;&gt;&gt;(dev_a);</span><br><span class="line">	<span class="keyword">clock_t</span> <span class="built_in">end</span> = clock();</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Time init on GPU: %.10f \n"</span>, (<span class="keyword">double</span>)(<span class="built_in">end</span> - start) / CLOCKS_PER_SEC);</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 3) </span></span><br><span class="line">	cudaMemcpy(h_a, dev_a, N*N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 4) show h_a</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N*N; i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>, h_a[i]);</span><br><span class="line">		<span class="keyword">if</span> ((i + <span class="number">1</span>) % N == <span class="number">0</span>)</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5) </span></span><br><span class="line">	<span class="built_in">free</span>(h_a);</span><br><span class="line">	cudaFree(dev_a);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>解释一下：<br>1）分别在Host和Device上开辟空间<code>h_a</code>和<code>dev_a</code>。<br>2）调用kernel，这个kernel的作用是初始化<code>dev_a</code>。注意并不是从Host中拷贝过去的。以及其他需要在GPU上执行的操作。<br>3）GPU上的操作完成后，得到的最终数据保存在<code>dev_a</code>中。拷贝到Host<code>h_a</code>中。<br>4）显示最终计算结果。<br>5）释放资源。</p>
<p>其中kernel函数<code>initOnGPU</code>的作用是初始化<code>dec_a</code>中元素为321。</p>
<p>结果如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Time init on GPU: <span class="number">0.0000160000</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> </span><br><span class="line"><span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span> <span class="number">321</span></span><br></pre></td></tr></table></figure>

<p>所以说，直接在Device上初始化，相对先在Host上初始化后拷贝到Device，是更高效的。<br>CPU与GPU的通信是通过PCIe 实现的。PCIe第三代 的极限速度是 16GB每秒。相比较，费米架构的GPU中，GPU芯片与GPU存储之间的数据交换速度高达144GB 每秒。所以说Host和Device间的数据传输是CUDA应用的 性能瓶颈。</p>
<p><font color="green" size="5">敲黑板</font> CUDA程序的一个基本规则是，尽可能减少host 与device间的数据交换。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/06/CUDA-%E5%9C%A8Device%E4%B8%8A%E5%88%9D%E5%A7%8B%E5%8C%96/" data-id="ck71en4je0016hefz7h6252c9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-二维kernel的全局ID" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/06/CUDA-%E4%BA%8C%E7%BB%B4kernel%E7%9A%84%E5%85%A8%E5%B1%80ID/" class="article-date">
  <time datetime="2019-10-06T08:17:38.000Z" itemprop="datePublished">2019-10-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/06/CUDA-%E4%BA%8C%E7%BB%B4kernel%E7%9A%84%E5%85%A8%E5%B1%80ID/">CUDA-二维kernel的全局ID</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="确定2Dkernel-的thread-全局ID"><a href="#确定2Dkernel-的thread-全局ID" class="headerlink" title="确定2Dkernel 的thread 全局ID"></a>确定2Dkernel 的thread 全局ID</h1><p>假如我configure 了一个kernel：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> row;</span><br><span class="line"><span class="keyword">int</span> col;</span><br><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">12</span>, <span class="number">12</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">((row + block.x - <span class="number">1</span>) / block.x, (col + block.y - <span class="number">1</span>) / block.y)</span></span>;</span><br></pre></td></tr></table></figure>
<p>那么在<code>__globla__</code>中的全局thread ID 用如下方法确定：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">func</span><span class="params">(struct Points* dev_a, </span></span></span><br><span class="line"><span class="function"><span class="params">						struct Points* dev_b, </span></span></span><br><span class="line"><span class="function"><span class="params">						struct Points p1,      <span class="comment">//注意 C是不支持传入参数的引用的</span></span></span></span><br><span class="line"><span class="function"><span class="params">						struct Points p2,</span></span></span><br><span class="line"><span class="function"><span class="params">						<span class="keyword">float</span>* dev_c, </span></span></span><br><span class="line"><span class="function"><span class="params">						<span class="keyword">const</span> <span class="keyword">int</span> row, </span></span></span><br><span class="line"><span class="function"><span class="params">						<span class="keyword">const</span> <span class="keyword">int</span> col)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> ix = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="keyword">int</span> iy = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">	<span class="keyword">int</span> tid = ix * col + iy;    <span class="comment">// 用这个公式来确定全局ID</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (ix &lt; row &amp;&amp; iy &lt; col)&#123;</span><br><span class="line">		dev_b[tid].x = dev_a[tid].x + p1.x;</span><br><span class="line">		dev_b[tid].y = dev_a[tid].y + p1.y;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对每个元素进行所需要的操作，</span></span><br><span class="line">		Line(dev_b[tid], p1, p2);</span><br><span class="line">		getValue(dev_b[tid], dev_c[tid]);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>int tid = ix * col + iy;</code>用x和y两个方向的分量来确定threads的全局ID。</p>
<p>同理，在求矩阵转置时的kernel是如下实现的：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">transpose</span><span class="params">(<span class="keyword">int</span> *m, <span class="keyword">int</span> *mt)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> idx = blockIdx.x*blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="keyword">int</span> idy = blockIdx.y*blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> tidM, tidT;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (idx &lt; N &amp;&amp; idy &lt;N)&#123;</span><br><span class="line">		tidM = idx * N + idy;</span><br><span class="line">		tidT = idy * N + idx;</span><br><span class="line"></span><br><span class="line">		mt[tidT] = m[tidM];   <span class="comment">// copy value from original matrix to transpose matrix</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>tidM = idx * N + idy;</code>为原矩阵的thread ID。<code>tidT = idy * N + idx;</code>是转置后的矩阵thread ID。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/06/CUDA-%E4%BA%8C%E7%BB%B4kernel%E7%9A%84%E5%85%A8%E5%B1%80ID/" data-id="ck71en4j8000rhefzft4kany6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-理解线程ID" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/06/CUDA-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8BID/" class="article-date">
  <time datetime="2019-10-06T02:51:01.000Z" itemprop="datePublished">2019-10-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/06/CUDA-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8BID/">CUDA-理解线程ID</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="理解线程ID"><a href="#理解线程ID" class="headerlink" title="理解线程ID"></a>理解线程ID</h1><p>从线程逻辑结构上讲，所有线程有三层结构：threads，blocks，grids。每一层有三个维度：x，y，z。下面小例子展示了CUDA是怎样给不同的threads编号的：</p>
<p>假如我配置的kernel如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nElem = <span class="number">6</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">((nElem + block.x - <span class="number">1</span>) / block.x)</span></span>;</span><br></pre></td></tr></table></figure>
<p>grid中结果为2，所以在kernel中：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">checkDeviceIndex &lt;&lt;&lt; grid, block &gt;&gt;&gt;();</span><br></pre></td></tr></table></figure>
<p>grid处为2，block处为3，即<code>&lt;&lt;&lt;2, 3&gt;&gt;&gt;</code>. 表示<strong>有2个blocks，每个blocks中有3个threads</strong>。其结构如下图：</p>
<div align="center"><img src="/2019/10/06/CUDA-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8BID/config.png" width="800"></div>

<p>一个蓝色矩形表示一个block，一个曲线箭头表示一个thread，在本例中一个grid由两个blocks 组成。</p>
<p>解释为：<br>对于grid<br>在x方向为2，表示在x方向由2个blocks。<br>y方向为1，表示在y方向上有1个block。<br>z方向为1，表示在z方向有1个block。</p>
<p>对于block<br>在x方向为2，表示在x方向上有3个threads。<br>y方向为1，表示在y方向上有1个thread。<br>在方向为1，表示在z方向上有1个thread。</p>
<p>threads是构成blocks和grids的最小单位，也是执行操作的最小单位。</p>
<p>从执行结上检验上述：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">printf</span>(<span class="string">"grid.x %d grid.y %d grid.z %d \n"</span>, grid.x, grid.y, grid.z);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"block.x %d block.y %d block.z %d \n"</span>, block.x, block.y, block.z);</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grid.x=<span class="number">2</span> grid.y=<span class="number">1</span> grid.z=<span class="number">1</span> </span><br><span class="line">block.x=<span class="number">3</span> block.y=<span class="number">1</span> block.z=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>与上述描述相符。</p>
<p>那么在kernel中是如何编号的呢！设计kernel：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkDeviceIndex</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"threadIdx:(%d, %d, %d)\n"</span>, threadIdx.x, threadIdx.y, threadIdx.z);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"blockIdx:(%d, %d, %d)\n"</span>, blockIdx.x, blockIdx.y, blockIdx.z);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"blockDim:(%d, %d, %d)\n"</span>, blockDim.x, blockDim.y, blockDim.z);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"gridDim:(%d, %d, %d)\n"</span>, gridDim.x, gridDim.y, gridDim.z);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>表示每一个thread都会打印4条信息，共有2*3=6个threads。结果为：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">threadIdx:(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">threadIdx:(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">threadIdx:(<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">threadIdx:(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">threadIdx:(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">threadIdx:(<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockIdx:(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockIdx:(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockIdx:(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockIdx:(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockIdx:(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockIdx:(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">blockDim:(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">blockDim:(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">blockDim:(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">blockDim:(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">blockDim:(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">blockDim:(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">gridDim:(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">gridDim:(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">gridDim:(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">gridDim:(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">gridDim:(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">gridDim:(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>根据threads ID的计算公式：<code>int tid = threadIdx.x + blockIdx.x * blockDim.x</code><br>可以得到6个threads的ID分别是：<br>0 + 0 × 3 = 0，<br>1 + 0 × 3 = 1，<br>2 + 0 × 3 = 2，<br>0 + 1 × 3 = 3，<br>1 + 1 × 3 = 4，<br>2 + 1 × 3 = 5，</p>
<p>可以看出，CUDA kernel是根据公式给每一个threads编号的，保证每个threads有唯一的ID。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/06/CUDA-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8BID/" data-id="ck71en4jo001xhefz5ogn7eu5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-几点要记住-更新ID" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/04/CUDA-%E5%87%A0%E7%82%B9%E8%A6%81%E8%AE%B0%E4%BD%8F-%E6%9B%B4%E6%96%B0ID/" class="article-date">
  <time datetime="2019-10-04T13:14:31.000Z" itemprop="datePublished">2019-10-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/04/CUDA-%E5%87%A0%E7%82%B9%E8%A6%81%E8%AE%B0%E4%BD%8F-%E6%9B%B4%E6%96%B0ID/">CUDA-几点要记住-更新ID</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Bare-in-mind"><a href="#Bare-in-mind" class="headerlink" title="Bare in mind:"></a>Bare in mind:</h1><ul>
<li>不管你的数据是一维的二维的还是更高维度的，在GPU端，高维被扁平化，都将被看成一维的，所以么有必要在Device上开辟，比如，一个二维数组。</li>
<li>CUDA code 需要你并行地思考：<code>Think parallel</code>. </li>
<li>当你在写CUDA code， 实际上你是在为一个thread 写串行code，而每一个thread都执行这个段相同的串行code。看下图体会。</li>
<li>可以这样理解，对于简单问题，把CPU code的for 循环去掉，其实就得到了GPU code。每个thread 有自己唯一的ID，其他都一样。<div align="center"><img src="/2019/10/04/CUDA-%E5%87%A0%E7%82%B9%E8%A6%81%E8%AE%B0%E4%BD%8F-%E6%9B%B4%E6%96%B0ID/parallel.png" width="800"></div>

</li>
</ul>
<h1 id="配置kernel"><a href="#配置kernel" class="headerlink" title="配置kernel"></a>配置kernel</h1><ul>
<li><p>当GPU可用的thread非常多，而当前所需解决的任务规模并不大时，可以一次invoke 足量的threads，这样便不用更新threads ID。如下：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 1&lt;&lt;7</span></span><br><span class="line">...</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">grid</span> <span class="params">((N + block.x - <span class="number">1</span>) / block.x )</span></span>;</span><br><span class="line"></span><br><span class="line">	kernal_func &lt;&lt;&lt;grid, block &gt;&gt;&gt;(d_c, d_a, d_b);</span><br></pre></td></tr></table></figure>

<p>  或者二维的configuration：</p>
  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 1&lt;&lt;7</span></span><br><span class="line">...</span><br><span class="line">	<span class="function">dim3 <span class="title">block</span> <span class="params">(<span class="number">1024</span>, <span class="number">1024</span>)</span></span>;</span><br><span class="line">	<span class="function">dim3 <span class="title">grid</span> <span class="params">(( N + block.x <span class="number">-1</span>) / block.x, (N + block.y <span class="number">-1</span>) / block.y )</span></span>;</span><br><span class="line"></span><br><span class="line">	kernel_func &lt;&lt;&lt;grid, block &gt;&gt;&gt;(dev_m, dev_mt);</span><br></pre></td></tr></table></figure>

<p>  当然所有的configuration都应该在你的GPU硬件极限内。</p>
</li>
<li><p>当数据量很大时，所有CUDA cores 就需要工作不止一波，第一波后，就需要更新threads ID 继续工作下一波：</p>
  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b, <span class="keyword">int</span> *c)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">	<span class="keyword">while</span> (tid &lt; N) &#123;</span><br><span class="line">		c[tid] = a[tid] + b[tid];</span><br><span class="line">		<span class="comment">// OPERATIONS</span></span><br><span class="line">		tid += blockDim.x * gridDim.x;     <span class="comment">// update id when #threads are less than #elements </span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>  此处的<code>while</code>循环 表示，只要threads ID 还小于元素个数N，就更新ID。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/04/CUDA-%E5%87%A0%E7%82%B9%E8%A6%81%E8%AE%B0%E4%BD%8F-%E6%9B%B4%E6%96%B0ID/" data-id="ck71en4kl003vhefz5cp47is6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-Linux下计时器-GPU信息-Device函数修饰词" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/04/CUDA-Linux%E4%B8%8B%E8%AE%A1%E6%97%B6%E5%99%A8-GPU%E4%BF%A1%E6%81%AF-Device%E5%87%BD%E6%95%B0%E4%BF%AE%E9%A5%B0%E8%AF%8D/" class="article-date">
  <time datetime="2019-10-04T11:24:26.000Z" itemprop="datePublished">2019-10-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/04/CUDA-Linux%E4%B8%8B%E8%AE%A1%E6%97%B6%E5%99%A8-GPU%E4%BF%A1%E6%81%AF-Device%E5%87%BD%E6%95%B0%E4%BF%AE%E9%A5%B0%E8%AF%8D/">CUDA-Linux下计时器-GPU信息-Device函数修饰词</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="CUDA-中修饰函数的三个修饰词"><a href="#CUDA-中修饰函数的三个修饰词" class="headerlink" title="CUDA 中修饰函数的三个修饰词"></a>CUDA 中修饰函数的三个修饰词</h1><p><code>__global__</code> : 此函数由CPU调用，在GPU端执行。可调用自身或者两一个<strong>global</strong>函数。</p>
<p><code>__host__</code>: 此函数由CPU调用，在CPU端执行。一般默认省略。在CPU端只能调用<strong>global</strong>函数，不能调用<strong>device</strong>函数。</p>
<p><code>__device__</code> : 此函数由GPU调用，在GPU端执行。只能由<strong>global</strong>函数或<strong>device</strong>函数调用。可调用<strong>device</strong>函数。</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//add 1 for each element in the vector. </span></span><br><span class="line"><span class="comment">//__device__ functions can be called by __gloable__  functions</span></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">addOne</span><span class="params">(<span class="keyword">float</span>&amp; z)</span></span>&#123;</span><br><span class="line">	z += <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// add yourself to you </span></span><br><span class="line"><span class="comment">//__device__fucntions can be called by __device__ functions</span></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">addSelf</span><span class="params">(<span class="keyword">float</span>&amp; z)</span></span>&#123;</span><br><span class="line">	z += z;</span><br><span class="line">	addOne(z);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) add __global__ to kernel, AKA device code</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">float</span>* y, <span class="keyword">float</span>* z)</span></span>&#123;  </span><br><span class="line">	<span class="keyword">int</span> tid = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (tid &lt; N)&#123;</span><br><span class="line">		z[tid] = x[tid] + y[tid];</span><br><span class="line">		addSelf(z[tid]);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">体会`Think Parallel`</span><br></pre></td></tr></table></figure>

<p>在CPU端， 只能调用add()。在add() 函数中，对于每一个线程，除了元素求和之外，还调用了addSelf() 函数。因为addSelf() 由<code>__device__</code>修饰，所以可以被add() 函数调用。</p>
<p>在addSelf() 函数中，每个元素自己加上自己，后调用了另一个<code>__device__</code>函数： addOne()：元素加一。</p>
<h1 id="Linux-下的计时器"><a href="#Linux-下的计时器" class="headerlink" title="Linux 下的计时器"></a>Linux 下的计时器</h1><p>在<code>&lt;sys/time.h&gt;</code>中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cpuSecond</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> <span class="title">tp</span>;</span></span><br><span class="line">	gettimeofday(&amp;tp,<span class="literal">NULL</span>);</span><br><span class="line">	<span class="keyword">return</span> ((<span class="keyword">double</span>)tp.tv_sec + (<span class="keyword">double</span>)tp.tv_usec*<span class="number">1.e-6</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">double</span> iStart = cpuSecond();</span><br><span class="line">    <span class="comment">// Do what ever you want here</span></span><br><span class="line">	<span class="keyword">double</span> iElaps = cpuSecond() - iStart;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"time: %.10f \n"</span>, iElaps);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="获得当前使用GPU的信息"><a href="#获得当前使用GPU的信息" class="headerlink" title="获得当前使用GPU的信息"></a>获得当前使用GPU的信息</h1><p>这应当是写CUDA code的第一步，了解你所用工具的基本信息。</p>
<p>当机器由不止一个GPU时，需要知道当前由多少个GPU，默认使用哪一个，指定使用哪一个。</p>
<ul>
<li><p>可使用（CUDA-enabled）的GPU个数: <code>cudaGetDeviceCount()</code></p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> deviceCount = <span class="number">0</span>;</span><br><span class="line">cudaError_t error_id = cudaGetDeviceCount(&amp;deviceCount);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Device number: %d\n"</span>, deviceCount);</span><br></pre></td></tr></table></figure>

<p>  GPU个数存在<code>deviceCount</code>中， 此时可以使用循环来打印各个GPU的信息：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (dev = <span class="number">0</span>; dev &lt; deviceCount; ++dev) &#123;  </span><br><span class="line">	cudaSetDevice(dev);            <span class="comment">// 制定使用索引为dev的GPU</span></span><br><span class="line">	cudaDeviceProp deviceProp;      <span class="comment">// 创建一个property对象</span></span><br><span class="line">	cudaGetDeviceProperties(&amp;deviceProp, dev);   <span class="comment">//得到这个GPU的property</span></span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"\nDevice %d: \"%s\"\n"</span>, dev, deviceProp.name);</span><br><span class="line">	cudaDriverGetVersion(&amp;driverVersion);</span><br><span class="line">	cudaRuntimeGetVersion(&amp;runtimeVersion);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\n"</span>, </span><br><span class="line">		driverVersion / <span class="number">1000</span>, (driverVersion % <span class="number">100</span>) / <span class="number">10</span>, runtimeVersion / <span class="number">1000</span>, (runtimeVersion % <span class="number">100</span>) / <span class="number">10</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"  CUDA Capability Major/Minor version number:    %d.%d\n"</span>, </span><br><span class="line">		deviceProp.major, deviceProp.minor);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>当前使用哪一个GPU: <code>cudaGetDevice()</code></p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// the device that is currently used</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setupDevice</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> dev;</span><br><span class="line">	cudaGetDevice(&amp;dev);</span><br><span class="line">	cudaDeviceProp prop;</span><br><span class="line">	cudaGetDeviceProperties(&amp;prop, dev);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"\nDevice name %d: %s \n"</span>, dev, prop.name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>制定使用哪个GPU: <code>cudaSetDevice()</code></p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> dev = <span class="number">2</span>;</span><br><span class="line">cudaSetDevice(dev);  <span class="comment">// 使用索引为2 的GPU</span></span><br></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/04/CUDA-Linux%E4%B8%8B%E8%AE%A1%E6%97%B6%E5%99%A8-GPU%E4%BF%A1%E6%81%AF-Device%E5%87%BD%E6%95%B0%E4%BF%AE%E9%A5%B0%E8%AF%8D/" data-id="ck71en4j0000ahefz6nzp27nr" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-CUDA-PCIe速率" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/04/CUDA-PCIe%E9%80%9F%E7%8E%87/" class="article-date">
  <time datetime="2019-10-04T10:45:01.000Z" itemprop="datePublished">2019-10-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/CUDA/">CUDA</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/04/CUDA-PCIe%E9%80%9F%E7%8E%87/">CUDA-PCIe速率</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="检测PCIe的数据传输速度"><a href="#检测PCIe的数据传输速度" class="headerlink" title="检测PCIe的数据传输速度"></a>检测PCIe的数据传输速度</h1><p>当从Host 拷贝数据到Device的过程中，数据需要通过PCIe实现拷贝。所以你的主板的PCIe的版本和传输速度就会影响CUDA 代码的效率。</p>
<p>首先你需要知道你的GPU的显存大小。比如我的P106 有6GB 的VRAM。然后分别传输1GB，2GB，… 的数据。</p>
<p>假如传输int型数据，根据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">printf</span>(<span class="string">"this TYPE size: %lu Bytes\n"</span>, <span class="keyword">sizeof</span>(TYPE));</span><br></pre></td></tr></table></figure>
<p>来得到所使用的机器存储一个int型所需多少空间。我的机器存储int型需要4Bytes。</p>
<p>如果要传输3GB的数据，那么所需int型数据的数量为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>GB = <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">1</span> Bytes</span><br><span class="line"><span class="number">1</span>GB / <span class="number">4B</span>yte = <span class="number">268435456</span></span><br></pre></td></tr></table></figure>
<p>将这个数赋值给N。</p>
<p>之后，分别在Host和Device上开辟空间，最后计时从Host拷贝到Device所需时间：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 268435456  <span class="comment">// 1GB int</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>* h_a;</span><br><span class="line">h_a = (<span class="keyword">int</span>*)<span class="built_in">malloc</span>(N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>* dev_a;</span><br><span class="line">cudaMalloc((<span class="keyword">int</span>**)&amp;dev_a, N*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">    h_a[i] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">clock_t</span> start = clock();</span><br><span class="line">cudaMemcpy(dev_a, h_a, N*<span class="keyword">sizeof</span>(TYPE), cudaMemcpyHostToDevice);</span><br><span class="line"><span class="keyword">clock_t</span> end = clock();</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"time: %.10f s \n"</span>, (<span class="keyword">double</span>)(end - start) / CLOCKS_PER_SEC);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">free</span>(h_a);</span><br><span class="line">cudaFree(dev_a);</span><br></pre></td></tr></table></figure>

<p>之后可以逐步增加数据量，知道VRAM极限。如下是实验结果，传输数据量及所需时间：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>GB: <span class="number">268435456</span>    <span class="comment">//  1.349s</span></span><br><span class="line"><span class="number">2</span>GB: <span class="number">268435456</span>*<span class="number">2</span>  <span class="comment">//  2.700s </span></span><br><span class="line"><span class="number">3</span>GB: <span class="number">268435456</span>*<span class="number">3</span>  <span class="comment">//  4.050s</span></span><br><span class="line"><span class="number">4</span>GB: <span class="number">268435456</span>*<span class="number">4</span>  <span class="comment">//  5.400s</span></span><br><span class="line"><span class="number">5</span>GB: <span class="number">268435456</span>*<span class="number">5</span>  <span class="comment">//  6.750s</span></span><br><span class="line"><span class="number">5.5</span>GB:            <span class="comment">//  7.430s          </span></span><br><span class="line"><span class="number">5.75</span>GB:           <span class="comment">//  7.760s</span></span><br><span class="line"></span><br><span class="line"><span class="number">6</span>GB: <span class="number">268435456</span>*<span class="number">6</span>  <span class="comment">//  0.000s</span></span><br></pre></td></tr></table></figure>

<p>6GB 的数据错误是因为VRAM不可能全部给用户使用。<br>所传输数据量越大，经过PCIe传输时间也就越长。这样可以感受PCIe的速度。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/04/CUDA-PCIe%E9%80%9F%E7%8E%87/" data-id="ck71en4km003yhefz3jhf1ykr" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回顾cpp-继承-二" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/08/%E5%9B%9E%E9%A1%BEcpp-%E7%BB%A7%E6%89%BF-%E4%BA%8C/" class="article-date">
  <time datetime="2019-09-08T14:16:00.000Z" itemprop="datePublished">2019-09-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/C/">C++</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/08/%E5%9B%9E%E9%A1%BEcpp-%E7%BB%A7%E6%89%BF-%E4%BA%8C/">回顾cpp-继承-二</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>继承方式  check</li>
<li>隐藏   check</li>
<li>多继承   check</li>
<li>多重继承  check</li>
<li>虚继承  check</li>
</ul>
<h1 id="多继承"><a href="#多继承" class="headerlink" title="多继承"></a>多继承</h1><p>类间关系是<code>树</code>。</p>
<p>一个子类由多个不同的父类：<code>可飞行 &lt;- 蝙蝠，哺乳动物 &lt;- 蝙蝠</code>，蝙蝠 <code>Is-a</code> 可飞行，同时 <code>Is-a</code> 哺乳动物。而可飞行与哺乳动物没有任何关系。具体实现可以是这样：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flyable</span>&#123;</span>&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mammal</span>&#123;</span>&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bat</span> :</span> <span class="keyword">public</span> Flyable, <span class="keyword">public</span> Mammal&#123;&#125;;</span><br></pre></td></tr></table></figure>

<p>实例化一个<code>Bat</code>子类时，会先调用其<font color="red">所有</font>的父类构造函数，再调用<font color="red">所有</font>子类构造函数。析构函数会以相反的顺序被调用。</p>
<h1 id="多重继承"><a href="#多重继承" class="headerlink" title="多重继承"></a>多重继承</h1><p>类间关系是<code>串</code>。</p>
<p>Human被Asion继承，Asion被Chinese继承：<code>Human &lt;- Asion &lt;- Chinese</code>，三者由一下关系：Chinese <code>Is-a</code> Asion，Chinese <code>Is-a</code> Human，Asion<code>Is-a</code> Human。具体实现可以是这样：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span>&#123;</span>&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Asion</span> :</span> <span class="keyword">public</span> Human&#123;&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span> :</span> <span class="keyword">public</span> Asion&#123;&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="虚继承"><a href="#虚继承" class="headerlink" title="虚继承"></a>虚继承</h1><p>复杂的继承关系，如菱形继承：</p>
<div align="center"><img src="/2019/09/08/%E5%9B%9E%E9%A1%BEcpp-%E7%BB%A7%E6%89%BF-%E4%BA%8C/lingxing2.png" width="600"></div>

<p>为什么会有虚继承。因为在继承过程中，为了避免<code>MigrantWorker</code>继承两次<code>Person</code>，使用<code>virtual</code>关键字，实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &#123;</span>&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span> :</span> <span class="keyword">virtual</span> <span class="keyword">public</span> Person &#123;&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Farmer</span> :</span> <span class="keyword">virtual</span> <span class="keyword">public</span> Person &#123;&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MigrantWoker</span> :</span> <span class="keyword">public</span> Worker, <span class="keyword">public</span> Farmer &#123;&#125;;</span><br></pre></td></tr></table></figure>

<p><font color="#9932CC" size="5">敲黑板</font>为避免重复继承，使用虚继承<code>virtual</code>。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/09/08/%E5%9B%9E%E9%A1%BEcpp-%E7%BB%A7%E6%89%BF-%E4%BA%8C/" data-id="ck71en4ki003lhefzga655wq4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-数值优化算法-梯度下降" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="article-date">
  <time datetime="2019-09-06T07:46:05.000Z" itemprop="datePublished">2019-09-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">数值优化算法-BGD-SGD</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>作用是优化一个目标函数，它是一个基于搜索的数值优化算法。具体说来，当需要最小化一个损失函数，使用梯度下降；当最大化一个效用函数时，使用梯度上升发。</li>
<li>在线性回归中使用的是最小二乘法得到最终结果，而对于许多机器学习算法，其最小化损失函数的方法是不能使用类似最小二乘法直接求解的。此时可以考虑借鉴数值计算方法中的凸优化问题的解法。</li>
<li>目标函数是参数的函数，输入数据是常量。所以目的是找到目标函数取最小值时的参数。</li>
<li>搜索可能陷入局部最优解，解决方法有比如<strong>动量梯度下降</strong>，跳出局部最优值。还可以多次运行GD，每次的起始点随机化。</li>
<li>线性回归问题的目标有唯一的最优解，（凸优化问题）</li>
<li>当目标函数是凹函数时，才求最小值。目标函数对参数求导数，得到梯度。梯度表示这一点的切线斜率的值。根据参数迭代公式：当在某一点的梯度小于0时，迭代后参数值增大，即对应的目标函数值减小；当在某一点的梯度大于0时，迭代后参数值变小，即目标函数值减小。可见，不管点的位置在哪，迭代参数后，目标函数值都减小。（详见数学笔记）</li>
</ul>
<h1 id="模拟梯度下降"><a href="#模拟梯度下降" class="headerlink" title="模拟梯度下降"></a>模拟梯度下降</h1><p>先创建一个数据集，使其图像为凹函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-1</span>, <span class="number">6</span>, <span class="number">200</span>)</span><br><span class="line">y = (x<span class="number">-2.5</span>)**<span class="number">2</span><span class="number">-1</span></span><br></pre></td></tr></table></figure>

<p>x为参数，y为目标函数。y对x求导数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta)</span>:</span></span><br><span class="line">    <span class="string">"""计算梯度"""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(theta<span class="number">-2.5</span>)</span><br></pre></td></tr></table></figure>
<p>对x求对应y：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(theta)</span>:</span></span><br><span class="line">    <span class="string">"""对应目标函数值"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> (theta<span class="number">-2.5</span>)**<span class="number">2</span><span class="number">-1</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> float(<span class="string">'inf'</span>)   <span class="comment"># 当J太大时，返回浮点最大值</span></span><br></pre></td></tr></table></figure>

<p>根据迭代公式，编码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">epsilon = <span class="number">1e-8</span></span><br><span class="line">theta = <span class="number">0</span></span><br><span class="line">theta_history = [theta]</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    grad = gradient(theta)</span><br><span class="line">    last_theta = theta</span><br><span class="line">    theta = theta - learning_rate * grad</span><br><span class="line">    theta_history.append(theta)</span><br><span class="line">    <span class="keyword">if</span> (abs(loss(theta)- loss(last_theta)) &lt; epsilon):</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>其中，epsilon是一个很小的值，当<code>abs(loss(theta)- loss(last_theta)) &lt; epsilon</code>时，表示迭代参数使得目标函数值达到最小值。为什么不是当梯度为零停止迭代呢，因为浮点数0，在计算机中不是零，这样，就本实验而言循环不会停止。</p>
<p><code>theta_history</code>记录了每次迭代的参数值。<br>当<code>learning_rate = 0.1，theta = 0</code>时绘出函数图像，及参数迭代路径：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot(np.asarray(theta_history), loss(np.asarray(theta_history)), marker=<span class="string">'+'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>如图：</p>
<div align="center"><img src="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/theta1.png" width="500"></div>

<p>一共迭代了46次，最终停在点<code>(2.499891109642585, -0.99999998814289)</code>，与最小值点<code>(2.5, -1)</code>非常接近。这表明了算法的正确性。</p>
<p>当当<code>learning_rate = 0.1，theta = 5.5</code>时绘出函数图像，及参数迭代路径：</p>
<div align="center"><img src="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/theta2.png" width="500"></div>

<p>可以看出，不论参数初始值在哪，最终会停在最小值处。</p>
<p>当学习率较小时：<code>learning_rate = 0.01，theta = 5.5</code>绘出函数图像，及参数迭代路径：</p>
<div align="center"><img src="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/theta3.png" width="500"></div>

<p>依旧停在最小值处，但是迭代次数增加到433次。</p>
<p>当学习率较大时：<code>learning_rate = 0.9，theta = 5.5</code>绘出函数图像，及参数迭代路径：</p>
<div align="center"><img src="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/theta4.png" width="500"></div>

<p>可以看出，学习率较大时，迭代路径在最优值左右震动。</p>
<p>为了避免迭代无止境的循环下去，设置参数<code>n_itr</code>，表示最大迭代次数。</p>
<p>当设置学习率过大时，<code>learning_rate = 1.1``theta = 0，n_itr=5</code>，出现梯度上升情况，如图：</p>
<div align="center"><img src="/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/theta5.png" width="500"></div>

<p>限定只迭代5次。</p>
<p>所以要设置最合适的学习率。</p>
<h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p>以上实现过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(init_theta, learning_rate, n_itr, epsilon=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    theta = init_theta</span><br><span class="line">    theta_history.append(theta)</span><br><span class="line">    i_itr = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i_itr &lt; n_itr:</span><br><span class="line">        gradient = dJ(theta)</span><br><span class="line">        last_theta = theta</span><br><span class="line">        theta = theta - learning_rate*gradient</span><br><span class="line">        theta_history.append(theta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (abs(J(theta) - J(last_theta)) &lt; epsilon):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i_itr += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_theta_history</span><span class="params">()</span>:</span></span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(np.asarray(theta_history), J(np.asarray(theta_history)), marker=<span class="string">'+'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">theta_history = []</span><br><span class="line"></span><br><span class="line">gradient_descent(<span class="number">0</span>, learning_rate, <span class="number">5</span>)</span><br><span class="line">plot_theta_history()</span><br></pre></td></tr></table></figure>

<h1 id="多元线性回归的GD"><a href="#多元线性回归的GD" class="headerlink" title="多元线性回归的GD"></a>多元线性回归的GD</h1><h2 id="python数据行为"><a href="#python数据行为" class="headerlink" title="python数据行为"></a>python数据行为</h2><p>创建一个一维向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line">x1 = <span class="number">2</span> * np.random.random(size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p>x1的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ 1.39293837  0.57227867  0.45370291 ... 1.10262954  1.43893794 ]</span><br></pre></td></tr></table></figure>

<p>x1是一个长度为100的一维向量。向量每个元素为一个标量。用 同样的方式在创建一个等长度的一维向量x2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x2 = <span class="number">3</span> * np.random.random(size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p>假如变量y由x1和x2共同决定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x1 * <span class="number">3</span> + x2 * <span class="number">4</span> + (<span class="number">5</span> + np.random.normal(size=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<p>此时x1和x2作为输入样本的两个特称，现在把两者合并在一起：<br>第一步变形为1列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x1.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>返回一个二维向量，每一个元素为一个长度为1的一维向量，每一个元素表示一个样本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[ 1.39293837]</span><br><span class="line"> [ 0.57227867]</span><br><span class="line"> [ 0.45370291]</span><br><span class="line"> ...</span><br><span class="line"> [ 1.10262954]</span><br><span class="line"> [ 1.43893794]</span><br><span class="line"> [ 0.84621292]]    100X1</span><br></pre></td></tr></table></figure>
<p>第二步，将两个特征合并在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = np.hstack([x1.reshape(<span class="number">-1</span>, <span class="number">1</span>), x2.reshape(<span class="number">-1</span>, <span class="number">1</span>)])</span><br></pre></td></tr></table></figure>
<p>返回X为一个二维向量，这个向量的每个元素为一个样本，每个样本由两个值表达：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[ 1.39293837  1.53938446]</span><br><span class="line"> [ 0.57227867  1.99987365]</span><br><span class="line"> [ 0.45370291  0.31772546]</span><br><span class="line"> ...</span><br><span class="line"> [ 1.10262954  0.39268485]</span><br><span class="line"> [ 1.43893794  0.96594182]</span><br><span class="line"> [ 0.84621292  1.98469301]]     100X2</span><br></pre></td></tr></table></figure>

<p>因为y的组成还有一个常量，可以用x0表示，x0恒为1，并且把x0也X中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.hstack([np.ones((len(X), <span class="number">1</span>)), X])</span><br></pre></td></tr></table></figure>

<p>返回<code>X_b</code> 中每个元素由3个值决定：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[ 1.          1.39293837  1.53938446]</span><br><span class="line"> [ 1.          0.57227867  1.99987365]</span><br><span class="line"> [ 1.          0.45370291  0.31772546]</span><br><span class="line"> ...</span><br><span class="line"> [ 1.          1.10262954  0.39268485]</span><br><span class="line"> [ 1.          1.43893794  0.96594182]</span><br><span class="line"> [ 1.          0.84621292  1.98469301]]   100X3</span><br></pre></td></tr></table></figure>
<p>现在可以看看X_b 的属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(type(X_b))</span><br><span class="line">print(X_b.shape)</span><br><span class="line">print(X_b.shape[<span class="number">0</span>])   </span><br><span class="line">print(X_b.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>返回：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#39;numpy.ndarray&#39;&gt;    # 是一个 numpy.ndarray 对象</span><br><span class="line">(100, 3)    # 矩阵 100行 3列</span><br><span class="line">100         # 外维度 100</span><br><span class="line">3           # 内维度  3</span><br></pre></td></tr></table></figure>
<p>100行表示，一共有100个样本，3列表示每个样本有3个特征。</p>
<p>因为此时<code>X_b</code>是一个矩阵，所以还可以对<code>X_b</code>进行切片操作，就是取它是一部分：<br>取索引为0的元素，即第一行，所有列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(X_b[<span class="number">0</span>, :])</span><br><span class="line"></span><br><span class="line">[ <span class="number">1.</span>          <span class="number">1.39293837</span>  <span class="number">1.53938446</span>]</span><br></pre></td></tr></table></figure>
<p>取所有行，索引是0和1的列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(X_b[:, <span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">[[ <span class="number">1.</span>          <span class="number">1.39293837</span>]</span><br><span class="line"> [ <span class="number">1.</span>          <span class="number">0.57227867</span>]</span><br><span class="line"> [ <span class="number">1.</span>          <span class="number">0.45370291</span>]</span><br><span class="line"> ...</span><br><span class="line"> [ <span class="number">1.</span>          <span class="number">1.10262954</span>]</span><br><span class="line"> [ <span class="number">1.</span>          <span class="number">1.43893794</span>]</span><br><span class="line"> [ <span class="number">1.</span>          <span class="number">0.84621292</span>]]   <span class="number">100</span>X2</span><br></pre></td></tr></table></figure>

<p>其他函数的用法：<br><code>np.ones((4, 2))</code>：创建矩阵4行2列。</p>
<h2 id="dot"><a href="#dot" class="headerlink" title="dot()"></a>dot()</h2><p><code>.dot()</code> 是<code>numpy.ndarray</code>对象的方法，实例：<br><code>X_b</code>是100X3的矩阵，<code>theta</code>为长为3的向量，y为长度为100 的向量，则：</p>
<p><code>X_b.dot(theta)</code> 结果为长度为100 的向量。<br><code>y - X_b.dot(theta)</code>结果为长度为100的向量。<br><code>(y - X_b.dot(theta))**2</code>  结果为长100 的向量。平方差。</p>
<p><code>np.sum((y - X_b.dot(theta))**2)</code> 结果为100个数就和。 平方差之和。<br><code>np.sum((y - X_b.dot(theta))**2) / len(X_b)</code> 就是MSE。</p>
<p><code>对象.dot(参数)</code>中<code>对象</code>是一个矩阵，它的每一行代表一个样本，所有行表示所有样本。<code>参数</code>是一个向量，这个向量中每一个值分别与<code>对象</code>的每一行的每个值相乘后相加，最后得到一个与<code>参数</code>大小一样的向量。</p>
<h2 id="BGD算法过程"><a href="#BGD算法过程" class="headerlink" title="BGD算法过程"></a>BGD算法过程</h2><p>目标：最小化loss：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">    <span class="string">"""目标函数"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> np.sum((y - X_b.dot(theta))**<span class="number">2</span>) / len(X_b)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> float(<span class="string">'inf'</span>)</span><br></pre></td></tr></table></figure>

<p>迭代过程中需要求梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">    <span class="string">"""loss对参theta的梯度"""</span></span><br><span class="line">    res = np.empty(len(theta))</span><br><span class="line">    <span class="comment"># 根据数学推到得 梯度的三个分量：</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(theta)):</span><br><span class="line">        res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])   <span class="comment"># dot操作中左后一步是Sum</span></span><br><span class="line">    <span class="keyword">return</span> res * <span class="number">2</span> / len(X_b)</span><br></pre></td></tr></table></figure>
<p>其中res[i]的值所用公式见数学笔记。</p>
<p>GD的过程如下，输入样本<code>X_b</code>已知, <code>y</code>表示真是拟合值，只需要初始化参数<code>theta</code>就可执行GD。<br><code>init_theta =  np.zeros(X_b.shape[1])</code> 初始值为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X_b, y, init_theta, learning_rate=<span class="number">0.01</span>, n_itr=<span class="number">1000</span>, epsilon=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    theta = init_theta</span><br><span class="line">    i_itr = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i_itr &lt; n_itr:</span><br><span class="line">        grad = gradient(theta, X_b, y)</span><br><span class="line">        last_theta = theta</span><br><span class="line">        theta = theta - learning_rate*grad</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (abs(loss(theta, X_b, y) - loss(last_theta, X_b, y)) &lt; epsilon):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i_itr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>

<p>调用GD：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta = gradient_descent(X_b, y, init_theta, learning_rate=<span class="number">0.01</span>, <span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">得到迭代最终的参数theta：</span><br><span class="line">[ <span class="number">5.09735976</span>  <span class="number">2.81188565</span>  <span class="number">4.01306057</span>]</span><br></pre></td></tr></table></figure>

<p>结果分别对应（5 + np.random.normal(size=100)，3，4。</p>
<p>其中，对于求梯度，观察式子，发现可以使用<strong>向量化</strong>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_vectorize</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">    <span class="string">"""向量化"""</span></span><br><span class="line">    <span class="keyword">return</span> X_b.T.dot(X_b.dot(theta)-y) * <span class="number">2</span>/len(X_b)</span><br></pre></td></tr></table></figure>

<h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>当样本的每种特征数值量级差别很大时：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00</span><br><span class="line">    5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00</span><br><span class="line">    1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02</span><br><span class="line">    4.98000000e+00]</span><br></pre></td></tr></table></figure>
<p>使用一般的步长，得到的结果在python中为<code>nan</code>。因为特征数值相差太大时，一般的步长也变得很大很大（步长对数据特征值之差敏感）。此时减小步长，增加迭代次数也就可以达到结果，但效率很低。</p>
<p>所以使用梯度算法前，要对数据进行归一化。</p>
<h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p>随机梯度下降法，每次处理一个样本，马上计算一次梯度（见数学笔记）。与BGD相比，SGD的loss函数改变了：<br>BGD的loss是将<font color="red" size="4">所有样本带入目标函数后的结果</font>（看公式）。而SGD的loss是把<font color="red" size="4">当前这一个样本带入目标函数的结果</font>。</p>
<p>所以，BGD的参数迭代路径的方向是由所有样本决定，是实际方向。而SGD的参数路径是曲折的，因为每一步只是由一个样本得到，这个方向不能代表所有样本的实际方向。</p>
<p>SGD一般设置学习率为变化值。<code>learning_rate = a / (itr+b)</code>。</p>
<p>SGD实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_vectorize</span><span class="params">(theta, X_b_i, y_i)</span>:</span></span><br><span class="line">    <span class="string">"""每一个样本的梯度"""</span></span><br><span class="line">    <span class="keyword">return</span> X_b_i.T.dot(X_b_i.dot(theta)-y_i) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(X_b, y, init_theta, n_itr)</span>:</span></span><br><span class="line"></span><br><span class="line">    t0 =<span class="number">5</span></span><br><span class="line">    t1 = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 变化的学习率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learning_rate</span><span class="params">(t)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line">    theta = init_theta</span><br><span class="line">    <span class="keyword">for</span> itr_i <span class="keyword">in</span> range(n_itr):</span><br><span class="line">        <span class="comment"># 随机选一个索引：</span></span><br><span class="line">        curr_sample_id = np.random.randint(len(X_b))   </span><br><span class="line">        <span class="comment"># 取出该索引对应的样本，计算对应样本的梯度：</span></span><br><span class="line">        grad = gradient_vectorize(theta, X_b[curr_sample_id], y[curr_sample_id]) </span><br><span class="line">        <span class="comment"># 根据一个样本更新梯度：</span></span><br><span class="line">        theta = theta - learning_rate(itr_i) * grad</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>

<p><font color="gree" size="5">敲黑板</font></p>
<ul>
<li>本篇笔记的GD算法是一次性把所有样本带入，也就是遍历一遍所有样本才更新一次参数。称为BGD</li>
<li>向量化速度快</li>
<li>与步长有关的迭代算法，首先要对数据进行归一化。</li>
<li>常用python函数</li>
<li>BGD和SGD的loss函数是不同的，BGD在迭代过程中目标函数不变，而SGD的loss随每个样本而改变。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/09/06/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" data-id="ck7bjjf680015i5fz6alsa7p0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">38</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">31</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hardware/" rel="tag">hardware</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 16.67px;">CUDA</a> <a href="/tags/Test-Analysis/" style="font-size: 13.33px;">Test Analysis</a> <a href="/tags/hardware/" style="font-size: 10px;">hardware</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/03/05/cpp-pro-tip-1/">cpp pro tip 1</a>
          </li>
        
          <li>
            <a href="/2020/02/28/CUDA-%E5%B9%B6%E8%A1%8C%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF/">CUDA-并行一维卷积</a>
          </li>
        
          <li>
            <a href="/2020/02/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-stack/">LeetCode-方法论-stack</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%9D%82%E8%AE%B0%E5%BE%85%E5%BD%92%E7%B1%BB/">CUDA-杂记待归类</a>
          </li>
        
          <li>
            <a href="/2020/02/20/CUDA-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95/">CUDA-扫描算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>