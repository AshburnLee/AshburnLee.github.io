<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Junhui&#39;s Journal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Junhui&#39;s Journal">
<meta property="og:url" content="http://yoursite.com/page/10/index.html">
<meta property="og:site_name" content="Junhui&#39;s Journal">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Junhui">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Junhui&#39;s Journal" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Junhui&#39;s Journal</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-LeetCode-方法论-数组相关-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-24T18:28:14.000Z" itemprop="datePublished">2019-08-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LeetCode/">LeetCode</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/">LeetCode-方法论-数组相关-一</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>与数组相关的问题是最常出现的问题。<br>这篇笔记记录问题编号：<br>283, 167, 209, 75, 11, 125, 3</p>
<p><font color="gree" size="5">敲黑板</font><br>常用技术：</p>
<ul>
<li>计数排序，当元素种类较小时使用</li>
<li>对撞指针，当数据元素有序时使用</li>
<li>滑动串口，注意此窗口大小不一定固定</li>
<li>图示简化实现，有些情况下，画好了中间过程的图示，实现起来就像看图说话</li>
<li>循环不变量，保证程序正确性，并且使图示简化实现成为可能</li>
<li>更新记录，更新循环中的每一步结果</li>
<li>跳过，</li>
<li>频数记录，技巧 记录频数</li>
<li>大条件先满足，在if语句中，大小条件一定要在小条件之前</li>
</ul>
<p>实现时的注意：</p>
<ul>
<li>对边界的正确处理，明确循环不变量的定义且需要始终维护。</li>
<li>使用小数据集调试，先保证算法的正确性。</li>
<li>应尽量减少代码量，合并可以合并的，删掉无用的。经验上讲，同一个算法，代码量越多越容易出错。</li>
<li>先由算法过程，后实现，不要上来就实现。</li>
</ul>
<h1 id="283-Move-Zeros"><a href="#283-Move-Zeros" class="headerlink" title="#283 Move Zeros"></a>#283 Move Zeros</h1><ul>
<li><p>描述：给出一个序列，将所有0元素移动到序列尾部，并且其他元素相对位置不变。</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: [0,1,0,3,12]</span><br><span class="line">Output: [1,3,12,0,0]</span><br></pre></td></tr></table></figure>
</li>
<li><p>思路一：</p>
  <div align="center"><img src="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/array1.png"></div>

<p>  如图，整个过程保持[0,..,k)中元素非零。遍历结束后，将k及其以后的元素值设为0。就得到最后值。</p>
<p>  时间复杂度：O(N)<br>  空间复杂度：O(1)</p>
</li>
<li><p>思路二：</p>
  <div align="center"><img src="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/array3.png" width="800"></div>

<p>  将思路一中，当<code>nums[i]!=0</code>时，的执行改为 <code>swap(nums[i], nums[k++])</code>。如此不需要思路一的最后一步。</p>
<p>  时间复杂度：O(N)<br>  空间复杂度：O(1)</p>
</li>
<li><p>实现见<a href="https://github.com/AshburnLee/LeetCode" target="_blank" rel="noopener">这里</a>，的对应文件。</p>
</li>
</ul>
<p>关键：<font color="red" size="4">协调两指针</font></p>
<h1 id="167-Two-Sum-II-对撞指针"><a href="#167-Two-Sum-II-对撞指针" class="headerlink" title="#167 Two Sum II 对撞指针"></a>#167 Two Sum II 对撞指针</h1><ul>
<li><p>问题描述：</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: numbers = [2,7,11,15], target = 9</span><br><span class="line">Output: [1,2]</span><br></pre></td></tr></table></figure>
<p>  当数组有序时使用对撞指针。<br>  <code>number[0+1] + number[1+1] = target</code>.</p>
</li>
<li><p>思路：</p>
<pre><code>一个指针`i`从左端向右，另一个指针`j`从右端向左。
如果`number[i] + mumber[j] = target` 时，返回对应的`i+1, j+1`。
如果`number[i] + number[j] &lt; target`, 说明`number[i]`小，所以`i++`；
如果`number[i] + number[j] &gt; taregt`, 说明`number[j]`大，所以`j--`；</code></pre></li>
<li><p>实现见<a href="https://github.com/AshburnLee/LeetCode" target="_blank" rel="noopener">这里</a>，的对应文件。</p>
</li>
<li><p>同类问题：<br>345</p>
</li>
</ul>
<h1 id="209-Minimum-Size-Subarray-Sum-滑动窗口"><a href="#209-Minimum-Size-Subarray-Sum-滑动窗口" class="headerlink" title="#209 Minimum Size Subarray Sum  滑动窗口"></a>#209 Minimum Size Subarray Sum  滑动窗口</h1><ul>
<li><p>问题描述：</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: s = 7, nums = [2,3,1,2,4,3]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: the subarray [4,3] has the minimal length under the problem constraint.</span><br></pre></td></tr></table></figure></li>
<li><p>思路</p>
<p>  设置窗口左右端，并且初始化：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span>  l=<span class="number">0</span>, r=<span class="number">-1</span>  <span class="comment">//始终保证nums[l,...,r]为滑动窗口，初始化为空</span></span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> res = nums.size()<span class="number">-1</span>  <span class="comment">// 结果设置为可能的最大值</span></span><br></pre></td></tr></table></figure>

<p>  然后窗口向右移动，移动过程中要判断两次：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(l &lt; nums.size())&#123;</span><br><span class="line">    <span class="comment">// 第一次判断</span></span><br><span class="line">    <span class="keyword">if</span> (r+<span class="number">1</span> &lt; nums.size() &amp;&amp; sum &lt; s)</span><br><span class="line">        sum += nums[++r];</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        sum -= nums[l++];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第二次判断</span></span><br><span class="line">    <span class="keyword">if</span> (sum&gt;s)</span><br><span class="line">        res = min(res, r-l+<span class="number">1</span>);  <span class="comment">// 更新结果</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>  循环中:</p>
<pre><code>第一次判断:
    如果r没有到头，且sum小于s：则r右移，sum加上此时的nums[r]。
    如果r到最右边，或sum大于等于s：则Sum减去nums[l]，且l右移。
第二次判断:
    如果sum大于等于s，
    结果res取这次res和上次res的最小值。

最后一步，判断，如果res扔等于初始值，即没有发生变化，则表示sum中所有元素和都小于s。返回0.</code></pre><p>  第一次判断的两种情况：</p>
  <div align="center"><img src="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/win.png" width="700"></div>

</li>
</ul>
<p>关键：<font color="red" size="4">图示简化实现</font>，<font color="red" size="4">循环不变量</font>，<font color="red" size="4">更新记录</font>，<font color="red" size="4">滑动窗口</font>，<font color="red" size="4">大条件先满足</font></p>
<h1 id="75-Sort-Colors-三路快排"><a href="#75-Sort-Colors-三路快排" class="headerlink" title="#75 Sort Colors 三路快排"></a>#75 Sort Colors 三路快排</h1><ul>
<li><p>问题描述：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一个数组由n个元素，元素只有0,1,2三种数值，为这个数组排序</span><br></pre></td></tr></table></figure></li>
<li><p>思路</p>
<ol>
<li><p>思路一： 计数排序</p>
<p> 先统计每个数值出现过多少次，之后从小到大将对应的值放入元素组，放入多少个呢，放入对应数值出现的次数个。实现：</p>
 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sortColor</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count[<span class="number">3</span>] = &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;;   </span><br><span class="line">    <span class="keyword">if</span> (nums.size() == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录每个数值出现的频数</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.size(); i++)&#123;</span><br><span class="line">        count[nums[i]]++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 挨个儿放入元素组</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;count[<span class="number">0</span>]; i++)</span><br><span class="line">        nums[index++] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;count[<span class="number">1</span>]; i++)</span><br><span class="line">        nums[index++] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;count[<span class="number">2</span>]; i++)</span><br><span class="line">        nums[index++] = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 很容易理解。</p>
</li>
<li><p>思路二： 三路快排</p>
<p> 初始化函数操作，</p>
 <div align="center"><img src="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/xxx.png" , width="700"></div>

 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sortColors</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> zero = <span class="number">-1</span>;         <span class="comment">// 定义[0,...,zero] 的元素为0</span></span><br><span class="line">    <span class="keyword">int</span> two = nums.size();   <span class="comment">// 定义[two,...,n-1] 的元素为 3</span></span><br></pre></td></tr></table></figure>

<p> 明确循环不变量的定义 <code>zero：数组中元素为0的最后一个index</code>，<code>two：数组中元素为2的第一个index</code>。</p>
<p> 之后执行如下图的操作：</p>
 <div align="center"><img src="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/xxx.png" , width="700"></div>

 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;two; )&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums[i] == <span class="number">1</span>)</span><br><span class="line">            i++;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(nums[i] == <span class="number">2</span>)</span><br><span class="line">            swap(nums[--two], nums[i] );</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            assert( nums[i] == <span class="number">0</span> );</span><br><span class="line">            swap(nums[++zero], nums[i++]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<p>关键：<font color="red" size="4">图示简化实现</font>，<font color="red" size="4">循环不变量</font></p>
<h1 id="11-Container-With-Most-Water-对撞指针"><a href="#11-Container-With-Most-Water-对撞指针" class="headerlink" title="#11 Container With Most Water 对撞指针"></a>#11 Container With Most Water 对撞指针</h1><ul>
<li><p>问题描述：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. </span><br><span class="line"></span><br><span class="line">Input: [1,8,6,2,5,4,8,3,7]</span><br><span class="line">Output: 49</span><br></pre></td></tr></table></figure>
</li>
<li><p>思路(无序数组求最值)：</p>
<p>  两指针分别从数组首位处开始，两个指针向中间移动，两指针的距离为<code>宽</code>，两指针对应的数值的较小值为<code>高度</code>，要最大化<code>宽度</code>x<code>高度</code>。注意两指针相互靠近，所以<code>宽度</code>是单调减小的，所以，要想记录最大值，就要跳过高度减小的值，即<code>i++</code>和<code>j++</code>。</p>
</li>
<li><p>实现</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxArea</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; height)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 初始化时，宽度是最大的</span></span><br><span class="line">    <span class="keyword">int</span> maxWater = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> j=height.size()<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开始对撞</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;j)&#123;</span><br><span class="line">        <span class="keyword">int</span> h = min(height[i], height[j]);</span><br><span class="line">        maxWater = max(maxWater, h*(j-i));  <span class="comment">// 每次循环，更新最大值</span></span><br><span class="line">        <span class="keyword">while</span>(height[i]&lt;=h &amp;&amp; i&lt;j) i++;   <span class="comment">// h减小了，所以跳过</span></span><br><span class="line">        <span class="keyword">while</span>(height[j]&lt;=h &amp;&amp; i&lt;j) j--;   <span class="comment">// 跳过</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maxWater;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关键：<font color="red" size="4">两指针对撞</font>，<font color="red" size="4">跳过</font>，<font color="red" size="4">更新记录</font></p>
</li>
</ul>
<h1 id="125-PalineDrome-判断是否是回文"><a href="#125-PalineDrome-判断是否是回文" class="headerlink" title="#125 PalineDrome 判断是否是回文"></a>#125 PalineDrome 判断是否是回文</h1><ul>
<li><p>问题描述：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;A man, a plan, a canal: Panama&quot;</span><br><span class="line">Output: true</span><br><span class="line">Note: For the purpose of this problem, we define empty string as valid palindrome.</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPalindrome</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> l = <span class="number">0</span>,r=s.length()<span class="number">-1</span>;l&lt;r; l++,r-- )&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="built_in">isalnum</span>(s[l])==<span class="literal">false</span> &amp;&amp; l&lt;r) l++;  <span class="comment">//跳过 non alnum</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="built_in">isalnum</span>(s[r])==<span class="literal">false</span> &amp;&amp; l&lt;r) r--;  <span class="comment">//跳过 non alnum</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">tolower</span>(s[l]) != <span class="built_in">tolower</span>(s[r])) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>关键：<font color="red" size="4">两指针对撞</font>，<font color="red" size="4">跳过</font>，<font color="red" size="4">常用字符串函数</font></p>
<h1 id="3-Longest-Substring-Without-Repeating-Charactors-最长无重复子串"><a href="#3-Longest-Substring-Without-Repeating-Charactors-最长无重复子串" class="headerlink" title="#3 Longest Substring Without Repeating Charactors 最长无重复子串"></a>#3 Longest Substring Without Repeating Charactors 最长无重复子串</h1><ul>
<li><p>描述</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;pwwkew&quot;</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The answer is &quot;wke&quot;, with the length of 3. </span><br><span class="line">            Note that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring.</span><br></pre></td></tr></table></figure>
</li>
<li><p>思路见图示：</p>
  <div align="center"><img src="/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/leetcode_array_3.png" , width="400"></div>
</li>
<li><p>看图说话：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> freq[<span class="number">256</span>] = &#123;<span class="number">0</span>&#125;;   <span class="comment">// 记录频数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>;    <span class="comment">// 滑动窗口保证s[l,...,r]始终无重复字符，初始化为空 </span></span><br><span class="line">    <span class="keyword">int</span> r = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> res = <span class="number">0</span>;  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (l &lt; s.size())&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (r+<span class="number">1</span>&lt;s.size() &amp;&amp; freq[s[r+<span class="number">1</span>]] == <span class="number">0</span>)&#123;</span><br><span class="line">            r++;</span><br><span class="line">            freq[s[r]] ++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;   <span class="comment">// r is out of bound || freq[r+1] == 1</span></span><br><span class="line">            freq[s[l]] --;</span><br><span class="line">            l++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        res = max(res, r-l+<span class="number">1</span>);   <span class="comment">// 更新结果</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>关键：<font color="red" size="4">图示简化实现</font>，<font color="red" size="4">循环不变量</font>，<font color="red" size="4">更新记录</font>，<font color="red" size="4">记录频数</font>，<font color="red" size="4">滑动窗口</font>，<font color="red" size="4">大条件先满足</font></p>
<hr>
<p><span style="font-family:Papyrus; font-size:2em">敲黑板</span>想要思维升级，就需要见足够多的问题类型，每种类型见过并解决不止一遍。只见过一遍就像完全掌握，是不实际的。见得多了，自然大脑就接受了，思维就升级了。另外一点，“回头看”会把之前不明白或者不能接受的问题“回锅”，可以增强大脑对这个问题的接纳程度。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/25/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E6%95%B0%E7%BB%84%E7%9B%B8%E5%85%B3-%E4%B8%80/" data-id="ckatsrgu4007ixqfzha512x3r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-LeetCode-方法论-回溯法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/" class="article-date">
  <time datetime="2019-08-22T14:15:58.000Z" itemprop="datePublished">2019-08-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LeetCode/">LeetCode</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/">LeetCode-方法论-回溯法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>46, 77, 40, 515, </p>
<ul>
<li>回溯法解决一类问题，排列与组合。</li>
<li>属于树型问题，所以通常需要画递归树。</li>
<li>通常需要有个容器来保存状态。</li>
<li>实现方法：理解问题，画递归树。</li>
<li>递归实现，需要“跳进跳出”的思维</li>
<li><font color="red" size="4">分清楚操作部分和结点移动部分</font></li>
<li>一个模式：<font color="red" size="4">移动控制+结点操作</font> 对上一条的强调</li>
</ul>
<h1 id="46"><a href="#46" class="headerlink" title="#46"></a>#46</h1><ul>
<li><p>描述</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">给出一组不重复的整数，返回所有排列。如：</span><br><span class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">Output:</span><br><span class="line">[</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>],</span><br><span class="line">[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>],</span><br><span class="line">[<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">[<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li><p>逻辑</p>
<p>  每一种排列包含3个元素，思路很直接：构建一棵树，树的结点表示形成的一个组合，叶节点表示一个完整的组合。过程中需要一个容器来记录每一个叶节点，即一个排列。还需要一个布尔型容器来记录已经处理过的元素。最后还需要一个容器记录所有找到的排列，即最终返回的结果。过程可以用一棵树的先序遍历完成：</p>
  <div align="center"><img src="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/tree.png" width></div>

<p>  图中橘红色箭头表示程序执行过程。<font color="green" size="4">体会递归“跳进跳出”的执行方式，每到“触底反弹”，便体现了回溯的“回”，所有变量值均回到上一层</font>。递归算法很”整齐”，所有结点执行<strong>相同</strong>的操作。</p>
</li>
<li><p>实现</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// res 记录所有排列，最终返回res</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line"><span class="comment">// used 记录检查过的元素</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; used;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个函数找到一个排列, num是输入，index表示当前考察元素的Index，p表示逐渐形成的一个排列</span></span><br><span class="line"><span class="comment">// 向这个排列的末尾添加第index个元素，获得一个有Index个元素的排列。</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getPermutaion</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> index, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; p)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归到底的情况，所有元素都考察过之后。</span></span><br><span class="line">    <span class="keyword">if</span> (index == nums.size())&#123;</span><br><span class="line">        res.push_back(p);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 以每一个元素作为这棵树的根：</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.size(); i++)&#123;</span><br><span class="line">        <span class="comment">// 只有当元素没有考察过，才执行以下</span></span><br><span class="line">        <span class="keyword">if</span> (used[i] == <span class="literal">false</span>)&#123;</span><br><span class="line">            used[i] = <span class="literal">true</span>;        <span class="comment">//</span></span><br><span class="line">            p.push_back(nums[i]);   <span class="comment">// 把这个元素放入p中</span></span><br><span class="line">            getPermutaion(nums, index+<span class="number">1</span>, p);   <span class="comment">// 形成这棵树的子树</span></span><br><span class="line">            p.pop_back();    <span class="comment">// 这里体现了回溯的“回”，回到上一步</span></span><br><span class="line">            used[i]=<span class="literal">false</span>;   <span class="comment">// 回到上一步</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 入口函数</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; permute(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">    used = <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;(nums.size(), <span class="literal">false</span>);</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; p;</span><br><span class="line">    getPermutaion(nums, <span class="number">0</span>, p);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><font color="gree" size="5">敲黑板</font></p>
</li>
</ul>
<ol>
<li>思维：跳进跳出</li>
<li>实现：跳进跳“回”</li>
<li>明确(写出)结点函数的定义，并且保持整个过程定义不变。</li>
</ol>
<h2 id="组合问题"><a href="#组合问题" class="headerlink" title="组合问题"></a>组合问题</h2><h1 id="77"><a href="#77" class="headerlink" title="#77"></a>#77</h1><ul>
<li><p>描述：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">从n个数中取k个数，一共有哪些组合：</span><br><span class="line">Input: n &#x3D; 4, k &#x3D; 2</span><br><span class="line">Output:</span><br><span class="line">[</span><br><span class="line">[2,4],</span><br><span class="line">[3,4],</span><br><span class="line">[2,3],</span><br><span class="line">[1,2],</span><br><span class="line">[1,3],</span><br><span class="line">[1,4],</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li><p>逻辑</p>
<p>  分析问题：<br>  开始，根节点中不存在任何值，它的子节点从1开始遍历，形成组合中的第一个值<code>[1], [2], [3], [4]</code>。<br>  当结点第一个值为<code>1</code>时，它的子节点从<code>2</code>开始向后遍历。形成的组合有<code>[1,2], [1,3], [1,4]</code>。<br>  当结点第一个值为<code>2</code>时，其子节点从<code>3</code>开始遍历。得到组合<code>[2,3], [2,4]</code>。<br>  当结点第一个值为<code>3</code>时，其子节点从<code>4</code>开始遍历。得到组合<code>[3,4]</code>。<br>  当结点第一个值为<code>4</code>时，<code>4</code>超过了索引<code>0~3</code>，返回到根节点。</p>
<p>  给出递归树：</p>
  <div align="center"><img src="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/tree2.png"></div>
</li>
<li><p>实现</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// res保存所有的组合</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义：从n个数中取k个数，把当前的数值放入c中，从Index开始向后查找：</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">generatCombination</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> k, <span class="keyword">int</span> index, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; c)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 当c的大小为2时，表示找到一个组合</span></span><br><span class="line">    <span class="keyword">if</span> (c.size()==k)&#123;</span><br><span class="line">        res.push_back(c);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=index; i&lt;n; i++)&#123;</span><br><span class="line">        c.push_back(i);</span><br><span class="line">        <span class="comment">// 以当前结点为根，从index+1开始向后找：</span></span><br><span class="line">        generatCombination(n, k, index+<span class="number">1</span>, c);</span><br><span class="line">        c.pop_back();   <span class="comment">// 回溯的“回”，跳回上一层。</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; combination(<span class="keyword">int</span> n, <span class="keyword">int</span> k)&#123;</span><br><span class="line">    <span class="keyword">if</span> (n&lt;k || n&lt;=<span class="number">0</span>||k&lt;=<span class="number">0</span> )</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; c;</span><br><span class="line">    <span class="comment">// 从根节点开始，</span></span><br><span class="line">    generatCombination(n, k, <span class="number">1</span>, c);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>  这个问题的实现中，在递归函数里的for循环，循环变量i与index有关，表示从Index后查找，这保证了，组合中元素无重复，且组合无重复。这也是与上一个问题不同之处。可以回过去看排列问题，其递归函数中for循环i与Index无关，表示，i每次从0开始查找，使得每个排列中元素不必只是递增，就是说像<code>[3,2,1]</code>，也是一个排列。</p>
</li>
</ul>
<p><font color="gree" size="5">敲黑板</font>体会递归函数中for循环循环变量与index有关，无关的不同。</p>
<h1 id="40-Combination-Sum-II"><a href="#40-Combination-Sum-II" class="headerlink" title="#40. Combination Sum II"></a>#40. Combination Sum II</h1><ul>
<li><p>描述</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input: candidates &#x3D; [2,5,2,1,2], target &#x3D; 5,</span><br><span class="line">A solution set is:</span><br><span class="line">[</span><br><span class="line">[1,2,2],</span><br><span class="line">[5]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
</li>
<li><p>思路</p>
<p>  感觉上，需要回溯，所以先画出递归树：</p>
<p>  假设<code>candidate=[1,2,3,4,5]</code>, <code>target=5</code>。</p>
  <div align="center"><img src="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/40.png"></div>
</li>
<li><p>实现</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; combinationSum2(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candidates, <span class="keyword">int</span> target)&#123;</span><br><span class="line">    <span class="built_in">set</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp;   <span class="comment">// res中的每个元素</span></span><br><span class="line">    sort(candidates.begin(), candidates.end());</span><br><span class="line">    DFS(candidates, target, res, tmp ,<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> &#123;res.begin(), res.end()&#125;;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每一个结点的操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candidates, <span class="keyword">int</span> target, <span class="built_in">set</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; res, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; tmp, <span class="keyword">int</span> index)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (target==<span class="number">0</span>)&#123;   <span class="comment">// 如果最后剩下为0，则表示找到一个sum为target</span></span><br><span class="line">        res.insert(tmp);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=index; i&lt;candidates.size(); i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (candidates[i]&lt;=target)&#123;</span><br><span class="line">            tmp.push_back(candidates[i]);</span><br><span class="line">            DFS(candidates, target-candidates[i], res, tmp, i+<span class="number">1</span>);</span><br><span class="line">            tmp.pop_back();</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><font color="gree" size="5">敲黑板</font>一定要<strong>先</strong>画递归树，<strong>后</strong>写code，试图从别人的code中画递归树，是很容易懵掉的。</p>
</li>
</ul>
<h1 id="515-Find-Largest-Value-in-Each-Tree-Row"><a href="#515-Find-Largest-Value-in-Each-Tree-Row" class="headerlink" title="#515 Find Largest Value in Each Tree Row"></a>#515 Find Largest Value in Each Tree Row</h1><ul>
<li><p>描述</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">You need to find the largest value in each row of a binary tree.</span><br><span class="line">Example:</span><br><span class="line"></span><br><span class="line">Input: </span><br><span class="line"></span><br><span class="line">        1</span><br><span class="line">        &#x2F; \</span><br><span class="line">        3   2</span><br><span class="line">    &#x2F; \   \</span><br><span class="line">    5   3   9 </span><br><span class="line"></span><br><span class="line">Output: [1, 3, 9]</span><br></pre></td></tr></table></figure>
</li>
<li><p>逻辑</p>
<p>  先画二叉树，见下图。</p>
<p>  本质是二叉树的遍历，先序遍历，顺序为下图中<font color="pink" size="4">粉色</font>箭头。而对于每个结点的操作是下图中<font color="orange" size="4">橙色</font>箭头。每个操作改变的是res数组，根据res的长度与row的索引决定。</p>
  <div align="center"><img src="/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/515.png"></div>



</li>
</ul>
<ul>
<li><p>实现</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; largestValues(TreeNode* root)&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    DFS(root, <span class="number">0</span>, res);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(TreeNode* root, <span class="keyword">int</span> row, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; res)</span></span>&#123;</span><br><span class="line">    <span class="comment">// operation ORANGE</span></span><br><span class="line">    <span class="keyword">if</span> (!root) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &gt;= res.size())</span><br><span class="line">        res.push_back(root-&gt;val);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        res[row] = max(res[row], root-&gt;val);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// move PINK</span></span><br><span class="line">    DFS(root-&gt;left, row+<span class="number">1</span>, res);</span><br><span class="line">    DFS(root-&gt;right, row+<span class="number">1</span>, res);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>关键： <font color="red" size="4">变量row的跳进跳出</font>,<font color="red" size="4">分清楚操作部分和结点移动部分</font></p>
<h1 id="17-Letter-Combinations-of-a-phone-number"><a href="#17-Letter-Combinations-of-a-phone-number" class="headerlink" title="#17 Letter Combinations of a phone number"></a>#17 Letter Combinations of a phone number</h1><h1 id="491-All-increasing-Sub-sequences"><a href="#491-All-increasing-Sub-sequences" class="headerlink" title="#491 All increasing Sub-sequences"></a>#491 All increasing Sub-sequences</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/22/LeetCode-%E6%96%B9%E6%B3%95%E8%AE%BA-%E5%9B%9E%E6%BA%AF%E6%B3%95/" data-id="ckatsrgu5007lxqfz96ry69tn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-线性模型-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-22T07:02:28.000Z" itemprop="datePublished">2019-08-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/">线性模型-(一)-最小二乘</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>线性模型形式简单，具有很好的可解释性。虽然简单却蕴藏着重要的机器学习的基本思想。一般实践中，都会先使用线性回归方法尝试。</li>
<li>把分类问题的样本点放在在坐标轴上，坐标轴每一个轴代表一个特征。而回归问题的样本点放到坐标轴上，<strong>x轴是连续的，可以是其中一个特征的连续值</strong>，其他轴代表不同的特征。</li>
<li>线性回归的目标是度量出模型没有拟合住样本点的部分，此时的目标函数称为 loss fucntion。</li>
<li>而有的算法中度量的是拟合的程度，此时成目标函数为效用函数 utility function 。</li>
<li>通过分析问题，确定问题的目标函数，通过最优化目标(最小化loss 或最大化utility)，获得机器学习模型。</li>
<li>为参数学习，找到一组参数，这组参数可以最小化loss 或最大化utility。涉及到<strong>最优化原理</strong>或<strong>凸优化理论</strong>，如常见的牛顿法，梯度下降，模拟退火等算法。许多计算机问题如最短路径，背包问题都属于最优化问题。</li>
<li>对于(一元或多元)线性回归，直接用最小二乘法求解。</li>
<li>使用线性模型的前提，是假设数据与目标间由线性关系。</li>
</ul>
<h1 id="最小二乘法解一元线性回归"><a href="#最小二乘法解一元线性回归" class="headerlink" title="最小二乘法解一元线性回归"></a>最小二乘法解一元线性回归</h1><p>基于<strong>最小化均方误差</strong>来进行模型求解的方法为<strong>最小二乘法</strong>，即试图找一条直线，使得所有样本点到该直线的欧氏距离之和最小。具体的结果是经过数学推导，而非迭代参数学习。</p>
<p>数据为一维向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 小写X表示这是一个向量</span></span><br><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,])</span><br><span class="line">y = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">9</span>,<span class="number">11</span>,])</span><br></pre></td></tr></table></figure>

<p>根据最小二乘法结果公式的得：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最小二乘法</span></span><br><span class="line">x_bar = np.mean(x)</span><br><span class="line">y_bar = np.mean(y)</span><br><span class="line"></span><br><span class="line">fenzi = <span class="number">0.0</span></span><br><span class="line">fenmu = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> zip(x, y):</span><br><span class="line">    fenzi += (i - x_bar) * (j - y_bar)</span><br><span class="line">    fenmu += (i - x_bar)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">a = fenzi / fenmu</span><br><span class="line">b = y_bar - a * x_bar</span><br></pre></td></tr></table></figure>

<p>绘制拟合直线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制直线</span></span><br><span class="line">y_hat = a*x + b</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, y_hat, color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">12</span>, <span class="number">0</span>, <span class="number">12</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<div align="center"><img src="/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/linear.png" width="500"></div>

<p>使用模型，分别预测单个值，预测一组值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测单个值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测多个值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_X</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.asarray([a * x + b <span class="keyword">for</span> x <span class="keyword">in</span> X])</span><br></pre></td></tr></table></figure>

<p>上述计算过程是遍历每一个x和y，部分相乘后相加，这样计算的效率并不高。另一种方式是使用矩阵的乘法，即<strong>内积(Dot Product 点乘)</strong></p>
<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>使用矩阵的内积，便可以用python中的向量相乘法则快速计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_mean = np.mean(x_train)</span><br><span class="line">y_mean = np.mean(y_train)</span><br><span class="line"></span><br><span class="line">a = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean)</span><br><span class="line">b = y_mean - a * x_mean</span><br></pre></td></tr></table></figure>

<p>用内积代替了循环遍历，计算性能上会有很大提升。对于1亿的数据量，两者使用时间：</p>
<pre><code>循环遍历： 55.904422
内积：    2.4315210000000036</code></pre><p>很多时候，<font color="red" size="4">算法原理的数学推导，最终要能变化成向量内积的形式，很重要</font>。</p>
<h1 id="评价现行模型的性能"><a href="#评价现行模型的性能" class="headerlink" title="评价现行模型的性能"></a>评价现行模型的性能</h1><p>评价线性模型主要用MSE, RMSE, MAE:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公式：</span></span><br><span class="line">mse = np.sum((y_predict - y_test)**<span class="number">2</span> / len(y_test))</span><br><span class="line">rmse = math.sqrt(mse)</span><br><span class="line">mae = np.sum(np.absolute(y_predict - y_test)) / len(y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br></pre></td></tr></table></figure>

<p>相对来说，RMSE比MAE好，最小化RMSE，它表示错误样本中最大的错误值相应的比较小。而在线性回归中最好的指标要数R Squared。</p>
<h1 id="R-Squared"><a href="#R-Squared" class="headerlink" title="R Squared"></a>R Squared</h1><p>这个是线性回归最常用的性能指标。其定义为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r_squared = <span class="number">1</span> - (y_predict - y_test).dot(y_predict - y_test)</span><br><span class="line">               /</span><br><span class="line">               (y_means - y_test).dot(y_means - y_test)</span><br></pre></td></tr></table></figure>

<p>其中分子可以表示，使用我们的模型预测产生的错误。<br>而分母表示使用baseline 模型y=y_means 预测产生的错误。</p>
<p>分母的预测是不论x是多少，我都把他预测成y_means，错误率自然多。而分子是我们训练模型的实际预测，即我的模型实际拟合住样本的地方。</p>
<p>所以</p>
<ul>
<li>r_squared 是我的模型与baseline的比较。</li>
<li>两者相除小于等于1，</li>
<li>r_squared 越大，表示我们的模型越好</li>
<li>当r_squared = 1，表示，我的模型没有犯任何错。</li>
<li>当r_squared = 0，表示，我的模型性能等同于baseline。</li>
<li>当r_squared &lt; 0，表示，我的模型不如baseline，很可能，原数据不存在现<strong>线性关系</strong>。</li>
</ul>
<p>当r_squared 定义中分子分母同时除以测试样本个数，r_squared还可以写成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r_squared = <span class="number">1</span> - mse(y_predict, y_test)/var(y_test)</span><br></pre></td></tr></table></figure>
<p>其中var(y_test)，表示y_test 数据方差。所以r_squared 是由统计意义的。</p>
<p>在sklearn中使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"></span><br><span class="line">r2_score(y_predict, y_test)</span><br></pre></td></tr></table></figure>
<p>在sklearn中的LinearRegression模型里的 score成员方法返回的就是r_squared：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MODEL 表示任何模型的实例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(x_test, y_test)</span>:</span></span><br><span class="line">    y_predict = MODEL.predict(x_test)</span><br><span class="line">    <span class="keyword">return</span> r2_score(y_test, y_predict)</span><br></pre></td></tr></table></figure>

<h1 id="多元线性回归与Normal-Equation"><a href="#多元线性回归与Normal-Equation" class="headerlink" title="多元线性回归与Normal Equation"></a>多元线性回归与Normal Equation</h1><p>依旧可以使用最小二乘法得到最终解，即normal equation。但是计算机求解normal equation 的时间复杂度为O(n^3)。优点是根据数学解方程得到，不需要归一化处理。</p>
<p>sklearn中多元线性回归套路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line"><span class="comment"># print(boston.DESCR)  # (503, 13)</span></span><br><span class="line"><span class="comment"># print(boston.feature_names)</span></span><br><span class="line"></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除去异常点</span></span><br><span class="line">X = X[y &lt; <span class="number">50</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(lin_reg.coef_)      <span class="comment">#  得到最终模型所有参数</span></span><br><span class="line">print(lin_reg.intercept_)  <span class="comment"># 包括截距</span></span><br><span class="line">print(lin_reg.score(X_test, y_test))   <span class="comment"># 应用在测试数据集上的得分</span></span><br></pre></td></tr></table></figure>

<p>实现normal equation：</p>
<p>首先在原本X_train中加入一列全为1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br></pre></td></tr></table></figure>

<p>根据normal equation 的公式，其中intercept_表示直线上的截距，截距与样本特征无关，或者说，截距对应的特征是常数1。coef_表示coefficient，每个特征的系数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)  <span class="comment"># 完全根据公式</span></span><br><span class="line"></span><br><span class="line">intercept_ = _theta[<span class="number">0</span>]  <span class="comment"># 参数的第一列为截距</span></span><br><span class="line">coef_ = _theta[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>

<p>有了参数，就可以用于测试样本。写成函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X_sample)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> intercept_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> coef_ <span class="keyword">in</span> <span class="keyword">not</span> <span class="literal">None</span>, \</span><br><span class="line">        <span class="string">"Must fit before predict"</span></span><br><span class="line">    <span class="keyword">assert</span> X_sample.shape[<span class="number">1</span>] == len(coef_), \</span><br><span class="line">        <span class="string">"the feature number of X_sample must be equal to the lenght of coef_"</span></span><br><span class="line">    </span><br><span class="line">    X_b = np.hstack([np.ones((len(X_sample), <span class="number">1</span>)), X_sample])</span><br><span class="line">    <span class="keyword">return</span> X_b.dot(_theta)</span><br></pre></td></tr></table></figure>
<p>创建样本，使用模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_sample = np.array([[<span class="number">0.5</span>, <span class="number">20.</span>, <span class="number">4.</span>, <span class="number">0.</span>, <span class="number">0.57</span>, <span class="number">7.</span>, <span class="number">52.</span>, <span class="number">2.8</span>, <span class="number">5.</span>, <span class="number">264.</span>, <span class="number">13.</span>, <span class="number">390.</span>, <span class="number">3.16</span>]])</span><br><span class="line"></span><br><span class="line">lr.predict(X_sample)  <span class="comment"># 返回模型认为的预测值</span></span><br></pre></td></tr></table></figure>
<h2 id="线性回归的可解释性"><a href="#线性回归的可解释性" class="headerlink" title="线性回归的可解释性"></a>线性回归的可解释性</h2><p>对所有数据解normal equation 方程得所有系数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ -1.05574295e-01   3.52748549e-02  -4.35179251e-02   4.55405227e-01</span><br><span class="line">  -1.24268073e+01   3.75411229e+00  -2.36116881e-02  -1.21088069e+00</span><br><span class="line">   2.50740082e-01  -1.37702943e-02  -8.38888137e-01   7.93577159e-03</span><br><span class="line">  -3.50952134e-01]</span><br></pre></td></tr></table></figure>
<p>系数的正负表示，这个特征与预测指标(如房价)是正相关还是负相关。<br>正值表示正相关，越大表示这个特征越大房价越高。<br>负值表示负相关，越大表示这个特征越大，房价越低。 <br>系数的绝对值的大小表示了该特征对房价的影响程度。可以把特征的值从小到大排列，分析什么特征的影响大，什么特和那个的影响小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按系数排序，返回排序后的索引</span></span><br><span class="line">sort_index = np.argsort(lin_reg2.coef_)</span><br><span class="line"><span class="comment"># 使用排序后的索引，给特征名称排序 </span></span><br><span class="line">boston.feature_names[sort_index]</span><br></pre></td></tr></table></figure>
<p>比如，最大值系数表示房间数目。房间数目与房价正相关，且越大房价越高。合理。<br>最小系数对应一氧化氮浓度。浓度与房价成负相关，且值越大，房价越低。合理。</p>
<p>这就是“现行模型的可解释性”，有针对的采集更多数据，帮助决策。 </p>
<p><strong>注意</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])  </span><br><span class="line">np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
<p>前者是向量，大小为： (8,)<br><br>后者是矩阵，大小为： (2, 8)</p>
<p><font color="gree" size="5">敲黑板</font>机器学习算法实现不是目的，一个算法的优劣是将它放在特定任务中，与其他算法比较得出的。也就是说，单单训练好一个模型，还没结束，而是与其他训练好的模型一起评价，比如使用由confusion matrix 得到的指标：查准率，查全率，F-alpha，P-R曲线， ROC曲线等。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/22/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E4%B8%80/" data-id="ckatsrgvd008exqfz5a1h4bst" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-集成学习-Boosting-四" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Boosting-%E5%9B%9B/" class="article-date">
  <time datetime="2019-08-21T07:03:16.000Z" itemprop="datePublished">2019-08-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Boosting-%E5%9B%9B/">集成学习-Boosting-(四)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>之前说过，集成学习分为两类：学习器间无关系，和基学习器间有关系。前者常使用Bagging 和随机森林。而后者常使用Boosting。Boosting的工作机制如下：<strong>先从训练集中训练出一个基学习器，然后根据这个学习器的表现调整样本分布，使得先前的学习器分类错误的样本在后续收到更多关注，最后基于调整后的样本训练下一个学习器。如此反复</strong>。也就是说，每个基学习器都在尝试提升整体效果。可以看出，Boosting不能并行执行。</p>
<p>常见两个Boosting</p>
<ol>
<li>Ada Boosting</li>
<li>Gradient Boosting</li>
</ol>
<h1 id="Ada-Boosting"><a href="#Ada-Boosting" class="headerlink" title="Ada Boosting"></a>Ada Boosting</h1><p>Boosting算法最著名的代表是AdaBoost：其过程如下：</p>
<div align="center"><img src="/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Boosting-%E5%9B%9B/ada.png" width="600"></div>
<div align="center">图 多个学习器串行，调整样本权值</div>

<p>初始样本集每个样本有一个权值，当第一个学习器学习完后，对于不能正确捕捉的样本，调整这些样本的权值。然后第二个学习器接着学习，对于不能正确捕捉的样本调整权值。然后让第三个学习器接着学习。如此反复。可以看出，在整个样本的学习过程中，样本的权值在不断变化。<br>那么怎样给样本点附上权值，这其实是个<strong>问题转化</strong>，转化为求极值的问题。</p>
<p>sklearn中使用<code>AdaBoostClassifier()</code>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x, y = datasets.make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.4</span>, random_state=<span class="number">111</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建adaboost 模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=<span class="number">2</span>),</span><br><span class="line">                         n_estimators=<span class="number">100</span>)</span><br><span class="line">model = ada.fit(x_train, y_train)</span><br><span class="line">print(model)</span><br><span class="line">print(model.score(x_test, y_test))</span><br></pre></td></tr></table></figure>

<p>在测试集上结果：92.0%</p>
<h1 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h1><p>Gradient Boosting 是另外一种思路，它<strong>仅以决策树</strong>为基学习器。过程如下图：</p>
<div align="center"><img src></div>

<p>对于所有样本，训练第一个模型<code>m1</code>，得到所有分类错误样本。对于上一步中的所有错误分类样本，训练第二个模型<code>m2</code>， 得到这次分类错误样本。如此反复。最终的模型<code>m</code>等于这个过程中所有模型之和：<code>m = m1 + m2 + m3 + m4 + ...</code>。</p>
<p>sklearn使用<code>GradientBoostingClassifier()</code>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同样的数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient Boosting使用决策树为基学习器：</span></span><br><span class="line">gb = GradientBoostingClassifier(max_depth=<span class="number">2</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line">gb.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(gb.score(x_test, y_test))</span><br></pre></td></tr></table></figure>
<p>模型在测试集上的性能： 88.0%</p>
<p>Gradient Boosting 其实是一种<font color="red">残差学习</font>，每一个学习器并不是学习整个样本集，只学习错误分类集。</p>
<p>同样的，Ada Boosting 和 Gradient Boosting 也可以用来处理回归任务。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Boosting-%E5%9B%9B/" data-id="ckatsrgth006axqfzbizxb1d1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-集成学习-三-随机森林" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E4%B8%89-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" class="article-date">
  <time datetime="2019-08-21T06:04:54.000Z" itemprop="datePublished">2019-08-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E4%B8%89-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">集成学习-(三)-随机森林</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="为什么随机森林强"><a href="#为什么随机森林强" class="headerlink" title="为什么随机森林强"></a>为什么随机森林强</h1><p>当集成学习的基学习器均为决策树时，称随机森林，是Bagging的一种变体。决策树在<font color="orange">选择划分属性</font>时，是在当前属性集合中选择最优属性；<font color="red">而在随机森林中，对每个基决策树的每个结点，从该节点的属性集合中随机选择一个包含<code>k</code>个属性的子集，再从这个子集中选择最优的属性用作划分</font>。</p>
<p><font color="red">这个<code>k</code>控制了随机性的程度</font>：当k值等于当前结点的属性集合中所有属性时，表示随机性为0；当k=1时，则表示，在当前结点的属性集合中随机选取一个，之后就使用这一个作为划分，不管它是好是坏(因为没得选，只有这一个)。一般情况下，选取k为<code>log(d)</code>, 以2为底数。</p>
<p>为什么随机森林可以“代表集成学习技术”，Bagging中的基学习器<font color="orange">多样性</font>是仅仅通过数据的扰动达到的。而随机森林的多样性，<font color="red">不仅来源于数据扰动，还来自属性的扰动</font>。</p>
<p>实现随机森林：</p>
<h1 id="例"><a href="#例" class="headerlink" title="例"></a>例</h1><p>使用moon数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x, y = datasets.make_moons(n_samples=<span class="number">1000</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">111</span>)</span><br></pre></td></tr></table></figure>

<p>构建一片由500棵树的森林：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">500</span>,</span><br><span class="line">                            random_state=<span class="number">233</span>,</span><br><span class="line">                            oob_score=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = rf.fit(x, y)</span><br><span class="line"><span class="comment"># 查看模型参数</span></span><br><span class="line">print(model)</span><br><span class="line"><span class="comment"># 使用oob，用剩下的样本做测试</span></span><br><span class="line">print(model.oob_score_)</span><br></pre></td></tr></table></figure>

<p>模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(bootstrap=<span class="literal">True</span>, class_weight=<span class="literal">None</span>, criterion=<span class="string">'gini'</span>,</span><br><span class="line">            max_depth=<span class="literal">None</span>, max_features=<span class="string">'auto'</span>, max_leaf_nodes=<span class="literal">None</span>,</span><br><span class="line">            min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>,</span><br><span class="line">            min_samples_leaf=<span class="number">1</span>, min_samples_split=<span class="number">2</span>,</span><br><span class="line">            min_weight_fraction_leaf=<span class="number">0.0</span>, n_estimators=<span class="number">500</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">            oob_score=<span class="literal">True</span>, random_state=<span class="number">233</span>, verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>可以看出随机森林由那些参数，可以如何设置。<font color="red" size="3">理解了算法原理，才能理解每个参数如何使用</font>。测试集上准确率为95.6%.</p>
<h1 id="Extra-Tree"><a href="#Extra-Tree" class="headerlink" title="Extra-Tree"></a>Extra-Tree</h1><p>还有一种特殊随机森林：Extra-Tree，与经典森林不同的是，这种森林在结点划分时，完全随机，也就是说在当前节点的<font color="red">所有属性里随机选择一个，而非选择最优的</font>。如此一来，森林中的每一棵树的就更加不同了，即多样性更强了。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line">etree = ExtraTreesClassifier(n_estimators=<span class="number">500</span>,</span><br><span class="line">                             bootstrap=<span class="literal">True</span>,</span><br><span class="line">                             oob_score=<span class="literal">True</span>,</span><br><span class="line">                             random_state=<span class="number">233</span>)</span><br><span class="line"></span><br><span class="line">model2 = etree.fit(x, y)</span><br><span class="line">print(model2)</span><br><span class="line">print(model2.oob_score_)</span><br></pre></td></tr></table></figure>

<p>结果 95.5%</p>
<p>回顾了Bagging，随机森林，Extra-Tree 都进行了分类任务的应用。也可以将这些集成用于回归任务。</p>
<p><font color="gree" size="5">敲黑板</font>样本数量的扰动，样本属性的扰动，增强不同决策树的<font color="red">多样性</font>。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/21/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E4%B8%89-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" data-id="ckatsrgtk006jxqfzdinf39uk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-集成学习-ensemble-learning-二" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/" class="article-date">
  <time datetime="2019-08-20T15:43:46.000Z" itemprop="datePublished">2019-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/">集成学习-(二)-Bagging</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>集成学习可以分为两大类：</p>
<ul>
<li>学习器之间存在依赖关系，必须串行生成序列化方法。</li>
<li>学习器之间不存在依赖关系，可以同时生成的并行化方法。</li>
</ul>
<p>前者的代表是“Boosting”，后者的代表是“Bagging”和“随机森林(Random Forest)”.</p>
<p>得到泛化能力强的集成，每个学习器要尽量不同，如何做到不同。一个方法是由训练集产生多个不同的子集，在每个子集上训练学习器。如5000个样本，用5个学习器分别学习1000个样本集。如此产生5个不同的模型。但是如此一来，每个模型的性能会有所下降，同时，集成学习的一个优势是每个学习器并不需要具有很强的性能。</p>
<h1 id="从数据中采样"><a href="#从数据中采样" class="headerlink" title="从数据中采样"></a>从数据中采样</h1><p>但是，“好而不同”毕竟每个学习器要“好”。所以根据每个学习器只使用数据的一部分，产生两种采样方式：</p>
<ul>
<li>有放回抽样 自助采样（Bootstrap Sampling）</li>
<li>无放回抽样</li>
</ul>
<p>自助采样：假设原始数据集由m个样本，对于第一个学习器，随机采样一个样本，放入采样集中，后把该样本放回原数据集。如此采样m次便得到含有m个样本的子集来训练学习器<code>#1</code>。对于其他学习器，采用同样的方式得到训练集。如此得到的不同子集中一定存在重复的元素。最后基于每一个子集训练学习器，后集成。此过程成为<code>Bagging</code>。</p>
<p>对于无放回抽样，就如上述所述，5000个样本是数据集，若分给5个学习器，每个得到1000个样本；10 个学习器，每个得到500个样本。这种方法成为<code>Pasting</code>。其局限性很显然，学习器个数与其子集样本数成反比。</p>
<p>Bagging常用。例：<br>所使用数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x, y = datasets.make_moons(n_samples=<span class="number">1000</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">111</span>)</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">111</span>)</span><br></pre></td></tr></table></figure>

<p>使用决策树为基学习器，设置<code>5</code>个，采样为放回采样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"></span><br><span class="line">bagg = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                         n_estimators=<span class="number">5</span>, max_samples=<span class="number">100</span>, bootstrap=<span class="literal">True</span>)</span><br><span class="line">bagg.fit(x_train, y_train)</span><br><span class="line">print(bagg.score(x_test, y_test))</span><br></pre></td></tr></table></figure>

<p>测试集的正确率为：94.4%</p>
<p>设置<code>500</code>个基学习器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bagg2 = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                         n_estimators=<span class="number">500</span>, max_samples=x_train.shape[<span class="number">0</span>], bootstrap=<span class="literal">True</span>)</span><br><span class="line">bagg2.fit(x_train, y_train)</span><br><span class="line">print(bagg2.score(x_test, y_test))</span><br></pre></td></tr></table></figure>

<p>测试集准确率： 96.0%</p>
<h1 id="Out-of-bag-Estimate（OOB）"><a href="#Out-of-bag-Estimate（OOB）" class="headerlink" title="Out-of-bag Estimate（OOB）"></a>Out-of-bag Estimate（OOB）</h1><p>可以计算，<font color="red">通过自助采样，原始数据集中大约有36.8%的样本未出现在每个学习器的样本子集中。所以对于Bagging，天然的，在原始数据集中就有测试集了</font>，即那剩下的原始数据集中的36.7%。</p>
<p>使用sklearn 中的<code>oob_score_</code> 实现<code>oob</code>：<br><font color="red">在放回采样过程中，记录那些样本没有被取到，这些未被取到的作为验证集或测试集</font>，这个过程由oob_score=True 实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bagg3 = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                          n_estimators=<span class="number">500</span>, max_samples=x.shape[<span class="number">0</span>],</span><br><span class="line">                          bootstrap=<span class="literal">True</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">bagg3.fit(x, y)</span><br><span class="line">print(bagg3.oob_score_)</span><br></pre></td></tr></table></figure>

<p>结果0.958。</p>
<h1 id="多核心并行"><a href="#多核心并行" class="headerlink" title="多核心并行"></a>多核心并行</h1><p>由于每个学习器没有相互影响，所以所有学习器可以同时学习：<br>指定<code>n_jobs</code>的值，当其为<code>-1</code>时，表示使用计算机所有物理核心：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 并行执行，使用所有核心</span></span><br><span class="line">bagg4 = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                          n_estimators=<span class="number">500</span>, max_samples=x.shape[<span class="number">0</span>],</span><br><span class="line">                          bootstrap=<span class="literal">True</span>,</span><br><span class="line">                          oob_score=<span class="literal">True</span>,</span><br><span class="line">                          n_jobs=<span class="number">-1</span>)</span><br><span class="line">start2 = clock()</span><br><span class="line">bagg4.fit(x, y)</span><br><span class="line">end2 = clock()</span><br><span class="line">print(bagg4.oob_score_)</span><br><span class="line">print(end2 - start2)</span><br></pre></td></tr></table></figure>
<p>计时得到最终执行时间：0.1591450000000001<br>而使用一个核心的执行时间为：0.6764169999999998</p>
<h1 id="更多采样方式"><a href="#更多采样方式" class="headerlink" title="更多采样方式"></a>更多采样方式</h1><p><font color="red">当数据集的特征较多时，可是对特征进行随机采样</font>：<code>Random Subspaces</code>。只在列上随机采样。如下左图。<br><br>另一种采样方式，既对样本随机采样，又对特征的随机采样：<code>Random Patches</code>。既在行又在列上随机采样。如下右图：</p>
<div align="center"><img src="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/sampling.png" width="600"></div>
<div align="center">左Radom Subspaces & 右Random Patches</div>

<p>既然有对样本数量的自助采样（bootstrap sampling），也有对样本特征的自助采样。如此得到的特征为<code>bootstrap features</code>：</p>
<ul>
<li><p>Random Subspaces</p>
<p>  只对特征进行随机采样，将学习机的<code>bootstrap_features</code>参数置为<code>True</code>：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭样数量的随机采样</span></span><br><span class="line">random_subspaces_bag = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                                        n_estimators=<span class="number">500</span>,</span><br><span class="line">                                        max_samples=x.shape[<span class="number">0</span>],  <span class="comment"># 所有行</span></span><br><span class="line">                                        max_features=<span class="number">1</span>,</span><br><span class="line">                                        bootstrap=<span class="literal">True</span>,</span><br><span class="line">                                        bootstrap_features=<span class="literal">True</span>,  <span class="comment"># 一部分列</span></span><br><span class="line">                                        oob_score=<span class="literal">True</span>,</span><br><span class="line">                                        n_jobs=<span class="number">-1</span>)</span><br><span class="line">start3 = clock()</span><br><span class="line">random_subspaces_bag.fit(x, y)</span><br><span class="line">end3 = clock()</span><br><span class="line">print(random_subspaces_bag.oob_score_)</span><br><span class="line">print(end3 - start3)</span><br></pre></td></tr></table></figure>
<p>  结果为： 0.835<br>  运行时间：0.1876460000000002</p>
</li>
</ul>
<ul>
<li><p>Random Patches，</p>
<p>  既对样本数据量采样，又对特征进行采样。因为原始数据只有2个特征，所以此处只采样一个特征。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## random patches 既有样本数的随机采样， 又有特征的随机采样：</span></span><br><span class="line">random_patches_bag = BaggingClassifier(DecisionTreeClassifier(),</span><br><span class="line">                                        n_estimators=<span class="number">500</span>,</span><br><span class="line">                                        max_samples=<span class="number">200</span>,  <span class="comment"># 一部分行</span></span><br><span class="line">                                        max_features=<span class="number">1</span>,</span><br><span class="line">                                        bootstrap=<span class="literal">True</span>,</span><br><span class="line">                                        bootstrap_features=<span class="literal">True</span>,  <span class="comment"># 一部分列</span></span><br><span class="line">                                        oob_score=<span class="literal">True</span>,</span><br><span class="line">                                        n_jobs=<span class="number">-1</span>)</span><br><span class="line">start3 = clock()</span><br><span class="line">random_patches_bag.fit(x, y)</span><br><span class="line">end3 = clock()</span><br><span class="line">print(random_patches_bag.oob_score_)</span><br><span class="line">print(end3 - start3)</span><br></pre></td></tr></table></figure>
<p>  结果为：0.897<br>  运行时间： 0.16168400000000016</p>
<p>  对于图像信息，<font color="red">除了pooling采样操作，还可以使用这两种采样操作</font>。</p>
</li>
</ul>
<p><font color="gree" size="5">敲黑板</font></p>
<pre><code>”好而不同“是集成学习的核心。
”不同“的实现方式之一是”采样“：对样本数量采样，对样本特征采样。
即，使用一部分数据训练基学习器。</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%BA%8C/" data-id="ckatsrgtl006mxqfz51xi2dww" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-集成学习-ensemble-learning-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-20T14:28:32.000Z" itemprop="datePublished">2019-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%B8%80/">集成学习(ensemble-learning)-(一)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>类似，生病了找多名专家从不同方面一起确诊。</li>
<li>集成学习将多个学习器进行结合，通常可以获得比单一的学习器更优的泛化性能。</li>
<li>集成学习可以得带3中不同结果：提升性能，不起作用，起负作用。</li>
<li>所以要想获得好的集成结果，每个学习器应该“好而不同”，即每个学习器要有一定的准确性，同时，每个学习器各不相同。即“多样性”。</li>
<li>实际上，如何产生“好而不同”的基学习器，是集成学习的核心。</li>
</ul>
<h1 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h1><p>使用数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x, y = datasets.make_moons(n_samples=<span class="number">500</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">321</span>)</span><br></pre></td></tr></table></figure>

<p>构建三个基学习器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1） 使用逻辑回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log = LogisticRegression()</span><br><span class="line">log.fit(x_train, y_train)</span><br><span class="line">print(log.score(x_test, y_test))   <span class="comment">#  0.896</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2) 使用SVM</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(x_train, y_train)</span><br><span class="line">print(svm.score(x_test, y_test))   <span class="comment">#  0.952</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3) 使用决策树分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">tree = DecisionTreeClassifier()</span><br><span class="line">tree.fit(x_train, y_train)</span><br><span class="line">print(tree.score(x_test, y_test))  <span class="comment">#  0.944</span></span><br></pre></td></tr></table></figure>

<p>结果分别为 0.896, 0.952, 0.944.</p>
<p>投票操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># voting</span></span><br><span class="line">y_pre1 = log.predict(x_test)</span><br><span class="line">y_pre2 = svm.predict(x_test)</span><br><span class="line">y_pre3 = tree.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 少数服从多数</span></span><br><span class="line"><span class="comment"># 对于三个模型的预测值，至少2个模型判断它为1，我才认为它是1：</span></span><br><span class="line">y_p = np.asarray((y_pre1 + y_pre2 + y_pre3) &gt;= <span class="number">2</span>, dtype=int)</span><br><span class="line">print(y_pre1[:<span class="number">10</span>])   <span class="comment">#  [1 1 1 1 1 0 1 0 0 0]</span></span><br><span class="line">print(y_pre2[:<span class="number">10</span>])   <span class="comment">#  [1 1 1 1 1 0 1 0 0 1]</span></span><br><span class="line">print(y_pre3[:<span class="number">10</span>])   <span class="comment">#  [1 1 1 1 1 0 1 0 0 1]</span></span><br><span class="line">print(y_p[:<span class="number">10</span>])      <span class="comment">#  [1 1 1 1 1 0 1 0 0 1]</span></span><br></pre></td></tr></table></figure>

<p>判断集成效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">acc = accuracy_score(y_test, y_p)</span><br><span class="line">print(acc)     <span class="comment">#  0.96</span></span><br></pre></td></tr></table></figure>

<p>最终集成性能是0.96。</p>
<h2 id="Hard-voting"><a href="#Hard-voting" class="headerlink" title="Hard voting"></a>Hard voting</h2><p>hard voting 即少数服从多数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"></span><br><span class="line">voting = VotingClassifier(estimators=[</span><br><span class="line">    (<span class="string">'log'</span>, LogisticRegression()),</span><br><span class="line">    (<span class="string">'svm'</span>, SVC()),</span><br><span class="line">    (<span class="string">'tree'</span>, DecisionTreeClassifier())</span><br><span class="line">], voting=<span class="string">'hard'</span>)</span><br><span class="line"></span><br><span class="line">voting.fit(x_train, y_train)</span><br><span class="line">print(voting.score(x_test, y_test))    <span class="comment">#  0.96</span></span><br></pre></td></tr></table></figure>

<p>实际中，<font color="red">通常把基学习器调参到最优，后再集成</font>。</p>
<h2 id="Soft-voting"><a href="#Soft-voting" class="headerlink" title="Soft voting"></a>Soft voting</h2><p>hard voting 其实是“少数服从多数”，而 soft voting <font color="red">带权投票</font>，如歌唱的专业评审团的投票权重就大。如：</p>
<p>5个学习器把同一个样本分为A类或B类的概率分别如下：</p>
<pre><code>            A类    B类
学习器#0   99.0%  1.0%
学习器#1   49.0%  51.0%
学习器#2   43.0%  57.0%
学习器#3   98.0%  2.0%
学习器#4   34.0%  64.0%</code></pre><p>如果使用hard voting 该样本最终本分为B类。</p>
<p>但是，显然学习器<code>#0</code>和<code>#3</code>有很大的把握认为该样本属于A类，而其他三个学习器并没有很确定该样本的类别。所以“少数服从多数”不合适。使用Soft voting 计算<br>属于A类：(99.0%+49.0%+43.0%+98.0%+34.0%) / 5 = 64.6%. <br>属于B类：(1.0%+51.0%+57.0%+2.0%+64.0%) / 5 = 35.0%.<br> 所以该样本应该被分类为A。</p>
<p><font color="red">使用 Soft voting 的基学习器要求都应该估计概率</font>，即可以调用 predict_proba 函数。SVM模型把probability设为true，便可以进行概率估计。调用VotingClassifier时指明voting方式为soft即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line">voting2 = VotingClassifier(estimators=[</span><br><span class="line">    (<span class="string">'log'</span>, LogisticRegression()),</span><br><span class="line">    (<span class="string">'svm'</span>, SVC(probability=<span class="literal">True</span>)),</span><br><span class="line">    (<span class="string">'tree'</span>, DecisionTreeClassifier())</span><br><span class="line">], voting=<span class="string">'soft'</span>)</span><br><span class="line"></span><br><span class="line">voting2.fit(x_train, y_train)</span><br><span class="line">print(voting2.score(x_test, y_test))   <span class="comment">#  0.976</span></span><br></pre></td></tr></table></figure>

<p>所以，当基学习器可以求出样本概率估计时，Soft voting 比 hard voting 性能优。 </p>
<p><font color="green" size="5">敲黑板</font>理解当前问题，才能找到已有方法的不足，才能找到更合适的方法。就如本文表达的用Soft voting 替代已有的Hard voting。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/20/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-ensemble-learning-%E4%B8%80/" data-id="ckatsrgti006dxqfz76k8gph2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回顾决策树-一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/" class="article-date">
  <time datetime="2019-08-19T16:00:11.000Z" itemprop="datePublished">2019-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/">回顾决策树(一)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>决策树是非参数学习算法</li>
<li>天然解决多分类问题</li>
<li>有很好的可解释性</li>
<li><font color="red">关键问题是使用哪个特征做为根节点</font></li>
<li>对于连续值的特征，在哪个值上做划分</li>
</ul>
<h1 id="离散数据集"><a href="#离散数据集" class="headerlink" title="离散数据集"></a>离散数据集</h1><p>使用iris数据集，调用sklearn的Decision Tree 模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只是用数据2个特征</span></span><br><span class="line">x = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">tree_clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>, criterion=<span class="string">"entropy"</span>)</span><br><span class="line">tree_clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(tree_clf, axis=[<span class="number">0.5</span>, <span class="number">7.5</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">0</span>, <span class="number">0</span>], x[y == <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">2</span>, <span class="number">0</span>], x[y == <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">1</span>, <span class="number">0</span>], x[y == <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<div align="center"><img src="/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/tree1.png" width="500"></div>
<div align="center">图 深度为2的决策树 </div>

<p>划分后使得系统的<font color="red">熵</font>（即不确定性）降低。所以对于一个划分，如果划分后的系统信息熵比其他划分后的熵都要小，则当前就是用这个划分。根据这个原理，可以就特征为连续值的数据集进行划分：</p>
<h1 id="连续值的数据集"><a href="#连续值的数据集" class="headerlink" title="连续值的数据集"></a>连续值的数据集</h1><p>已知特征d，和value， 对x进行划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(x, y, d, value)</span>:</span></span><br><span class="line">    index_a = (x[:, d] &lt;= value)</span><br><span class="line">    index_b = (x[:, d] &gt; value)</span><br><span class="line">    <span class="keyword">return</span> x[index_a], x[index_b], y[index_a], y[index_b]</span><br></pre></td></tr></table></figure>

<p>传入label列表，求此时的系统信息熵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_entropy</span><span class="params">(y)</span>:</span></span><br><span class="line">    <span class="comment"># counter 为字典，[类别，这个类别所含样本数]</span></span><br><span class="line">    counter = Counter(y)  </span><br><span class="line">    res = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> counter.values():</span><br><span class="line">        p = num / len(y)</span><br><span class="line">        res += -p * log2(p)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>定义一次划分，即，划分算法：搜索找到使得熵最小的特征d和value：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_split</span><span class="params">(x, y)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始值最大</span></span><br><span class="line">    best_entropy = float(<span class="string">'inf'</span>)  </span><br><span class="line">    <span class="comment"># best_e_l, best_e_r = -1, -1</span></span><br><span class="line">    <span class="comment"># 初始化d 和 value</span></span><br><span class="line">    best_d, best_v = <span class="number">-1</span>, <span class="number">-1</span>   </span><br><span class="line">    <span class="comment"># 在x 的所有维度（特征）搜索d：</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="comment"># 在特征d的哪个值上划分：先排序，后找相邻的样本在特征d上的中间值是多少</span></span><br><span class="line">        sorted_index = np.argsort(x[:, d])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(x)):  <span class="comment"># 遍历所有样本</span></span><br><span class="line">            <span class="keyword">if</span> x[sorted_index[i<span class="number">-1</span>], d] != x[sorted_index[i], d]:</span><br><span class="line">                v = (x[sorted_index[i<span class="number">-1</span>], d] + x[sorted_index[i], d]) / <span class="number">2</span></span><br><span class="line">                <span class="comment"># 有了 d 和 v， split：</span></span><br><span class="line">                x_l, x_r, y_l, y_r = split(x, y, d, v) </span><br><span class="line">                <span class="comment"># 此时系统熵是多少</span></span><br><span class="line">                e = cal_entropy(y_l) + cal_entropy(y_r)</span><br><span class="line">                <span class="comment"># e_l = cal_entropy(y_l)</span></span><br><span class="line">                <span class="comment"># e_r = cal_entropy(y_r)</span></span><br><span class="line">                <span class="comment"># 判断新的熵e是否小于best_entropy，更新best_entropy</span></span><br><span class="line">                <span class="keyword">if</span> e &lt; best_entropy:</span><br><span class="line">                    best_entropy, best_v, best_d = e, v, d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_entropy, best_d, best_v</span><br></pre></td></tr></table></figure>

<p>调用函数得第一次划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一次划分：</span></span><br><span class="line">best_entropy, best_d, best_v = try_split(x, y)</span><br><span class="line">x1_l, x1_r, y1_l, y1_r = split(x, y, best_d, best_v)</span><br><span class="line"></span><br><span class="line">print(cal_entropy(y1_l))  <span class="comment"># 0.0  </span></span><br><span class="line">print(cal_entropy(y1_r))  <span class="comment"># 1.0</span></span><br></pre></td></tr></table></figure>

<p>左子树的熵为0，右子树的熵大于0，所以对右子树进行第二次划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于右边再次划分：</span></span><br><span class="line">best2_entropy, best2_d, best2_v = try_split(x1_r, y1_r)</span><br><span class="line">x2_l, x2_r, y2_l, y2_r = split(x1_r, y1_r, best2_d, best2_v)</span><br><span class="line"></span><br><span class="line">print(cal_entropy(y2_l))  <span class="comment"># 0.44506485705083865</span></span><br><span class="line">print(cal_entropy(y2_r))  <span class="comment"># 0.15109697051711368</span></span><br></pre></td></tr></table></figure>

<p>可以继续深入划分。</p>
<p>由于数据集各个特征值是连续的，所以，上述过程是：<font color="red">遍历找到所划分的特征，这个特征中遍历的所有样本，排序后求相邻两个样本的均值作为这个特征的具体划分值</font>，时间复杂度为O(mxn)，即样本个数乘以特征个数。</p>
<p>对于各个特征值为离散的，使用<font color="red">信息增益</font>来找到消除不确定性最强的特征。每次都找信息增益最大的属性作为下一个划分属性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/20/%E5%9B%9E%E9%A1%BE%E5%86%B3%E7%AD%96%E6%A0%91-%E4%B8%80/" data-id="ckatsrgt20055xqfza3mq1kig" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回看SVM-三" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/" class="article-date">
  <time datetime="2019-08-18T14:42:57.000Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/">回看SVM-(三)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="升维"><a href="#升维" class="headerlink" title="升维"></a>升维</h1><p>在现实任务中，样本空间内很可能不存在一个能将样本正确划分的超平面。此时，可以将样本<font color="red">从原始空间映射到一个更高维的样本空间</font>，使得样本在这个高维空间内线性可分。如果原始空间是有限维的，也就是说，样本的属性个数有限，那么一定存在一个更高的样本维度使样本线性可分。</p>
<p>上述涉及到每一个特征映射到高维空间后之间的内积，由于特征空间的维度可能很高，甚至是无穷维的，因此先映射到高维空间后计算，是非常困难的。即，核函数可以等价代替<strong>样本映射后的特征向量间的内积</strong>。使用<code>Kernel Trick</code> 避开这个障碍。如此用一个近似的计算来取代映射到高维特征向量。好处：减少了计算量，节省了内存，Kernel Trick是一个可以应用于任何算法的技巧。</p>
<h1 id="RBF核"><a href="#RBF核" class="headerlink" title="RBF核"></a>RBF核</h1><p>假如原始数据只有一维信息，如果有两个landmark，就将数据变化为2维数据。每个样本的取值变化为x-&gt;( ，)，即含有2个特征。下面模拟该过程： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维数据x</span></span><br><span class="line">x = np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应的lable</span></span><br><span class="line">y = np.asarray((x &gt;= <span class="number">-3</span>) &amp; (x &lt;= <span class="number">3</span>), dtype=int)</span><br><span class="line"></span><br><span class="line">plt.scatter(x[y == <span class="number">0</span>], <span class="number">0</span>*(y[y == <span class="number">0</span>]))</span><br><span class="line">plt.scatter(x[y == <span class="number">1</span>], <span class="number">0</span>*(y[y == <span class="number">1</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>如图：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-1.png" width="500"></div>
<div align="center">图 原始数据 </div>

<p>定义一个函数，每个样本跟landmark进行计算，得到新的x：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, l)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    每个x 跟l计算得到新的x</span></span><br><span class="line"><span class="string">    :param x: 原来x</span></span><br><span class="line"><span class="string">    :param l: x跟谁计算</span></span><br><span class="line"><span class="string">    :return: 新的x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    gamma = <span class="number">0.05</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(-gamma * (x - l)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>对x的每个样本进行升维处理，得到新的x：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.empty((len(x), <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(x):</span><br><span class="line">    <span class="comment"># x_new 的第一个特征：</span></span><br><span class="line">    x_new[i, <span class="number">0</span>] = gaussian(data, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># x_new 的第二个特征：</span></span><br><span class="line">    x_new[i, <span class="number">1</span>] = gaussian(data, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(x_new[y == <span class="number">0</span>, <span class="number">0</span>], x_new[y == <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x_new[y == <span class="number">1</span>, <span class="number">0</span>], x_new[y == <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>如图：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-2.png" width="500"></div>
<div align="center">图 升维后的数据 </div>

<p>此2维数据显然是线性可分的。本例是使用l1和l2 两个landmark，即，只将数据变化为2维，原本20x1的数据映射成20x2。BRF高斯核实际上是使用每一个数据点作为landmark，即原本mxn的数据映射成mxm的数据。当m远远大于n时(一般合格的数据集)，数据特征由n增加到m。当然，对于一个m小于n的数据集(如NLP等情境下的数据)，依然变换为mxm，其实是降维处理。</p>
<h1 id="RBF核gamma"><a href="#RBF核gamma" class="headerlink" title="RBF核gamma"></a>RBF核gamma</h1><p>RBF中的参数gamma与高斯分布有关：gamma越大，分布越窄越高。<br>实验如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用moon数据：</span></span><br><span class="line">x, y = datasets.make_moons(noise=<span class="number">0.15</span>, random_state=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RBFKernel</span><span class="params">(gamma=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([(<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">                     (<span class="string">"svc"</span>, SVC(kernel=<span class="string">"rbf"</span>, gamma=gamma))])</span><br></pre></td></tr></table></figure>

<p>指定gamma，并学习，当gamma=100时：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">svc = RBFKernel(<span class="number">100</span>)</span><br><span class="line">svc.fit(x, y)</span><br><span class="line">plot_decision_boundary(svc, axis=[<span class="number">-1.5</span>, <span class="number">2.5</span>, <span class="number">-1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">0</span>, <span class="number">0</span>], x[y == <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y == <span class="number">1</span>, <span class="number">0</span>], x[y == <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>绘图：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-3.png" width="500"></div>
<div align="center">图 gamma=100 </div>

<p>当gamma=100，很大， 对应高斯分布越高越窄， 反映在图中就是每一个橘色样本点周围的小区域为一个高斯分布，橘色样本点本身为分布的顶点，即，模型判断，样本点只在这样一个区域内，才被判定为橘色点。 此情况显然<font color="red">过拟合</font>。</p>
<p>当gamma=10， 上述的分布区域变大了：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-4.png" width="500"></div>
<div align="center">图 gamma=10 </div>

<p>当gamma=0.1，很小时，不拟合：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/rbf-5.png" width="500"></div>
<div align="center">图 gamma=0.1 </div>

<p>所以可以说，gamma值在控制模型的复杂度，gamma越小，模型复杂度越小，<br>所以<font>要找到合适的gamma</font>。</p>
<h1 id="SVM解决回归问题：SVR"><a href="#SVM解决回归问题：SVR" class="headerlink" title="SVM解决回归问题：SVR"></a>SVM解决回归问题：SVR</h1><p>SVR 使得<font color="red">在margin范围里的样本点越多越好，表示这个范围可以较好地表示样本点。此时取中间的线为回归曲线</font>。</p>
<p>或者说，SVR解决的问题与SVM相反，soft SVM要使得margin间的点越少越好，而SVR要使得margin间的点越多越好。</p>
<p>又或者说，SVR目的与SVM相同，都是最大化margin间的距离。SVR假设我们可以容忍f(x)与y之间最多有epislon的偏差，即，<strong>当且仅当f(x)与y之间的距离绝对值大于epsilon时，才计算损失，损失仍是最小化margin间的距离</strong>。这个过程相当于构建了宽度为2×epsilon的间隔带，<strong>如果样本点在间隔带之间，则认为是被预测正确的</strong>。</p>
<p>sklearn中的使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVR</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR  <span class="comment"># 指明核函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">StandardLinearSVR</span><span class="params">(epsilon=<span class="number">0.1</span>, c=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    指明参数 epsilon 和 C 等</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"LinearSVR"</span>, LinearSVR(epsilon=epsilon, C=c))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%B8%89/" data-id="ckatsrgsu004mxqfz16ks7hi6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-回看SVM-二" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/" class="article-date">
  <time datetime="2019-08-18T13:18:45.000Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/">回看SVM-(二)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>对于线性不可分而非线性可分的数据，也可以使用线性SVM。只需要<font color="red">将数据转换为高维的含有多项式项的数据</font>。</p>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>先找一个，分线性可分数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">x, y = datasets.make_moons(noise=<span class="number">0.15</span>, random_state=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制出结果：</span></span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>, <span class="number">0</span>], x[y==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>, <span class="number">0</span>], x[y==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/svm-moon.png" width="500"></div>
<div align="center">图 待处理数据</div>


<h1 id="含多项式特征的LinearSVM"><a href="#含多项式特征的LinearSVM" class="headerlink" title="含多项式特征的LinearSVM"></a>含多项式特征的LinearSVM</h1><p><font color="red">指定多形式的阶数</font>，将数据转化成高维的含有多项式项数据，后传入<code>LinearSVM</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomialSVC</span><span class="params">(degree, C=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">"Poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">        (<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"LinearSVC"</span>, LinearSVC(C=C))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<p>使用<code>Pipline</code>可以顺序执行各部分。实例化模型，并查看模型信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poly_svc = PolynomialSVC(<span class="number">3</span>)</span><br><span class="line">poly_svc.fit(x, y)</span><br><span class="line">print(poly_svc)</span><br></pre></td></tr></table></figure>

<p>模型信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Pipeline(memory=<span class="literal">None</span>,</span><br><span class="line">     steps=[(<span class="string">'Poly'</span>, PolynomialFeatures(degree=<span class="number">3</span>, include_bias=<span class="literal">True</span>, interaction_only=<span class="literal">False</span>)), (<span class="string">'standard'</span>, StandardScaler(copy=<span class="literal">True</span>, with_mean=<span class="literal">True</span>, with_std=<span class="literal">True</span>)), (<span class="string">'LinearSVC'</span>, LinearSVC(C=<span class="number">0.1</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">True</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">     intercept_scaling=<span class="number">1</span>, loss=<span class="string">'squared_hinge'</span>, max_iter=<span class="number">1000</span>,</span><br><span class="line">     multi_class=<span class="string">'ovr'</span>, penalty=<span class="string">'l2'</span>, random_state=<span class="literal">None</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">     verbose=<span class="number">0</span>))])</span><br></pre></td></tr></table></figure>

<p>绘出分类边界：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_boundary(poly_svc, axis=[<span class="number">-1.5</span>, <span class="number">2.5</span>, <span class="number">-1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>, <span class="number">0</span>], x[y==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>, <span class="number">0</span>], x[y==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果为分线性边界：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/svm-moon-2.png" width="500"></div>
<div align="center">图 分类边界</div>

<h1 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h1><p>SVM可以<font color="red">直接使用数据的多项式特征，即多项式核函数</font>，所以使用SVC而非LinearSVC。此时Pipline中不需先得到多项式特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Polynomial_KernelSVC</span><span class="params">(degree, C=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">"standard"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"kernelSVC"</span>, SVC(kernel=<span class="string">"poly"</span>, degree=degree, C=C))  </span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<p>传入kernel参数，指定使用什么样的核函数。实例化模型，绘制分类边界：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">poly_kernel_svc = Polynomial_KernelSVC(degree=<span class="number">5</span>)</span><br><span class="line">poly_kernel_svc.fit(x, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(poly_kernel_svc, axis=[<span class="number">-1.5</span>, <span class="number">2.5</span>, <span class="number">-1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>, <span class="number">0</span>], x[y==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>, <span class="number">0</span>], x[y==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>边界：</p>
<div align="center"><img src="/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/svm-moon-3.png" width="500"></div>
<div align="center">图 使用多项式核函数的分类边界</div>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/18/%E5%9B%9E%E7%9C%8BSVM-%E4%BA%8C/" data-id="ckatsrgsv004pxqfzfw1xbm2m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/9/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/11/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA/">CUDA</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">23</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linear-Algebra/">Linear Algebra</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Utility/">Utility</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BE%85%E5%BD%92%E7%B1%BB/">待归类</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/" rel="tag">Algorithms</a><span class="tag-list-count">50</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a><span class="tag-list-count">33</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Caffe/" rel="tag">Caffe</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test-Analysis/" rel="tag">Test Analysis</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CUDA/" style="font-size: 18px;">CUDA</a> <a href="/tags/Caffe/" style="font-size: 16px;">Caffe</a> <a href="/tags/Test-Analysis/" style="font-size: 14px;">Test Analysis</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 12px;">二分查找</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 10px;">递归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">24</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">38</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/15/LeetCode-top-k/">LeetCode-top-k</a>
          </li>
        
          <li>
            <a href="/2020/06/13/LeetCode-153-%E6%89%BE%E5%88%B0%E6%9C%80%E5%B0%8F%E5%80%BC/">LeetCode-153-找到最小值</a>
          </li>
        
          <li>
            <a href="/2020/06/12/LeetCode-34-find-positions-of-elements/">LeetCode-34-find-positions-of-elements</a>
          </li>
        
          <li>
            <a href="/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9sqrt-x/">LeetCode-求一个数的平方根sqrt(x)</a>
          </li>
        
          <li>
            <a href="/2020/06/12/LeetCode-%E6%B1%82%E4%B8%80%E4%B8%AA%E6%95%B0%E7%9A%84n%E6%AC%A1%E5%B9%82/">LeetCode-求一个数的n次幂</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Junhui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>